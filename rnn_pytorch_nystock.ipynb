{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  HYPER PARAMAETERS  ##\n",
    "\n",
    "TICKER = \"KO\"\n",
    "SEQUENCE_NUM = 10\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 1\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>volume</th>\n",
       "      <th>high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>28.580000</td>\n",
       "      <td>28.520000</td>\n",
       "      <td>28.450001</td>\n",
       "      <td>13870400.0</td>\n",
       "      <td>28.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>28.424999</td>\n",
       "      <td>28.174999</td>\n",
       "      <td>28.070000</td>\n",
       "      <td>23172400.0</td>\n",
       "      <td>28.495001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>28.174999</td>\n",
       "      <td>28.165001</td>\n",
       "      <td>27.990000</td>\n",
       "      <td>19264600.0</td>\n",
       "      <td>28.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>28.165001</td>\n",
       "      <td>28.095000</td>\n",
       "      <td>27.875000</td>\n",
       "      <td>13234600.0</td>\n",
       "      <td>28.184999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369</th>\n",
       "      <td>27.730000</td>\n",
       "      <td>27.575001</td>\n",
       "      <td>27.375000</td>\n",
       "      <td>28712400.0</td>\n",
       "      <td>27.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849025</th>\n",
       "      <td>41.619999</td>\n",
       "      <td>41.599998</td>\n",
       "      <td>41.349998</td>\n",
       "      <td>6410600.0</td>\n",
       "      <td>41.689999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849525</th>\n",
       "      <td>41.560001</td>\n",
       "      <td>41.610001</td>\n",
       "      <td>41.540001</td>\n",
       "      <td>6986100.0</td>\n",
       "      <td>41.759998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850025</th>\n",
       "      <td>41.490002</td>\n",
       "      <td>41.389999</td>\n",
       "      <td>41.389999</td>\n",
       "      <td>8825600.0</td>\n",
       "      <td>41.669998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850525</th>\n",
       "      <td>41.380001</td>\n",
       "      <td>41.599998</td>\n",
       "      <td>41.380001</td>\n",
       "      <td>6817900.0</td>\n",
       "      <td>41.689999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851025</th>\n",
       "      <td>41.689999</td>\n",
       "      <td>41.459999</td>\n",
       "      <td>41.349998</td>\n",
       "      <td>11466600.0</td>\n",
       "      <td>41.840000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1762 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             open      close        low      volume       high\n",
       "497     28.580000  28.520000  28.450001  13870400.0  28.610001\n",
       "965     28.424999  28.174999  28.070000  23172400.0  28.495001\n",
       "1433    28.174999  28.165001  27.990000  19264600.0  28.220000\n",
       "1901    28.165001  28.095000  27.875000  13234600.0  28.184999\n",
       "2369    27.730000  27.575001  27.375000  28712400.0  27.820000\n",
       "...           ...        ...        ...         ...        ...\n",
       "849025  41.619999  41.599998  41.349998   6410600.0  41.689999\n",
       "849525  41.560001  41.610001  41.540001   6986100.0  41.759998\n",
       "850025  41.490002  41.389999  41.389999   8825600.0  41.669998\n",
       "850525  41.380001  41.599998  41.380001   6817900.0  41.689999\n",
       "851025  41.689999  41.459999  41.349998  11466600.0  41.840000\n",
       "\n",
       "[1762 rows x 5 columns]"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/ny_stock.csv')\n",
    "df = df.loc[(df['symbol'] == TICKER)]\n",
    "df = df.drop(columns=['date','symbol'])\n",
    "df = df[['open','close','low','volume','high']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(df['open'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.empty((0, SEQUENCE_NUM, 5))\n",
    "t=np.empty((0, 5))\n",
    "\n",
    "for i in range(df.shape[0]-SEQUENCE_NUM-1):\n",
    "  x=np.append(x, [df.iloc[i:i+SEQUENCE_NUM]], axis=0)\n",
    "  t=np.append(t, [df.iloc[i+SEQUENCE_NUM+1]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1751, 10, 5]), torch.Size([1751, 5]))"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class StockDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self):\n",
    "    self.data = torch.Tensor(x)\n",
    "    self.target = torch.Tensor(t)\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.data[idx], self.target[idx]\n",
    "  \n",
    "dataset = StockDataset()\n",
    "\n",
    "data_mean = dataset.data.mean(dim=0, keepdim=True)\n",
    "data_std = dataset.data.std(dim=0, keepdim=True)\n",
    "\n",
    "target_mean = dataset.target.mean(dim=0, keepdim=True)\n",
    "target_std = dataset.target.std(dim=0, keepdim=True)\n",
    "\n",
    "dataset.data = (dataset.data - data_mean) / (data_std + 1e-7)\n",
    "dataset.target = (dataset.target - target_mean) / (target_std + 1e-7)\n",
    "\n",
    "generator = torch.Generator().manual_seed(69)\n",
    "train, validation, test = torch.utils.data.random_split(dataset, [.7, .2, .1], generator)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size, shuffle=True, drop_last=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation, batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size, shuffle=True)\n",
    "\n",
    "dataset.data.shape, dataset.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StockNeuralNetwork(\n",
       "  (rnn): LSTM(5, 100, num_layers=10, batch_first=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Dropout(p=0.3, inplace=False)\n",
       "    (1): Linear(in_features=100, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class StockNeuralNetwork(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.rnn = nn.LSTM(5, 100, 10, batch_first=True)\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Dropout(p=0.3),\n",
    "      nn.Linear(100, 5)\n",
    "    )\n",
    "\n",
    "    self.hn = (\n",
    "      torch.zeros(1, batch_size, 100),\n",
    "      torch.zeros(1, batch_size, 100))\n",
    "\n",
    "  def forward(self, x):\n",
    "    y, self.hn = self.rnn(x, self.hn)\n",
    "    y = self.layers(y)\n",
    "    return y\n",
    "  \n",
    "  def refresh_hidden(self):\n",
    "    weight = next(self.parameters()).data\n",
    "    self.hn = (\n",
    "      weight.new(10, batch_size, 100).zero_(),\n",
    "      weight.new(10, batch_size, 100).zero_())\n",
    "  \n",
    "model = StockNeuralNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "  def __init__(self, patience=1, min_delta=0):\n",
    "    self.patience = patience\n",
    "    self.min_delta = min_delta\n",
    "    self.counter = 0\n",
    "    self.min_validation_loss = float('inf')\n",
    "\n",
    "  def early_stop(self, validation_loss):\n",
    "    if validation_loss < self.min_validation_loss:\n",
    "      self.min_validation_loss = validation_loss\n",
    "      self.counter = 0\n",
    "    elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "      self.counter += 1\n",
    "      if self.counter >= self.patience:\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, Batch #0, Train Loss: 1.8128, Validation Loss: 4.0417\n",
      "Epoch #0, Batch #10, Train Loss: 0.5496, Validation Loss: 1.1926\n",
      "Epoch #0, Batch #20, Train Loss: 0.6713, Validation Loss: 0.7755\n",
      "Epoch #0, Batch #30, Train Loss: 0.0392, Validation Loss: 0.6779\n",
      "Epoch #0, Batch #40, Train Loss: 0.8460, Validation Loss: 0.5463\n",
      "Epoch #0, Batch #50, Train Loss: 0.2251, Validation Loss: 0.7843\n",
      "Epoch #0, Batch #60, Train Loss: 0.6096, Validation Loss: 0.7542\n",
      "Epoch #0, Batch #70, Train Loss: 1.0498, Validation Loss: 0.3455\n",
      "Epoch #0, Batch #80, Train Loss: 0.4963, Validation Loss: 0.5875\n",
      "Epoch #0, Batch #90, Train Loss: 0.1016, Validation Loss: 0.3706\n",
      "Epoch #0, Batch #100, Train Loss: 0.3360, Validation Loss: 0.6429\n",
      "Epoch #0, Batch #110, Train Loss: 0.1472, Validation Loss: 0.3626\n",
      "Epoch #0, Batch #120, Train Loss: 0.0258, Validation Loss: 0.5088\n",
      "Epoch #0, Batch #130, Train Loss: 0.6785, Validation Loss: 2.5852\n",
      "Epoch #0, Batch #140, Train Loss: 0.1636, Validation Loss: 1.0020\n",
      "Epoch #0, Batch #150, Train Loss: 0.3336, Validation Loss: 0.1078\n",
      "Epoch #0, Batch #160, Train Loss: 0.0313, Validation Loss: 1.3628\n",
      "Epoch #0, Batch #170, Train Loss: 1.2213, Validation Loss: 1.3752\n",
      "Epoch #0, Batch #180, Train Loss: 0.3073, Validation Loss: 1.3258\n",
      "Epoch #0, Batch #190, Train Loss: 0.0652, Validation Loss: 0.6542\n",
      "Epoch #0, Batch #200, Train Loss: 0.1243, Validation Loss: 0.0915\n",
      "Epoch #0, Batch #210, Train Loss: 0.4654, Validation Loss: 0.4940\n",
      "Epoch #0, Batch #220, Train Loss: 0.2333, Validation Loss: 0.7512\n",
      "Epoch #0, Batch #230, Train Loss: 0.1366, Validation Loss: 0.4685\n",
      "Epoch #0, Batch #240, Train Loss: 0.1917, Validation Loss: 1.4476\n",
      "Epoch #0, Batch #250, Train Loss: 0.2079, Validation Loss: 3.4722\n",
      "Epoch #0, Batch #260, Train Loss: 0.5297, Validation Loss: 0.0740\n",
      "Epoch #0, Batch #270, Train Loss: 0.1454, Validation Loss: 0.5856\n",
      "Epoch #0, Batch #280, Train Loss: 0.2730, Validation Loss: 1.8239\n",
      "Epoch #0, Batch #290, Train Loss: 0.0228, Validation Loss: 0.0532\n",
      "Epoch #0, Batch #300, Train Loss: 3.8328, Validation Loss: 0.1884\n",
      "Epoch #0, Batch #310, Train Loss: 0.2994, Validation Loss: 0.1310\n",
      "Epoch #0, Batch #320, Train Loss: 0.9277, Validation Loss: 0.1088\n",
      "Epoch #0, Batch #330, Train Loss: 0.4549, Validation Loss: 0.0733\n",
      "Epoch #0, Batch #340, Train Loss: 0.0542, Validation Loss: 0.2887\n",
      "Epoch #0, Batch #350, Train Loss: 0.0702, Validation Loss: 0.0753\n",
      "Epoch #0, Batch #360, Train Loss: 0.2041, Validation Loss: 2.8208\n",
      "Epoch #0, Batch #370, Train Loss: 0.0932, Validation Loss: 0.0167\n",
      "Epoch #0, Batch #380, Train Loss: 1.2318, Validation Loss: 1.1084\n",
      "Epoch #0, Batch #390, Train Loss: 1.4040, Validation Loss: 0.3517\n",
      "Epoch #0, Batch #400, Train Loss: 0.0666, Validation Loss: 2.3949\n",
      "Epoch #0, Batch #410, Train Loss: 0.0516, Validation Loss: 2.1713\n",
      "Epoch #0, Batch #420, Train Loss: 0.0488, Validation Loss: 1.0762\n",
      "Epoch #0, Batch #430, Train Loss: 0.5562, Validation Loss: 3.5436\n",
      "Epoch #0, Batch #440, Train Loss: 1.8154, Validation Loss: 1.6561\n",
      "Epoch #0, Batch #450, Train Loss: 0.0624, Validation Loss: 2.4859\n",
      "Epoch #0, Batch #460, Train Loss: 0.2163, Validation Loss: 2.7894\n",
      "Epoch #0, Batch #470, Train Loss: 0.2920, Validation Loss: 0.7053\n",
      "Epoch #0, Batch #480, Train Loss: 0.9174, Validation Loss: 3.6755\n",
      "Epoch #0, Batch #490, Train Loss: 0.1261, Validation Loss: 0.0390\n",
      "Epoch #0, Batch #500, Train Loss: 0.2122, Validation Loss: 0.1147\n",
      "Epoch #0, Batch #510, Train Loss: 0.2565, Validation Loss: 0.1038\n",
      "Epoch #0, Batch #520, Train Loss: 0.5063, Validation Loss: 0.8525\n",
      "Epoch #0, Batch #530, Train Loss: 0.3227, Validation Loss: 7.1050\n",
      "Epoch #0, Batch #540, Train Loss: 0.0291, Validation Loss: 4.3755\n",
      "Epoch #0, Batch #550, Train Loss: 0.2204, Validation Loss: 1.0339\n",
      "Epoch #0, Batch #560, Train Loss: 0.0206, Validation Loss: 1.5157\n",
      "Epoch #0, Batch #570, Train Loss: 0.1428, Validation Loss: 0.4732\n",
      "Epoch #0, Batch #580, Train Loss: 0.3820, Validation Loss: 3.2169\n",
      "Epoch #0, Batch #590, Train Loss: 0.0833, Validation Loss: 0.5956\n",
      "Epoch #0, Batch #600, Train Loss: 0.0810, Validation Loss: 6.1308\n",
      "Epoch #0, Batch #610, Train Loss: 0.1598, Validation Loss: 0.6426\n",
      "Epoch #0, Batch #620, Train Loss: 0.1563, Validation Loss: 1.9305\n",
      "Epoch #0, Batch #630, Train Loss: 0.5813, Validation Loss: 0.0961\n",
      "Epoch #0, Batch #640, Train Loss: 0.3833, Validation Loss: 0.5075\n",
      "Epoch #0, Batch #650, Train Loss: 0.0465, Validation Loss: 0.0645\n",
      "Epoch #0, Batch #660, Train Loss: 0.6507, Validation Loss: 1.2960\n",
      "Epoch #0, Batch #670, Train Loss: 0.7031, Validation Loss: 5.7511\n",
      "Epoch #0, Batch #680, Train Loss: 0.5970, Validation Loss: 0.1256\n",
      "Epoch #0, Batch #690, Train Loss: 0.1925, Validation Loss: 1.7789\n",
      "Epoch #0, Batch #700, Train Loss: 0.0021, Validation Loss: 0.6498\n",
      "Epoch #0, Batch #710, Train Loss: 0.6273, Validation Loss: 0.8155\n",
      "Epoch #0, Batch #720, Train Loss: 0.1061, Validation Loss: 2.7955\n",
      "Epoch #0, Batch #730, Train Loss: 0.3151, Validation Loss: 2.7969\n",
      "Epoch #0, Batch #740, Train Loss: 0.1155, Validation Loss: 0.0381\n",
      "Epoch #0, Batch #750, Train Loss: 0.0366, Validation Loss: 2.9233\n",
      "Epoch #0, Batch #760, Train Loss: 0.3068, Validation Loss: 0.7604\n",
      "Epoch #0, Batch #770, Train Loss: 1.0312, Validation Loss: 6.6622\n",
      "Epoch #0, Batch #780, Train Loss: 1.0087, Validation Loss: 5.6017\n",
      "Epoch #0, Batch #790, Train Loss: 0.2843, Validation Loss: 0.5015\n",
      "Epoch #0, Batch #800, Train Loss: 0.1406, Validation Loss: 2.8815\n",
      "Epoch #0, Batch #810, Train Loss: 0.0720, Validation Loss: 0.4266\n",
      "Epoch #0, Batch #820, Train Loss: 0.1365, Validation Loss: 0.1580\n",
      "Epoch #0, Batch #830, Train Loss: 0.1256, Validation Loss: 0.1667\n",
      "Epoch #0, Batch #840, Train Loss: 0.0047, Validation Loss: 2.0468\n",
      "Epoch #0, Batch #850, Train Loss: 0.1413, Validation Loss: 2.3852\n",
      "Epoch #0, Batch #860, Train Loss: 0.4596, Validation Loss: 4.6742\n",
      "Epoch #0, Batch #870, Train Loss: 0.1050, Validation Loss: 5.8187\n",
      "Epoch #0, Batch #880, Train Loss: 0.1150, Validation Loss: 0.1013\n",
      "Epoch #0, Batch #890, Train Loss: 0.4087, Validation Loss: 0.5585\n",
      "Epoch #0, Batch #900, Train Loss: 0.2334, Validation Loss: 5.3062\n",
      "Epoch #0, Batch #910, Train Loss: 0.8281, Validation Loss: 0.3643\n",
      "Epoch #0, Batch #920, Train Loss: 0.0261, Validation Loss: 0.0420\n",
      "Epoch #0, Batch #930, Train Loss: 0.2457, Validation Loss: 0.0753\n",
      "Epoch #0, Batch #940, Train Loss: 0.0931, Validation Loss: 0.1379\n",
      "Epoch #0, Batch #950, Train Loss: 0.6015, Validation Loss: 0.4497\n",
      "Epoch #0, Batch #960, Train Loss: 0.3358, Validation Loss: 0.6562\n",
      "Epoch #0, Batch #970, Train Loss: 0.2218, Validation Loss: 4.2208\n",
      "Epoch #0, Batch #980, Train Loss: 0.1553, Validation Loss: 5.5441\n",
      "Epoch #0, Batch #990, Train Loss: 0.0494, Validation Loss: 0.1266\n",
      "Epoch #0, Batch #1000, Train Loss: 0.3970, Validation Loss: 3.0582\n",
      "Epoch #0, Batch #1010, Train Loss: 0.4361, Validation Loss: 5.3323\n",
      "Epoch #0, Batch #1020, Train Loss: 0.0752, Validation Loss: 4.1162\n",
      "Epoch #0, Batch #1030, Train Loss: 0.1482, Validation Loss: 2.8997\n",
      "Epoch #0, Batch #1040, Train Loss: 0.0641, Validation Loss: 0.6948\n",
      "Epoch #0, Batch #1050, Train Loss: 0.7451, Validation Loss: 0.6832\n",
      "Epoch #0, Batch #1060, Train Loss: 0.0501, Validation Loss: 4.4076\n",
      "Epoch #0, Batch #1070, Train Loss: 0.5812, Validation Loss: 3.8747\n",
      "Epoch #0, Batch #1080, Train Loss: 0.3569, Validation Loss: 4.5652\n",
      "Epoch #0, Batch #1090, Train Loss: 0.0188, Validation Loss: 0.0780\n",
      "Epoch #0, Batch #1100, Train Loss: 0.3026, Validation Loss: 0.0755\n",
      "Epoch #0, Batch #1110, Train Loss: 0.0556, Validation Loss: 0.7561\n",
      "Epoch #0, Batch #1120, Train Loss: 0.1683, Validation Loss: 5.8914\n",
      "Epoch #0, Batch #1130, Train Loss: 0.0255, Validation Loss: 0.7520\n",
      "Epoch #0, Batch #1140, Train Loss: 0.1363, Validation Loss: 0.1056\n",
      "Epoch #0, Batch #1150, Train Loss: 0.5415, Validation Loss: 0.2340\n",
      "Epoch #0, Batch #1160, Train Loss: 0.1187, Validation Loss: 0.8046\n",
      "Epoch #0, Batch #1170, Train Loss: 1.6020, Validation Loss: 0.1257\n",
      "Epoch #0, Batch #1180, Train Loss: 0.1299, Validation Loss: 0.9461\n",
      "Epoch #0, Batch #1190, Train Loss: 0.1810, Validation Loss: 0.4711\n",
      "Epoch #0, Batch #1200, Train Loss: 0.0092, Validation Loss: 4.7629\n",
      "Epoch #0, Batch #1210, Train Loss: 0.4657, Validation Loss: 0.5104\n",
      "Epoch #0, Batch #1220, Train Loss: 0.0815, Validation Loss: 0.2420\n",
      "Epoch #1, Batch #0, Train Loss: 0.3028, Validation Loss: 0.5998\n",
      "Epoch #1, Batch #10, Train Loss: 0.1752, Validation Loss: 3.1279\n",
      "Epoch #1, Batch #20, Train Loss: 0.0463, Validation Loss: 2.8414\n",
      "Epoch #1, Batch #30, Train Loss: 0.3680, Validation Loss: 1.4805\n",
      "Epoch #1, Batch #40, Train Loss: 0.0413, Validation Loss: 9.5383\n",
      "Epoch #1, Batch #50, Train Loss: 0.3219, Validation Loss: 11.9328\n",
      "Epoch #1, Batch #60, Train Loss: 0.0989, Validation Loss: 9.1415\n",
      "Epoch #1, Batch #70, Train Loss: 0.0820, Validation Loss: 8.5921\n",
      "Epoch #1, Batch #80, Train Loss: 0.4365, Validation Loss: 0.8358\n",
      "Epoch #1, Batch #90, Train Loss: 0.0622, Validation Loss: 0.0423\n",
      "Epoch #1, Batch #100, Train Loss: 0.0922, Validation Loss: 3.3187\n",
      "Epoch #1, Batch #110, Train Loss: 0.0900, Validation Loss: 0.5271\n",
      "Epoch #1, Batch #120, Train Loss: 0.1143, Validation Loss: 0.5482\n",
      "Epoch #1, Batch #130, Train Loss: 0.0694, Validation Loss: 4.1334\n",
      "Epoch #1, Batch #140, Train Loss: 0.1199, Validation Loss: 0.6210\n",
      "Epoch #1, Batch #150, Train Loss: 0.0845, Validation Loss: 0.0219\n",
      "Epoch #1, Batch #160, Train Loss: 0.1817, Validation Loss: 0.5554\n",
      "Epoch #1, Batch #170, Train Loss: 0.5220, Validation Loss: 0.8321\n",
      "Epoch #1, Batch #180, Train Loss: 0.3280, Validation Loss: 1.2584\n",
      "Epoch #1, Batch #190, Train Loss: 0.2118, Validation Loss: 1.0680\n",
      "Epoch #1, Batch #200, Train Loss: 0.1561, Validation Loss: 0.5719\n",
      "Epoch #1, Batch #210, Train Loss: 0.0655, Validation Loss: 0.3294\n",
      "Epoch #1, Batch #220, Train Loss: 0.2444, Validation Loss: 2.5610\n",
      "Epoch #1, Batch #230, Train Loss: 0.0379, Validation Loss: 4.3432\n",
      "Epoch #1, Batch #240, Train Loss: 0.0394, Validation Loss: 1.5540\n",
      "Epoch #1, Batch #250, Train Loss: 1.2531, Validation Loss: 7.6859\n",
      "Epoch #1, Batch #260, Train Loss: 0.0172, Validation Loss: 0.6768\n",
      "Epoch #1, Batch #270, Train Loss: 0.1264, Validation Loss: 5.7810\n",
      "Epoch #1, Batch #280, Train Loss: 0.2082, Validation Loss: 3.6845\n",
      "Epoch #1, Batch #290, Train Loss: 0.1985, Validation Loss: 3.4712\n",
      "Epoch #1, Batch #300, Train Loss: 0.1007, Validation Loss: 10.7055\n",
      "Epoch #1, Batch #310, Train Loss: 0.3387, Validation Loss: 7.3357\n",
      "Epoch #1, Batch #320, Train Loss: 0.2690, Validation Loss: 0.6775\n",
      "Epoch #1, Batch #330, Train Loss: 0.0387, Validation Loss: 0.0666\n",
      "Epoch #1, Batch #340, Train Loss: 0.0142, Validation Loss: 7.8328\n",
      "Epoch #1, Batch #350, Train Loss: 0.9449, Validation Loss: 5.7732\n",
      "Epoch #1, Batch #360, Train Loss: 0.0160, Validation Loss: 1.8029\n",
      "Epoch #1, Batch #370, Train Loss: 0.1771, Validation Loss: 1.0986\n",
      "Epoch #1, Batch #380, Train Loss: 0.2183, Validation Loss: 0.8569\n",
      "Epoch #1, Batch #390, Train Loss: 0.0242, Validation Loss: 2.1875\n",
      "Epoch #1, Batch #400, Train Loss: 0.0926, Validation Loss: 9.8054\n",
      "Epoch #1, Batch #410, Train Loss: 0.0582, Validation Loss: 0.4065\n",
      "Epoch #1, Batch #420, Train Loss: 0.4110, Validation Loss: 0.0668\n",
      "Epoch #1, Batch #430, Train Loss: 0.2121, Validation Loss: 0.8102\n",
      "Epoch #1, Batch #440, Train Loss: 0.0111, Validation Loss: 0.5527\n",
      "Epoch #1, Batch #450, Train Loss: 0.1395, Validation Loss: 8.4461\n",
      "Epoch #1, Batch #460, Train Loss: 0.0523, Validation Loss: 1.4939\n",
      "Epoch #1, Batch #470, Train Loss: 0.0815, Validation Loss: 5.3298\n",
      "Epoch #1, Batch #480, Train Loss: 0.3373, Validation Loss: 0.8646\n",
      "Epoch #1, Batch #490, Train Loss: 0.1022, Validation Loss: 6.0851\n",
      "Epoch #1, Batch #500, Train Loss: 0.0289, Validation Loss: 3.5490\n",
      "Epoch #1, Batch #510, Train Loss: 0.0190, Validation Loss: 6.8387\n",
      "Epoch #1, Batch #520, Train Loss: 0.1145, Validation Loss: 0.6362\n",
      "Epoch #1, Batch #530, Train Loss: 0.0777, Validation Loss: 8.6033\n",
      "Epoch #1, Batch #540, Train Loss: 0.4966, Validation Loss: 10.2298\n",
      "Epoch #1, Batch #550, Train Loss: 0.0714, Validation Loss: 1.7251\n",
      "Epoch #1, Batch #560, Train Loss: 0.2143, Validation Loss: 1.0675\n",
      "Epoch #1, Batch #570, Train Loss: 0.0836, Validation Loss: 8.7864\n",
      "Epoch #1, Batch #580, Train Loss: 0.4637, Validation Loss: 1.0790\n",
      "Epoch #1, Batch #590, Train Loss: 0.0651, Validation Loss: 4.1502\n",
      "Epoch #1, Batch #600, Train Loss: 0.0506, Validation Loss: 1.9862\n",
      "Epoch #1, Batch #610, Train Loss: 0.0547, Validation Loss: 6.3476\n",
      "Epoch #1, Batch #620, Train Loss: 0.0984, Validation Loss: 7.1073\n",
      "Epoch #1, Batch #630, Train Loss: 0.1766, Validation Loss: 11.2126\n",
      "Epoch #1, Batch #640, Train Loss: 0.0086, Validation Loss: 0.8568\n",
      "Epoch #1, Batch #650, Train Loss: 0.0579, Validation Loss: 9.1076\n",
      "Epoch #1, Batch #660, Train Loss: 0.4639, Validation Loss: 4.7957\n",
      "Epoch #1, Batch #670, Train Loss: 0.0167, Validation Loss: 1.2724\n",
      "Epoch #1, Batch #680, Train Loss: 0.1074, Validation Loss: 5.9093\n",
      "Epoch #1, Batch #690, Train Loss: 0.0657, Validation Loss: 1.3621\n",
      "Epoch #1, Batch #700, Train Loss: 0.1086, Validation Loss: 1.3369\n",
      "Epoch #1, Batch #710, Train Loss: 1.1572, Validation Loss: 0.2263\n",
      "Epoch #1, Batch #720, Train Loss: 0.0404, Validation Loss: 8.6813\n",
      "Epoch #1, Batch #730, Train Loss: 0.2770, Validation Loss: 3.6768\n",
      "Epoch #1, Batch #740, Train Loss: 0.1069, Validation Loss: 0.0078\n",
      "Epoch #1, Batch #750, Train Loss: 0.0326, Validation Loss: 3.1603\n",
      "Epoch #1, Batch #760, Train Loss: 0.0640, Validation Loss: 8.0946\n",
      "Epoch #1, Batch #770, Train Loss: 0.0201, Validation Loss: 5.4580\n",
      "Epoch #1, Batch #780, Train Loss: 0.0632, Validation Loss: 2.5963\n",
      "Epoch #1, Batch #790, Train Loss: 0.0683, Validation Loss: 9.0609\n",
      "Epoch #1, Batch #800, Train Loss: 0.1281, Validation Loss: 3.9971\n",
      "Epoch #1, Batch #810, Train Loss: 0.1250, Validation Loss: 1.0404\n",
      "Epoch #1, Batch #820, Train Loss: 0.0508, Validation Loss: 0.3934\n",
      "Epoch #1, Batch #830, Train Loss: 0.0385, Validation Loss: 0.5447\n",
      "Epoch #1, Batch #840, Train Loss: 0.0511, Validation Loss: 14.9223\n",
      "Epoch #1, Batch #850, Train Loss: 0.0148, Validation Loss: 2.9414\n",
      "Epoch #1, Batch #860, Train Loss: 0.5891, Validation Loss: 5.4922\n",
      "Epoch #1, Batch #870, Train Loss: 0.2036, Validation Loss: 6.6379\n",
      "Epoch #1, Batch #880, Train Loss: 0.0694, Validation Loss: 3.7493\n",
      "Epoch #1, Batch #890, Train Loss: 0.1989, Validation Loss: 0.9653\n",
      "Epoch #1, Batch #900, Train Loss: 0.0167, Validation Loss: 1.0366\n",
      "Epoch #1, Batch #910, Train Loss: 0.1743, Validation Loss: 6.4202\n",
      "Epoch #1, Batch #920, Train Loss: 0.0665, Validation Loss: 4.0608\n",
      "Epoch #1, Batch #930, Train Loss: 0.0667, Validation Loss: 1.5799\n",
      "Epoch #1, Batch #940, Train Loss: 0.1155, Validation Loss: 8.7114\n",
      "Epoch #1, Batch #950, Train Loss: 0.1542, Validation Loss: 2.4146\n",
      "Epoch #1, Batch #960, Train Loss: 0.0089, Validation Loss: 10.7345\n",
      "Epoch #1, Batch #970, Train Loss: 0.2507, Validation Loss: 2.9388\n",
      "Epoch #1, Batch #980, Train Loss: 1.0082, Validation Loss: 0.0189\n",
      "Epoch #1, Batch #990, Train Loss: 0.0479, Validation Loss: 0.7392\n",
      "Epoch #1, Batch #1000, Train Loss: 0.0139, Validation Loss: 0.4179\n",
      "Epoch #1, Batch #1010, Train Loss: 0.0203, Validation Loss: 2.0182\n",
      "Epoch #1, Batch #1020, Train Loss: 0.1866, Validation Loss: 0.5234\n",
      "Epoch #1, Batch #1030, Train Loss: 0.0962, Validation Loss: 0.9092\n",
      "Epoch #1, Batch #1040, Train Loss: 0.0857, Validation Loss: 5.4839\n",
      "Epoch #1, Batch #1050, Train Loss: 0.3069, Validation Loss: 1.2898\n",
      "Epoch #1, Batch #1060, Train Loss: 0.0075, Validation Loss: 11.9832\n",
      "Epoch #1, Batch #1070, Train Loss: 0.0652, Validation Loss: 0.4958\n",
      "Epoch #1, Batch #1080, Train Loss: 0.1278, Validation Loss: 1.6232\n",
      "Epoch #1, Batch #1090, Train Loss: 0.1518, Validation Loss: 3.1195\n",
      "Epoch #1, Batch #1100, Train Loss: 0.0353, Validation Loss: 3.7672\n",
      "Epoch #1, Batch #1110, Train Loss: 0.0391, Validation Loss: 0.2913\n",
      "Epoch #1, Batch #1120, Train Loss: 0.0819, Validation Loss: 7.0243\n",
      "Epoch #1, Batch #1130, Train Loss: 0.0128, Validation Loss: 4.0853\n",
      "Epoch #1, Batch #1140, Train Loss: 0.1807, Validation Loss: 0.3447\n",
      "Epoch #1, Batch #1150, Train Loss: 0.3543, Validation Loss: 8.3021\n",
      "Epoch #1, Batch #1160, Train Loss: 0.0541, Validation Loss: 10.3250\n",
      "Epoch #1, Batch #1170, Train Loss: 0.2698, Validation Loss: 1.2202\n",
      "Epoch #1, Batch #1180, Train Loss: 0.0386, Validation Loss: 6.2863\n",
      "Epoch #1, Batch #1190, Train Loss: 0.2399, Validation Loss: 4.5228\n",
      "Epoch #1, Batch #1200, Train Loss: 0.2702, Validation Loss: 4.9240\n",
      "Epoch #1, Batch #1210, Train Loss: 0.8322, Validation Loss: 2.4444\n",
      "Epoch #1, Batch #1220, Train Loss: 0.1424, Validation Loss: 0.0051\n",
      "Epoch #2, Batch #0, Train Loss: 0.0178, Validation Loss: 0.6235\n",
      "Epoch #2, Batch #10, Train Loss: 0.4593, Validation Loss: 1.1080\n",
      "Epoch #2, Batch #20, Train Loss: 0.1281, Validation Loss: 1.5202\n",
      "Epoch #2, Batch #30, Train Loss: 0.0836, Validation Loss: 6.7916\n",
      "Epoch #2, Batch #40, Train Loss: 0.0449, Validation Loss: 6.9718\n",
      "Epoch #2, Batch #50, Train Loss: 0.1843, Validation Loss: 6.5418\n",
      "Epoch #2, Batch #60, Train Loss: 0.3870, Validation Loss: 0.9716\n",
      "Epoch #2, Batch #70, Train Loss: 0.0879, Validation Loss: 5.8455\n",
      "Epoch #2, Batch #80, Train Loss: 0.5134, Validation Loss: 8.1446\n",
      "Epoch #2, Batch #90, Train Loss: 0.0372, Validation Loss: 1.1111\n",
      "Epoch #2, Batch #100, Train Loss: 0.0678, Validation Loss: 1.6282\n",
      "Epoch #2, Batch #110, Train Loss: 0.3418, Validation Loss: 2.3886\n",
      "Epoch #2, Batch #120, Train Loss: 0.1557, Validation Loss: 0.4568\n",
      "Epoch #2, Batch #130, Train Loss: 0.0273, Validation Loss: 0.5738\n",
      "Epoch #2, Batch #140, Train Loss: 0.0660, Validation Loss: 6.7900\n",
      "Epoch #2, Batch #150, Train Loss: 0.1369, Validation Loss: 1.1649\n",
      "Epoch #2, Batch #160, Train Loss: 0.0618, Validation Loss: 6.6765\n",
      "Epoch #2, Batch #170, Train Loss: 0.2254, Validation Loss: 2.8408\n",
      "Epoch #2, Batch #180, Train Loss: 0.0613, Validation Loss: 1.7422\n",
      "Epoch #2, Batch #190, Train Loss: 0.1433, Validation Loss: 0.5381\n",
      "Epoch #2, Batch #200, Train Loss: 0.3762, Validation Loss: 0.9093\n",
      "Epoch #2, Batch #210, Train Loss: 0.0454, Validation Loss: 1.9294\n",
      "Epoch #2, Batch #220, Train Loss: 0.0190, Validation Loss: 0.3987\n",
      "Epoch #2, Batch #230, Train Loss: 0.0497, Validation Loss: 1.9136\n",
      "Epoch #2, Batch #240, Train Loss: 0.2474, Validation Loss: 1.3251\n",
      "Epoch #2, Batch #250, Train Loss: 0.1333, Validation Loss: 2.6551\n",
      "Epoch #2, Batch #260, Train Loss: 0.1120, Validation Loss: 6.3406\n",
      "Epoch #2, Batch #270, Train Loss: 0.0421, Validation Loss: 0.2999\n",
      "Epoch #2, Batch #280, Train Loss: 0.0768, Validation Loss: 1.1335\n",
      "Epoch #2, Batch #290, Train Loss: 0.0084, Validation Loss: 5.0170\n",
      "Epoch #2, Batch #300, Train Loss: 0.0894, Validation Loss: 3.5402\n",
      "Epoch #2, Batch #310, Train Loss: 0.1155, Validation Loss: 1.2564\n",
      "Epoch #2, Batch #320, Train Loss: 0.6259, Validation Loss: 10.9775\n",
      "Epoch #2, Batch #330, Train Loss: 0.1400, Validation Loss: 6.0468\n",
      "Epoch #2, Batch #340, Train Loss: 0.0262, Validation Loss: 10.9426\n",
      "Epoch #2, Batch #350, Train Loss: 0.0082, Validation Loss: 0.1981\n",
      "Epoch #2, Batch #360, Train Loss: 0.2308, Validation Loss: 1.3140\n",
      "Epoch #2, Batch #370, Train Loss: 0.1368, Validation Loss: 0.6591\n",
      "Epoch #2, Batch #380, Train Loss: 0.0071, Validation Loss: 2.7057\n",
      "Epoch #2, Batch #390, Train Loss: 0.0269, Validation Loss: 5.3554\n",
      "Epoch #2, Batch #400, Train Loss: 0.0751, Validation Loss: 1.1163\n",
      "Epoch #2, Batch #410, Train Loss: 0.0221, Validation Loss: 13.2303\n",
      "Epoch #2, Batch #420, Train Loss: 0.1471, Validation Loss: 0.9520\n",
      "Epoch #2, Batch #430, Train Loss: 0.0366, Validation Loss: 6.0206\n",
      "Epoch #2, Batch #440, Train Loss: 0.0785, Validation Loss: 8.8319\n",
      "Epoch #2, Batch #450, Train Loss: 0.3211, Validation Loss: 2.7815\n",
      "Epoch #2, Batch #460, Train Loss: 0.0213, Validation Loss: 0.8324\n",
      "Epoch #2, Batch #470, Train Loss: 0.0759, Validation Loss: 1.0692\n",
      "Epoch #2, Batch #480, Train Loss: 0.0292, Validation Loss: 0.5458\n",
      "Epoch #2, Batch #490, Train Loss: 0.0731, Validation Loss: 1.8173\n",
      "Epoch #2, Batch #500, Train Loss: 0.0587, Validation Loss: 0.7623\n",
      "Epoch #2, Batch #510, Train Loss: 0.9651, Validation Loss: 0.1388\n",
      "Epoch #2, Batch #520, Train Loss: 0.0881, Validation Loss: 0.3471\n",
      "Epoch #2, Batch #530, Train Loss: 0.1636, Validation Loss: 5.1121\n",
      "Epoch #2, Batch #540, Train Loss: 0.1630, Validation Loss: 0.6137\n",
      "Epoch #2, Batch #550, Train Loss: 0.0009, Validation Loss: 12.1073\n",
      "Epoch #2, Batch #560, Train Loss: 0.0011, Validation Loss: 1.3756\n",
      "Epoch #2, Batch #570, Train Loss: 0.7765, Validation Loss: 0.8016\n",
      "Epoch #2, Batch #580, Train Loss: 0.2444, Validation Loss: 5.7838\n",
      "Epoch #2, Batch #590, Train Loss: 0.1241, Validation Loss: 0.0073\n",
      "Epoch #2, Batch #600, Train Loss: 0.3334, Validation Loss: 3.9608\n",
      "Epoch #2, Batch #610, Train Loss: 0.0256, Validation Loss: 6.4854\n",
      "Epoch #2, Batch #620, Train Loss: 0.1639, Validation Loss: 1.5187\n",
      "Epoch #2, Batch #630, Train Loss: 0.0185, Validation Loss: 0.9980\n",
      "Epoch #2, Batch #640, Train Loss: 0.2600, Validation Loss: 1.2137\n",
      "Epoch #2, Batch #650, Train Loss: 0.4011, Validation Loss: 7.5190\n",
      "Epoch #2, Batch #660, Train Loss: 0.0227, Validation Loss: 1.2895\n",
      "Epoch #2, Batch #670, Train Loss: 0.1157, Validation Loss: 1.4781\n",
      "Epoch #2, Batch #680, Train Loss: 0.0463, Validation Loss: 2.4985\n",
      "Epoch #2, Batch #690, Train Loss: 1.1871, Validation Loss: 10.0279\n",
      "Epoch #2, Batch #700, Train Loss: 0.0418, Validation Loss: 6.7758\n",
      "Epoch #2, Batch #710, Train Loss: 0.2819, Validation Loss: 2.9517\n",
      "Epoch #2, Batch #720, Train Loss: 0.0693, Validation Loss: 8.7254\n",
      "Epoch #2, Batch #730, Train Loss: 0.0978, Validation Loss: 10.0601\n",
      "Epoch #2, Batch #740, Train Loss: 0.1238, Validation Loss: 0.5776\n",
      "Epoch #2, Batch #750, Train Loss: 0.0368, Validation Loss: 0.1656\n",
      "Epoch #2, Batch #760, Train Loss: 0.0359, Validation Loss: 1.3380\n",
      "Epoch #2, Batch #770, Train Loss: 0.0875, Validation Loss: 5.7214\n",
      "Epoch #2, Batch #780, Train Loss: 0.0077, Validation Loss: 1.9018\n",
      "Epoch #2, Batch #790, Train Loss: 0.0146, Validation Loss: 1.0150\n",
      "Epoch #2, Batch #800, Train Loss: 0.0527, Validation Loss: 0.7730\n",
      "Epoch #2, Batch #810, Train Loss: 1.4714, Validation Loss: 23.8042\n",
      "Epoch #2, Batch #820, Train Loss: 0.0078, Validation Loss: 2.9489\n",
      "Epoch #2, Batch #830, Train Loss: 0.0625, Validation Loss: 3.5057\n",
      "Epoch #2, Batch #840, Train Loss: 0.1437, Validation Loss: 0.8492\n",
      "Epoch #2, Batch #850, Train Loss: 0.0560, Validation Loss: 1.0606\n",
      "Epoch #2, Batch #860, Train Loss: 0.0443, Validation Loss: 4.6667\n",
      "Epoch #2, Batch #870, Train Loss: 0.3896, Validation Loss: 0.3867\n",
      "Epoch #2, Batch #880, Train Loss: 0.0702, Validation Loss: 1.6978\n",
      "Epoch #2, Batch #890, Train Loss: 0.0595, Validation Loss: 1.6786\n",
      "Epoch #2, Batch #900, Train Loss: 0.0376, Validation Loss: 8.0322\n",
      "Epoch #2, Batch #910, Train Loss: 0.0425, Validation Loss: 2.2724\n",
      "Epoch #2, Batch #920, Train Loss: 0.2650, Validation Loss: 3.0751\n",
      "Epoch #2, Batch #930, Train Loss: 0.0490, Validation Loss: 1.1252\n",
      "Epoch #2, Batch #940, Train Loss: 0.0106, Validation Loss: 1.6734\n",
      "Epoch #2, Batch #950, Train Loss: 0.4145, Validation Loss: 7.4029\n",
      "Epoch #2, Batch #960, Train Loss: 0.0515, Validation Loss: 0.0136\n",
      "Epoch #2, Batch #970, Train Loss: 0.0580, Validation Loss: 2.0813\n",
      "Epoch #2, Batch #980, Train Loss: 0.3759, Validation Loss: 0.8190\n",
      "Epoch #2, Batch #990, Train Loss: 0.0170, Validation Loss: 1.3591\n",
      "Epoch #2, Batch #1000, Train Loss: 0.0260, Validation Loss: 3.0894\n",
      "Epoch #2, Batch #1010, Train Loss: 0.1660, Validation Loss: 0.2213\n",
      "Epoch #2, Batch #1020, Train Loss: 0.0561, Validation Loss: 14.3935\n",
      "Epoch #2, Batch #1030, Train Loss: 0.1197, Validation Loss: 0.9009\n",
      "Epoch #2, Batch #1040, Train Loss: 0.4896, Validation Loss: 5.5288\n",
      "Epoch #2, Batch #1050, Train Loss: 0.0270, Validation Loss: 1.1209\n",
      "Epoch #2, Batch #1060, Train Loss: 0.0685, Validation Loss: 2.0752\n",
      "Epoch #2, Batch #1070, Train Loss: 0.2027, Validation Loss: 1.1055\n",
      "Epoch #2, Batch #1080, Train Loss: 0.0858, Validation Loss: 0.9457\n",
      "Epoch #2, Batch #1090, Train Loss: 0.2887, Validation Loss: 1.7097\n",
      "Epoch #2, Batch #1100, Train Loss: 0.1264, Validation Loss: 7.9060\n",
      "Epoch #2, Batch #1110, Train Loss: 0.0776, Validation Loss: 1.3868\n",
      "Epoch #2, Batch #1120, Train Loss: 0.1686, Validation Loss: 1.8594\n",
      "Epoch #2, Batch #1130, Train Loss: 0.1089, Validation Loss: 3.0469\n",
      "Epoch #2, Batch #1140, Train Loss: 0.1490, Validation Loss: 0.0618\n",
      "Epoch #2, Batch #1150, Train Loss: 0.3607, Validation Loss: 0.6982\n",
      "Epoch #2, Batch #1160, Train Loss: 0.1250, Validation Loss: 10.7613\n",
      "Epoch #2, Batch #1170, Train Loss: 0.4222, Validation Loss: 12.1217\n",
      "Epoch #2, Batch #1180, Train Loss: 0.0360, Validation Loss: 1.5450\n",
      "Epoch #2, Batch #1190, Train Loss: 0.0875, Validation Loss: 1.0093\n",
      "Epoch #2, Batch #1200, Train Loss: 0.1411, Validation Loss: 0.9771\n",
      "Epoch #2, Batch #1210, Train Loss: 0.0578, Validation Loss: 5.3944\n",
      "Epoch #2, Batch #1220, Train Loss: 0.0643, Validation Loss: 0.8879\n",
      "Epoch #3, Batch #0, Train Loss: 0.0304, Validation Loss: 0.9517\n",
      "Epoch #3, Batch #10, Train Loss: 0.2895, Validation Loss: 4.3398\n",
      "Epoch #3, Batch #20, Train Loss: 0.0663, Validation Loss: 1.4288\n",
      "Epoch #3, Batch #30, Train Loss: 0.0053, Validation Loss: 1.2075\n",
      "Epoch #3, Batch #40, Train Loss: 0.3145, Validation Loss: 2.8133\n",
      "Epoch #3, Batch #50, Train Loss: 0.1239, Validation Loss: 1.9488\n",
      "Epoch #3, Batch #60, Train Loss: 0.0356, Validation Loss: 1.6935\n",
      "Epoch #3, Batch #70, Train Loss: 0.2628, Validation Loss: 3.6026\n",
      "Epoch #3, Batch #80, Train Loss: 0.0407, Validation Loss: 1.8547\n",
      "Epoch #3, Batch #90, Train Loss: 0.0486, Validation Loss: 4.6906\n",
      "Epoch #3, Batch #100, Train Loss: 0.1390, Validation Loss: 1.1993\n",
      "Epoch #3, Batch #110, Train Loss: 0.1012, Validation Loss: 8.2074\n",
      "Epoch #3, Batch #120, Train Loss: 0.0307, Validation Loss: 10.3193\n",
      "Epoch #3, Batch #130, Train Loss: 0.0726, Validation Loss: 0.8960\n",
      "Epoch #3, Batch #140, Train Loss: 0.0091, Validation Loss: 0.2992\n",
      "Epoch #3, Batch #150, Train Loss: 0.1136, Validation Loss: 0.2077\n",
      "Epoch #3, Batch #160, Train Loss: 1.1265, Validation Loss: 0.0687\n",
      "Epoch #3, Batch #170, Train Loss: 0.8029, Validation Loss: 1.0068\n",
      "Epoch #3, Batch #180, Train Loss: 0.0872, Validation Loss: 3.2135\n",
      "Epoch #3, Batch #190, Train Loss: 0.0091, Validation Loss: 1.6440\n",
      "Epoch #3, Batch #200, Train Loss: 0.0317, Validation Loss: 0.3455\n",
      "Epoch #3, Batch #210, Train Loss: 0.1267, Validation Loss: 9.9726\n",
      "Epoch #3, Batch #220, Train Loss: 0.0554, Validation Loss: 2.5516\n",
      "Epoch #3, Batch #230, Train Loss: 0.6091, Validation Loss: 7.3258\n",
      "Epoch #3, Batch #240, Train Loss: 0.0979, Validation Loss: 0.2476\n",
      "Epoch #3, Batch #250, Train Loss: 0.0711, Validation Loss: 0.3438\n",
      "Epoch #3, Batch #260, Train Loss: 0.0199, Validation Loss: 10.1518\n",
      "Epoch #3, Batch #270, Train Loss: 0.3804, Validation Loss: 12.8066\n",
      "Epoch #3, Batch #280, Train Loss: 0.0863, Validation Loss: 6.6129\n",
      "Epoch #3, Batch #290, Train Loss: 0.4323, Validation Loss: 3.4434\n",
      "Epoch #3, Batch #300, Train Loss: 0.0341, Validation Loss: 12.3089\n",
      "Epoch #3, Batch #310, Train Loss: 0.0047, Validation Loss: 0.1601\n",
      "Epoch #3, Batch #320, Train Loss: 0.3868, Validation Loss: 4.8798\n",
      "Epoch #3, Batch #330, Train Loss: 0.1646, Validation Loss: 0.9982\n",
      "Epoch #3, Batch #340, Train Loss: 0.0189, Validation Loss: 0.5243\n",
      "Epoch #3, Batch #350, Train Loss: 0.1180, Validation Loss: 0.6961\n",
      "Epoch #3, Batch #360, Train Loss: 0.1122, Validation Loss: 2.0459\n",
      "Epoch #3, Batch #370, Train Loss: 0.0971, Validation Loss: 0.9516\n",
      "Epoch #3, Batch #380, Train Loss: 0.0715, Validation Loss: 2.2261\n",
      "Epoch #3, Batch #390, Train Loss: 0.1492, Validation Loss: 0.2053\n",
      "Epoch #3, Batch #400, Train Loss: 0.1935, Validation Loss: 2.7587\n",
      "Epoch #3, Batch #410, Train Loss: 0.1602, Validation Loss: 2.3378\n",
      "Epoch #3, Batch #420, Train Loss: 0.0744, Validation Loss: 2.8931\n",
      "Epoch #3, Batch #430, Train Loss: 0.1016, Validation Loss: 1.1860\n",
      "Epoch #3, Batch #440, Train Loss: 0.1582, Validation Loss: 1.2603\n",
      "Epoch #3, Batch #450, Train Loss: 0.0720, Validation Loss: 0.6347\n",
      "Epoch #3, Batch #460, Train Loss: 0.0517, Validation Loss: 1.2685\n",
      "Epoch #3, Batch #470, Train Loss: 0.1031, Validation Loss: 4.7770\n",
      "Epoch #3, Batch #480, Train Loss: 0.1241, Validation Loss: 1.4064\n",
      "Epoch #3, Batch #490, Train Loss: 0.0185, Validation Loss: 11.5909\n",
      "Epoch #3, Batch #500, Train Loss: 0.0589, Validation Loss: 2.3652\n",
      "Epoch #3, Batch #510, Train Loss: 0.1804, Validation Loss: 2.5056\n",
      "Epoch #3, Batch #520, Train Loss: 0.0628, Validation Loss: 2.9085\n",
      "Epoch #3, Batch #530, Train Loss: 0.0289, Validation Loss: 0.1220\n",
      "Epoch #3, Batch #540, Train Loss: 0.0801, Validation Loss: 2.8729\n",
      "Epoch #3, Batch #550, Train Loss: 0.0775, Validation Loss: 0.8366\n",
      "Epoch #3, Batch #560, Train Loss: 0.1164, Validation Loss: 0.8823\n",
      "Epoch #3, Batch #570, Train Loss: 0.1080, Validation Loss: 1.9582\n",
      "Epoch #3, Batch #580, Train Loss: 0.1035, Validation Loss: 1.9555\n",
      "Epoch #3, Batch #590, Train Loss: 0.1245, Validation Loss: 9.5034\n",
      "Epoch #3, Batch #600, Train Loss: 0.0287, Validation Loss: 0.8557\n",
      "Epoch #3, Batch #610, Train Loss: 0.0319, Validation Loss: 2.4326\n",
      "Epoch #3, Batch #620, Train Loss: 0.0392, Validation Loss: 4.2472\n",
      "Epoch #3, Batch #630, Train Loss: 0.0243, Validation Loss: 0.0441\n",
      "Epoch #3, Batch #640, Train Loss: 0.1211, Validation Loss: 1.5951\n",
      "Epoch #3, Batch #650, Train Loss: 0.0521, Validation Loss: 3.9792\n",
      "Epoch #3, Batch #660, Train Loss: 0.0950, Validation Loss: 4.1030\n",
      "Epoch #3, Batch #670, Train Loss: 0.0882, Validation Loss: 5.8322\n",
      "Epoch #3, Batch #680, Train Loss: 0.0689, Validation Loss: 1.7917\n",
      "Epoch #3, Batch #690, Train Loss: 0.1734, Validation Loss: 0.8723\n",
      "Epoch #3, Batch #700, Train Loss: 0.1683, Validation Loss: 0.5549\n",
      "Epoch #3, Batch #710, Train Loss: 0.0742, Validation Loss: 1.8813\n",
      "Epoch #3, Batch #720, Train Loss: 0.3530, Validation Loss: 2.2861\n",
      "Epoch #3, Batch #730, Train Loss: 0.2042, Validation Loss: 5.0906\n",
      "Epoch #3, Batch #740, Train Loss: 0.0890, Validation Loss: 1.1823\n",
      "Epoch #3, Batch #750, Train Loss: 1.2952, Validation Loss: 0.0064\n",
      "Epoch #3, Batch #760, Train Loss: 0.0549, Validation Loss: 7.8153\n",
      "Epoch #3, Batch #770, Train Loss: 0.2121, Validation Loss: 1.7545\n",
      "Epoch #3, Batch #780, Train Loss: 0.1870, Validation Loss: 1.2178\n",
      "Epoch #3, Batch #790, Train Loss: 0.0268, Validation Loss: 1.8664\n",
      "Epoch #3, Batch #800, Train Loss: 0.0250, Validation Loss: 1.2589\n",
      "Epoch #3, Batch #810, Train Loss: 0.1517, Validation Loss: 1.7518\n",
      "Epoch #3, Batch #820, Train Loss: 0.0055, Validation Loss: 2.3953\n",
      "Epoch #3, Batch #830, Train Loss: 0.0924, Validation Loss: 1.5791\n",
      "Epoch #3, Batch #840, Train Loss: 0.0757, Validation Loss: 9.7920\n",
      "Epoch #3, Batch #850, Train Loss: 0.0557, Validation Loss: 1.2188\n",
      "Epoch #3, Batch #860, Train Loss: 0.0251, Validation Loss: 1.9929\n",
      "Epoch #3, Batch #870, Train Loss: 0.0003, Validation Loss: 1.2185\n",
      "Epoch #3, Batch #880, Train Loss: 0.3363, Validation Loss: 0.4157\n",
      "Epoch #3, Batch #890, Train Loss: 0.2119, Validation Loss: 0.5355\n",
      "Epoch #3, Batch #900, Train Loss: 0.0673, Validation Loss: 0.9703\n",
      "Epoch #3, Batch #910, Train Loss: 0.1188, Validation Loss: 4.7855\n",
      "Epoch #3, Batch #920, Train Loss: 0.1412, Validation Loss: 1.1993\n",
      "Epoch #3, Batch #930, Train Loss: 0.0647, Validation Loss: 13.8483\n",
      "Epoch #3, Batch #940, Train Loss: 0.0652, Validation Loss: 1.3380\n",
      "Epoch #3, Batch #950, Train Loss: 0.3431, Validation Loss: 0.7222\n",
      "Epoch #3, Batch #960, Train Loss: 0.0251, Validation Loss: 3.0357\n",
      "Epoch #3, Batch #970, Train Loss: 2.8732, Validation Loss: 1.2387\n",
      "Epoch #3, Batch #980, Train Loss: 0.0563, Validation Loss: 0.6396\n",
      "Epoch #3, Batch #990, Train Loss: 0.1853, Validation Loss: 11.2245\n",
      "Epoch #3, Batch #1000, Train Loss: 0.1284, Validation Loss: 2.5946\n",
      "Epoch #3, Batch #1010, Train Loss: 0.0797, Validation Loss: 2.0585\n",
      "Epoch #3, Batch #1020, Train Loss: 0.0492, Validation Loss: 1.1136\n",
      "Epoch #3, Batch #1030, Train Loss: 0.0189, Validation Loss: 4.3528\n",
      "Epoch #3, Batch #1040, Train Loss: 0.0701, Validation Loss: 1.5460\n",
      "Epoch #3, Batch #1050, Train Loss: 0.1471, Validation Loss: 1.3673\n",
      "Epoch #3, Batch #1060, Train Loss: 0.0120, Validation Loss: 11.2739\n",
      "Epoch #3, Batch #1070, Train Loss: 0.2281, Validation Loss: 2.5147\n",
      "Epoch #3, Batch #1080, Train Loss: 0.0295, Validation Loss: 0.5529\n",
      "Epoch #3, Batch #1090, Train Loss: 0.1004, Validation Loss: 0.1306\n",
      "Epoch #3, Batch #1100, Train Loss: 0.0110, Validation Loss: 14.0601\n",
      "Epoch #3, Batch #1110, Train Loss: 0.1106, Validation Loss: 0.1040\n",
      "Epoch #3, Batch #1120, Train Loss: 0.0075, Validation Loss: 13.8671\n",
      "Epoch #3, Batch #1130, Train Loss: 0.1071, Validation Loss: 1.3845\n",
      "Epoch #3, Batch #1140, Train Loss: 0.1641, Validation Loss: 10.3088\n",
      "Epoch #3, Batch #1150, Train Loss: 0.0738, Validation Loss: 5.9251\n",
      "Epoch #3, Batch #1160, Train Loss: 0.2971, Validation Loss: 1.8870\n",
      "Epoch #3, Batch #1170, Train Loss: 0.0488, Validation Loss: 0.0513\n",
      "Epoch #3, Batch #1180, Train Loss: 0.1446, Validation Loss: 2.5457\n",
      "Epoch #3, Batch #1190, Train Loss: 0.0355, Validation Loss: 7.6180\n",
      "Epoch #3, Batch #1200, Train Loss: 0.2424, Validation Loss: 1.4174\n",
      "Epoch #3, Batch #1210, Train Loss: 0.0895, Validation Loss: 1.9445\n",
      "Epoch #3, Batch #1220, Train Loss: 0.5357, Validation Loss: 0.2346\n",
      "Epoch #4, Batch #0, Train Loss: 0.0911, Validation Loss: 0.4805\n",
      "Epoch #4, Batch #10, Train Loss: 0.9965, Validation Loss: 3.3798\n",
      "Epoch #4, Batch #20, Train Loss: 0.0303, Validation Loss: 1.1929\n",
      "Epoch #4, Batch #30, Train Loss: 0.1797, Validation Loss: 0.6554\n",
      "Epoch #4, Batch #40, Train Loss: 0.2067, Validation Loss: 4.8113\n",
      "Epoch #4, Batch #50, Train Loss: 0.0219, Validation Loss: 6.8982\n",
      "Epoch #4, Batch #60, Train Loss: 0.0784, Validation Loss: 0.8668\n",
      "Epoch #4, Batch #70, Train Loss: 0.0123, Validation Loss: 3.4362\n",
      "Epoch #4, Batch #80, Train Loss: 0.0478, Validation Loss: 0.5393\n",
      "Epoch #4, Batch #90, Train Loss: 0.1259, Validation Loss: 8.8182\n",
      "Epoch #4, Batch #100, Train Loss: 0.0963, Validation Loss: 3.6547\n",
      "Epoch #4, Batch #110, Train Loss: 0.0960, Validation Loss: 2.4605\n",
      "Epoch #4, Batch #120, Train Loss: 0.0102, Validation Loss: 1.5922\n",
      "Epoch #4, Batch #130, Train Loss: 0.0191, Validation Loss: 4.7020\n",
      "Epoch #4, Batch #140, Train Loss: 0.1544, Validation Loss: 4.0249\n",
      "Epoch #4, Batch #150, Train Loss: 0.0502, Validation Loss: 6.4682\n",
      "Epoch #4, Batch #160, Train Loss: 0.0469, Validation Loss: 5.9192\n",
      "Epoch #4, Batch #170, Train Loss: 0.0301, Validation Loss: 13.1241\n",
      "Epoch #4, Batch #180, Train Loss: 0.0704, Validation Loss: 0.9004\n",
      "Epoch #4, Batch #190, Train Loss: 0.0423, Validation Loss: 4.6310\n",
      "Epoch #4, Batch #200, Train Loss: 0.0811, Validation Loss: 1.5326\n",
      "Epoch #4, Batch #210, Train Loss: 0.0063, Validation Loss: 1.0969\n",
      "Epoch #4, Batch #220, Train Loss: 0.0092, Validation Loss: 0.7197\n",
      "Epoch #4, Batch #230, Train Loss: 0.9868, Validation Loss: 9.3993\n",
      "Epoch #4, Batch #240, Train Loss: 0.2738, Validation Loss: 1.0656\n",
      "Epoch #4, Batch #250, Train Loss: 0.0670, Validation Loss: 9.4290\n",
      "Epoch #4, Batch #260, Train Loss: 0.2106, Validation Loss: 10.0663\n",
      "Epoch #4, Batch #270, Train Loss: 0.0519, Validation Loss: 0.2327\n",
      "Epoch #4, Batch #280, Train Loss: 0.0323, Validation Loss: 0.2578\n",
      "Epoch #4, Batch #290, Train Loss: 0.0926, Validation Loss: 0.0216\n",
      "Epoch #4, Batch #300, Train Loss: 0.1908, Validation Loss: 2.4515\n",
      "Epoch #4, Batch #310, Train Loss: 0.1458, Validation Loss: 0.0635\n",
      "Epoch #4, Batch #320, Train Loss: 0.0946, Validation Loss: 0.4679\n",
      "Epoch #4, Batch #330, Train Loss: 0.1078, Validation Loss: 1.3268\n",
      "Epoch #4, Batch #340, Train Loss: 0.1465, Validation Loss: 1.4057\n",
      "Epoch #4, Batch #350, Train Loss: 0.1030, Validation Loss: 0.1248\n",
      "Epoch #4, Batch #360, Train Loss: 0.0209, Validation Loss: 0.3171\n",
      "Epoch #4, Batch #370, Train Loss: 0.1951, Validation Loss: 11.8450\n",
      "Epoch #4, Batch #380, Train Loss: 0.4179, Validation Loss: 0.0386\n",
      "Epoch #4, Batch #390, Train Loss: 0.1480, Validation Loss: 1.1121\n",
      "Epoch #4, Batch #400, Train Loss: 0.0146, Validation Loss: 1.8214\n",
      "Epoch #4, Batch #410, Train Loss: 0.1639, Validation Loss: 10.4363\n",
      "Epoch #4, Batch #420, Train Loss: 0.0214, Validation Loss: 0.2044\n",
      "Epoch #4, Batch #430, Train Loss: 0.2598, Validation Loss: 1.9375\n",
      "Epoch #4, Batch #440, Train Loss: 0.2269, Validation Loss: 0.1663\n",
      "Epoch #4, Batch #450, Train Loss: 0.0427, Validation Loss: 0.5949\n",
      "Epoch #4, Batch #460, Train Loss: 0.0865, Validation Loss: 1.7140\n",
      "Epoch #4, Batch #470, Train Loss: 0.4428, Validation Loss: 2.4949\n",
      "Epoch #4, Batch #480, Train Loss: 0.0125, Validation Loss: 6.5500\n",
      "Epoch #4, Batch #490, Train Loss: 0.7669, Validation Loss: 0.3869\n",
      "Epoch #4, Batch #500, Train Loss: 0.0475, Validation Loss: 1.5182\n",
      "Epoch #4, Batch #510, Train Loss: 0.0216, Validation Loss: 1.1063\n",
      "Epoch #4, Batch #520, Train Loss: 1.1330, Validation Loss: 3.1291\n",
      "Epoch #4, Batch #530, Train Loss: 0.0471, Validation Loss: 0.8107\n",
      "Epoch #4, Batch #540, Train Loss: 0.0959, Validation Loss: 1.8711\n",
      "Epoch #4, Batch #550, Train Loss: 0.0279, Validation Loss: 0.0277\n",
      "Epoch #4, Batch #560, Train Loss: 0.1345, Validation Loss: 6.1728\n",
      "Epoch #4, Batch #570, Train Loss: 0.0449, Validation Loss: 5.0299\n",
      "Epoch #4, Batch #580, Train Loss: 0.0230, Validation Loss: 1.0913\n",
      "Epoch #4, Batch #590, Train Loss: 0.1237, Validation Loss: 1.0213\n",
      "Epoch #4, Batch #600, Train Loss: 0.1935, Validation Loss: 2.2583\n",
      "Epoch #4, Batch #610, Train Loss: 0.1449, Validation Loss: 10.9490\n",
      "Epoch #4, Batch #620, Train Loss: 0.1371, Validation Loss: 3.0822\n",
      "Epoch #4, Batch #630, Train Loss: 0.0270, Validation Loss: 7.9302\n",
      "Epoch #4, Batch #640, Train Loss: 0.1779, Validation Loss: 1.2161\n",
      "Epoch #4, Batch #650, Train Loss: 0.1049, Validation Loss: 5.0872\n",
      "Epoch #4, Batch #660, Train Loss: 0.0720, Validation Loss: 0.0266\n",
      "Epoch #4, Batch #670, Train Loss: 0.1111, Validation Loss: 1.8625\n",
      "Epoch #4, Batch #680, Train Loss: 0.1192, Validation Loss: 4.7367\n",
      "Epoch #4, Batch #690, Train Loss: 0.1125, Validation Loss: 0.1533\n",
      "Epoch #4, Batch #700, Train Loss: 0.0963, Validation Loss: 1.4259\n",
      "Epoch #4, Batch #710, Train Loss: 0.4705, Validation Loss: 1.1384\n",
      "Epoch #4, Batch #720, Train Loss: 0.0332, Validation Loss: 1.7133\n",
      "Epoch #4, Batch #730, Train Loss: 0.2075, Validation Loss: 0.5862\n",
      "Epoch #4, Batch #740, Train Loss: 0.0836, Validation Loss: 1.9020\n",
      "Epoch #4, Batch #750, Train Loss: 0.1849, Validation Loss: 0.8020\n",
      "Epoch #4, Batch #760, Train Loss: 0.0149, Validation Loss: 0.7883\n",
      "Epoch #4, Batch #770, Train Loss: 0.1637, Validation Loss: 1.4405\n",
      "Epoch #4, Batch #780, Train Loss: 0.0054, Validation Loss: 3.9427\n",
      "Epoch #4, Batch #790, Train Loss: 0.0420, Validation Loss: 1.4125\n",
      "Epoch #4, Batch #800, Train Loss: 0.4266, Validation Loss: 0.1807\n",
      "Epoch #4, Batch #810, Train Loss: 0.0951, Validation Loss: 7.8123\n",
      "Epoch #4, Batch #820, Train Loss: 0.3742, Validation Loss: 2.5040\n",
      "Epoch #4, Batch #830, Train Loss: 0.1243, Validation Loss: 6.9401\n",
      "Epoch #4, Batch #840, Train Loss: 0.0266, Validation Loss: 1.2766\n",
      "Epoch #4, Batch #850, Train Loss: 0.2043, Validation Loss: 6.1065\n",
      "Epoch #4, Batch #860, Train Loss: 0.2165, Validation Loss: 2.8060\n",
      "Epoch #4, Batch #870, Train Loss: 0.3336, Validation Loss: 9.6152\n",
      "Epoch #4, Batch #880, Train Loss: 0.0191, Validation Loss: 0.3885\n",
      "Epoch #4, Batch #890, Train Loss: 0.0533, Validation Loss: 0.9813\n",
      "Epoch #4, Batch #900, Train Loss: 0.0144, Validation Loss: 6.1728\n",
      "Epoch #4, Batch #910, Train Loss: 0.0210, Validation Loss: 0.7120\n",
      "Epoch #4, Batch #920, Train Loss: 0.0581, Validation Loss: 0.9792\n",
      "Epoch #4, Batch #930, Train Loss: 0.0868, Validation Loss: 1.1787\n",
      "Epoch #4, Batch #940, Train Loss: 0.1571, Validation Loss: 1.3904\n",
      "Epoch #4, Batch #950, Train Loss: 0.0723, Validation Loss: 1.0996\n",
      "Epoch #4, Batch #960, Train Loss: 0.0829, Validation Loss: 0.4885\n",
      "Epoch #4, Batch #970, Train Loss: 0.1462, Validation Loss: 0.8144\n",
      "Epoch #4, Batch #980, Train Loss: 0.2944, Validation Loss: 0.4223\n",
      "Epoch #4, Batch #990, Train Loss: 0.2901, Validation Loss: 11.3904\n",
      "Epoch #4, Batch #1000, Train Loss: 0.2128, Validation Loss: 0.5363\n",
      "Epoch #4, Batch #1010, Train Loss: 0.0493, Validation Loss: 5.2927\n",
      "Epoch #4, Batch #1020, Train Loss: 0.0301, Validation Loss: 0.9042\n",
      "Epoch #4, Batch #1030, Train Loss: 0.0090, Validation Loss: 2.7030\n",
      "Epoch #4, Batch #1040, Train Loss: 0.9250, Validation Loss: 0.6083\n",
      "Epoch #4, Batch #1050, Train Loss: 0.0524, Validation Loss: 0.2503\n",
      "Epoch #4, Batch #1060, Train Loss: 0.0700, Validation Loss: 1.0376\n",
      "Epoch #4, Batch #1070, Train Loss: 0.0882, Validation Loss: 2.4726\n",
      "Epoch #4, Batch #1080, Train Loss: 1.8903, Validation Loss: 9.1494\n",
      "Epoch #4, Batch #1090, Train Loss: 0.3427, Validation Loss: 0.4652\n",
      "Epoch #4, Batch #1100, Train Loss: 0.3303, Validation Loss: 8.1452\n",
      "Epoch #4, Batch #1110, Train Loss: 0.0200, Validation Loss: 1.3581\n",
      "Epoch #4, Batch #1120, Train Loss: 0.1409, Validation Loss: 6.1616\n",
      "Epoch #4, Batch #1130, Train Loss: 0.0323, Validation Loss: 4.3924\n",
      "Epoch #4, Batch #1140, Train Loss: 0.0266, Validation Loss: 2.3956\n",
      "Epoch #4, Batch #1150, Train Loss: 0.1761, Validation Loss: 1.8303\n",
      "Epoch #4, Batch #1160, Train Loss: 0.3929, Validation Loss: 0.8092\n",
      "Epoch #4, Batch #1170, Train Loss: 0.1924, Validation Loss: 0.2780\n",
      "Epoch #4, Batch #1180, Train Loss: 0.0118, Validation Loss: 5.4992\n",
      "Epoch #4, Batch #1190, Train Loss: 0.0651, Validation Loss: 4.7850\n",
      "Epoch #4, Batch #1200, Train Loss: 0.1404, Validation Loss: 0.1870\n",
      "Epoch #4, Batch #1210, Train Loss: 0.0137, Validation Loss: 0.0410\n",
      "Epoch #4, Batch #1220, Train Loss: 0.0185, Validation Loss: 15.2306\n",
      "Epoch #5, Batch #0, Train Loss: 0.0505, Validation Loss: 0.5554\n",
      "Epoch #5, Batch #10, Train Loss: 0.0325, Validation Loss: 1.9012\n",
      "Epoch #5, Batch #20, Train Loss: 0.0546, Validation Loss: 11.4260\n",
      "Epoch #5, Batch #30, Train Loss: 0.1866, Validation Loss: 1.0003\n",
      "Epoch #5, Batch #40, Train Loss: 0.0099, Validation Loss: 0.0826\n",
      "Epoch #5, Batch #50, Train Loss: 0.4605, Validation Loss: 2.0704\n",
      "Epoch #5, Batch #60, Train Loss: 0.6858, Validation Loss: 3.3823\n",
      "Epoch #5, Batch #70, Train Loss: 0.6378, Validation Loss: 2.3755\n",
      "Epoch #5, Batch #80, Train Loss: 0.0195, Validation Loss: 2.8608\n",
      "Epoch #5, Batch #90, Train Loss: 0.1310, Validation Loss: 7.4705\n",
      "Epoch #5, Batch #100, Train Loss: 0.0951, Validation Loss: 1.4968\n",
      "Epoch #5, Batch #110, Train Loss: 0.1477, Validation Loss: 6.1273\n",
      "Epoch #5, Batch #120, Train Loss: 0.0692, Validation Loss: 1.9291\n",
      "Epoch #5, Batch #130, Train Loss: 0.4398, Validation Loss: 6.2545\n",
      "Epoch #5, Batch #140, Train Loss: 0.0092, Validation Loss: 1.0141\n",
      "Epoch #5, Batch #150, Train Loss: 0.1002, Validation Loss: 0.7247\n",
      "Epoch #5, Batch #160, Train Loss: 0.1097, Validation Loss: 9.0354\n",
      "Epoch #5, Batch #170, Train Loss: 0.4550, Validation Loss: 1.3983\n",
      "Epoch #5, Batch #180, Train Loss: 0.0153, Validation Loss: 3.6905\n",
      "Epoch #5, Batch #190, Train Loss: 0.2320, Validation Loss: 0.1679\n",
      "Epoch #5, Batch #200, Train Loss: 0.2276, Validation Loss: 0.2601\n",
      "Epoch #5, Batch #210, Train Loss: 0.0558, Validation Loss: 1.6190\n",
      "Epoch #5, Batch #220, Train Loss: 0.1140, Validation Loss: 2.9749\n",
      "Epoch #5, Batch #230, Train Loss: 0.0753, Validation Loss: 7.2720\n",
      "Epoch #5, Batch #240, Train Loss: 0.2942, Validation Loss: 2.0971\n",
      "Epoch #5, Batch #250, Train Loss: 0.0336, Validation Loss: 0.2597\n",
      "Epoch #5, Batch #260, Train Loss: 0.0203, Validation Loss: 2.5544\n",
      "Epoch #5, Batch #270, Train Loss: 0.2959, Validation Loss: 0.9747\n",
      "Epoch #5, Batch #280, Train Loss: 0.1356, Validation Loss: 0.7498\n",
      "Epoch #5, Batch #290, Train Loss: 0.1954, Validation Loss: 7.5124\n",
      "Epoch #5, Batch #300, Train Loss: 0.1383, Validation Loss: 1.5206\n",
      "Epoch #5, Batch #310, Train Loss: 0.1220, Validation Loss: 1.0325\n",
      "Epoch #5, Batch #320, Train Loss: 0.0091, Validation Loss: 2.1311\n",
      "Epoch #5, Batch #330, Train Loss: 0.1866, Validation Loss: 2.9170\n",
      "Epoch #5, Batch #340, Train Loss: 0.0337, Validation Loss: 0.4187\n",
      "Epoch #5, Batch #350, Train Loss: 0.1735, Validation Loss: 3.8192\n",
      "Epoch #5, Batch #360, Train Loss: 0.1007, Validation Loss: 2.3755\n",
      "Epoch #5, Batch #370, Train Loss: 0.0773, Validation Loss: 2.2414\n",
      "Epoch #5, Batch #380, Train Loss: 0.0082, Validation Loss: 2.5531\n",
      "Epoch #5, Batch #390, Train Loss: 0.0690, Validation Loss: 6.4233\n",
      "Epoch #5, Batch #400, Train Loss: 0.0931, Validation Loss: 2.3124\n",
      "Epoch #5, Batch #410, Train Loss: 0.1079, Validation Loss: 1.6170\n",
      "Epoch #5, Batch #420, Train Loss: 0.0659, Validation Loss: 12.8900\n",
      "Epoch #5, Batch #430, Train Loss: 0.0467, Validation Loss: 4.7768\n",
      "Epoch #5, Batch #440, Train Loss: 0.2008, Validation Loss: 0.4630\n",
      "Epoch #5, Batch #450, Train Loss: 0.0607, Validation Loss: 0.3506\n",
      "Epoch #5, Batch #460, Train Loss: 0.2861, Validation Loss: 4.6781\n",
      "Epoch #5, Batch #470, Train Loss: 0.0508, Validation Loss: 0.0688\n",
      "Epoch #5, Batch #480, Train Loss: 0.0391, Validation Loss: 1.3286\n",
      "Epoch #5, Batch #490, Train Loss: 0.1662, Validation Loss: 1.5055\n",
      "Epoch #5, Batch #500, Train Loss: 0.4954, Validation Loss: 4.6366\n",
      "Epoch #5, Batch #510, Train Loss: 0.0182, Validation Loss: 2.5282\n",
      "Epoch #5, Batch #520, Train Loss: 0.2584, Validation Loss: 7.4859\n",
      "Epoch #5, Batch #530, Train Loss: 0.0285, Validation Loss: 0.7065\n",
      "Epoch #5, Batch #540, Train Loss: 0.1234, Validation Loss: 1.9268\n",
      "Epoch #5, Batch #550, Train Loss: 0.0702, Validation Loss: 0.4215\n",
      "Epoch #5, Batch #560, Train Loss: 0.0295, Validation Loss: 1.3149\n",
      "Epoch #5, Batch #570, Train Loss: 0.0266, Validation Loss: 0.0209\n",
      "Epoch #5, Batch #580, Train Loss: 0.1860, Validation Loss: 0.4939\n",
      "Epoch #5, Batch #590, Train Loss: 0.0890, Validation Loss: 0.7739\n",
      "Epoch #5, Batch #600, Train Loss: 0.0899, Validation Loss: 3.3218\n",
      "Epoch #5, Batch #610, Train Loss: 0.2180, Validation Loss: 0.5626\n",
      "Epoch #5, Batch #620, Train Loss: 0.1161, Validation Loss: 0.6513\n",
      "Epoch #5, Batch #630, Train Loss: 0.0454, Validation Loss: 0.2787\n",
      "Epoch #5, Batch #640, Train Loss: 0.1054, Validation Loss: 2.3805\n",
      "Epoch #5, Batch #650, Train Loss: 0.1508, Validation Loss: 2.4971\n",
      "Epoch #5, Batch #660, Train Loss: 0.1843, Validation Loss: 0.4737\n",
      "Epoch #5, Batch #670, Train Loss: 0.0869, Validation Loss: 0.0334\n",
      "Epoch #5, Batch #680, Train Loss: 0.0664, Validation Loss: 1.8068\n",
      "Epoch #5, Batch #690, Train Loss: 0.4941, Validation Loss: 0.1781\n",
      "Epoch #5, Batch #700, Train Loss: 0.4064, Validation Loss: 0.7951\n",
      "Epoch #5, Batch #710, Train Loss: 0.0530, Validation Loss: 0.8717\n",
      "Epoch #5, Batch #720, Train Loss: 0.0762, Validation Loss: 4.7182\n",
      "Epoch #5, Batch #730, Train Loss: 0.1104, Validation Loss: 5.2322\n",
      "Epoch #5, Batch #740, Train Loss: 0.0512, Validation Loss: 0.6943\n",
      "Epoch #5, Batch #750, Train Loss: 0.0375, Validation Loss: 1.1931\n",
      "Epoch #5, Batch #760, Train Loss: 0.0061, Validation Loss: 0.3343\n",
      "Epoch #5, Batch #770, Train Loss: 0.0249, Validation Loss: 5.5945\n",
      "Epoch #5, Batch #780, Train Loss: 0.0436, Validation Loss: 0.1729\n",
      "Epoch #5, Batch #790, Train Loss: 0.0500, Validation Loss: 4.0028\n",
      "Epoch #5, Batch #800, Train Loss: 0.1027, Validation Loss: 0.6545\n",
      "Epoch #5, Batch #810, Train Loss: 0.0526, Validation Loss: 2.7968\n",
      "Epoch #5, Batch #820, Train Loss: 0.0027, Validation Loss: 0.1940\n",
      "Epoch #5, Batch #830, Train Loss: 0.0695, Validation Loss: 0.0889\n",
      "Epoch #5, Batch #840, Train Loss: 0.0830, Validation Loss: 3.6426\n",
      "Epoch #5, Batch #850, Train Loss: 0.1991, Validation Loss: 0.5188\n",
      "Epoch #5, Batch #860, Train Loss: 0.2255, Validation Loss: 0.4097\n",
      "Epoch #5, Batch #870, Train Loss: 0.0212, Validation Loss: 0.9671\n",
      "Epoch #5, Batch #880, Train Loss: 0.0351, Validation Loss: 2.2227\n",
      "Epoch #5, Batch #890, Train Loss: 0.5618, Validation Loss: 1.8743\n",
      "Epoch #5, Batch #900, Train Loss: 0.0575, Validation Loss: 5.9067\n",
      "Epoch #5, Batch #910, Train Loss: 0.0290, Validation Loss: 1.2922\n",
      "Epoch #5, Batch #920, Train Loss: 0.0918, Validation Loss: 0.1402\n",
      "Epoch #5, Batch #930, Train Loss: 0.0421, Validation Loss: 5.7777\n",
      "Epoch #5, Batch #940, Train Loss: 0.0520, Validation Loss: 0.3023\n",
      "Epoch #5, Batch #950, Train Loss: 0.6672, Validation Loss: 5.1279\n",
      "Epoch #5, Batch #960, Train Loss: 0.4790, Validation Loss: 3.4090\n",
      "Epoch #5, Batch #970, Train Loss: 0.0755, Validation Loss: 1.1098\n",
      "Epoch #5, Batch #980, Train Loss: 0.0368, Validation Loss: 0.7342\n",
      "Epoch #5, Batch #990, Train Loss: 0.0432, Validation Loss: 7.8376\n",
      "Epoch #5, Batch #1000, Train Loss: 0.0546, Validation Loss: 1.0622\n",
      "Epoch #5, Batch #1010, Train Loss: 0.0408, Validation Loss: 8.4954\n",
      "Epoch #5, Batch #1020, Train Loss: 0.0477, Validation Loss: 4.9974\n",
      "Epoch #5, Batch #1030, Train Loss: 0.0093, Validation Loss: 2.1947\n",
      "Epoch #5, Batch #1040, Train Loss: 0.0013, Validation Loss: 0.7627\n",
      "Epoch #5, Batch #1050, Train Loss: 0.2507, Validation Loss: 0.4609\n",
      "Epoch #5, Batch #1060, Train Loss: 0.0480, Validation Loss: 0.9721\n",
      "Epoch #5, Batch #1070, Train Loss: 0.0016, Validation Loss: 1.2234\n",
      "Epoch #5, Batch #1080, Train Loss: 1.1486, Validation Loss: 0.6646\n",
      "Epoch #5, Batch #1090, Train Loss: 0.0104, Validation Loss: 0.6409\n",
      "Epoch #5, Batch #1100, Train Loss: 0.0063, Validation Loss: 2.8839\n",
      "Epoch #5, Batch #1110, Train Loss: 0.1230, Validation Loss: 0.9436\n",
      "Epoch #5, Batch #1120, Train Loss: 0.0488, Validation Loss: 1.3538\n",
      "Epoch #5, Batch #1130, Train Loss: 0.1414, Validation Loss: 1.4158\n",
      "Epoch #5, Batch #1140, Train Loss: 0.1214, Validation Loss: 2.6817\n",
      "Epoch #5, Batch #1150, Train Loss: 0.1918, Validation Loss: 0.0708\n",
      "Epoch #5, Batch #1160, Train Loss: 0.0052, Validation Loss: 3.1064\n",
      "Epoch #5, Batch #1170, Train Loss: 0.0763, Validation Loss: 2.1614\n",
      "Epoch #5, Batch #1180, Train Loss: 0.1888, Validation Loss: 0.6219\n",
      "Epoch #5, Batch #1190, Train Loss: 0.0465, Validation Loss: 1.6690\n",
      "Epoch #5, Batch #1200, Train Loss: 0.1663, Validation Loss: 1.6641\n",
      "Epoch #5, Batch #1210, Train Loss: 0.0199, Validation Loss: 0.1110\n",
      "Epoch #5, Batch #1220, Train Loss: 0.0213, Validation Loss: 0.3607\n",
      "Epoch #6, Batch #0, Train Loss: 0.1039, Validation Loss: 4.9379\n",
      "Epoch #6, Batch #10, Train Loss: 0.0580, Validation Loss: 8.8406\n",
      "Epoch #6, Batch #20, Train Loss: 0.1210, Validation Loss: 0.4894\n",
      "Epoch #6, Batch #30, Train Loss: 0.0101, Validation Loss: 0.9046\n",
      "Epoch #6, Batch #40, Train Loss: 0.1411, Validation Loss: 3.5398\n",
      "Epoch #6, Batch #50, Train Loss: 0.0144, Validation Loss: 3.4182\n",
      "Epoch #6, Batch #60, Train Loss: 0.0082, Validation Loss: 1.2831\n",
      "Epoch #6, Batch #70, Train Loss: 0.0255, Validation Loss: 1.0141\n",
      "Epoch #6, Batch #80, Train Loss: 0.3399, Validation Loss: 1.0545\n",
      "Epoch #6, Batch #90, Train Loss: 0.1452, Validation Loss: 3.6527\n",
      "Epoch #6, Batch #100, Train Loss: 0.0449, Validation Loss: 2.5541\n",
      "Epoch #6, Batch #110, Train Loss: 0.0897, Validation Loss: 2.2505\n",
      "Epoch #6, Batch #120, Train Loss: 0.1238, Validation Loss: 1.6549\n",
      "Epoch #6, Batch #130, Train Loss: 0.0866, Validation Loss: 4.3561\n",
      "Epoch #6, Batch #140, Train Loss: 0.0216, Validation Loss: 2.7606\n",
      "Epoch #6, Batch #150, Train Loss: 0.2473, Validation Loss: 0.1630\n",
      "Epoch #6, Batch #160, Train Loss: 0.0697, Validation Loss: 0.9893\n",
      "Epoch #6, Batch #170, Train Loss: 0.1116, Validation Loss: 0.8107\n",
      "Epoch #6, Batch #180, Train Loss: 0.0526, Validation Loss: 0.4581\n",
      "Epoch #6, Batch #190, Train Loss: 0.1405, Validation Loss: 0.6683\n",
      "Epoch #6, Batch #200, Train Loss: 0.2010, Validation Loss: 10.3900\n",
      "Epoch #6, Batch #210, Train Loss: 0.3472, Validation Loss: 2.1001\n",
      "Epoch #6, Batch #220, Train Loss: 0.1011, Validation Loss: 2.8028\n",
      "Epoch #6, Batch #230, Train Loss: 0.4037, Validation Loss: 7.8175\n",
      "Epoch #6, Batch #240, Train Loss: 0.1082, Validation Loss: 0.7595\n",
      "Epoch #6, Batch #250, Train Loss: 0.2006, Validation Loss: 0.0065\n",
      "Epoch #6, Batch #260, Train Loss: 0.0884, Validation Loss: 3.5705\n",
      "Epoch #6, Batch #270, Train Loss: 0.0791, Validation Loss: 3.8211\n",
      "Epoch #6, Batch #280, Train Loss: 0.1760, Validation Loss: 6.8049\n",
      "Epoch #6, Batch #290, Train Loss: 0.0478, Validation Loss: 1.0984\n",
      "Epoch #6, Batch #300, Train Loss: 0.1875, Validation Loss: 0.3067\n",
      "Epoch #6, Batch #310, Train Loss: 0.0140, Validation Loss: 1.9487\n",
      "Epoch #6, Batch #320, Train Loss: 0.3500, Validation Loss: 3.2688\n",
      "Epoch #6, Batch #330, Train Loss: 0.2369, Validation Loss: 0.2703\n",
      "Epoch #6, Batch #340, Train Loss: 0.1059, Validation Loss: 0.3991\n",
      "Epoch #6, Batch #350, Train Loss: 0.1100, Validation Loss: 0.4927\n",
      "Epoch #6, Batch #360, Train Loss: 0.0604, Validation Loss: 5.3183\n",
      "Epoch #6, Batch #370, Train Loss: 0.0144, Validation Loss: 0.1256\n",
      "Epoch #6, Batch #380, Train Loss: 1.6427, Validation Loss: 4.9388\n",
      "Epoch #6, Batch #390, Train Loss: 0.1188, Validation Loss: 2.9309\n",
      "Epoch #6, Batch #400, Train Loss: 0.0779, Validation Loss: 1.4062\n",
      "Epoch #6, Batch #410, Train Loss: 0.0934, Validation Loss: 2.3413\n",
      "Epoch #6, Batch #420, Train Loss: 0.1508, Validation Loss: 2.4157\n",
      "Epoch #6, Batch #430, Train Loss: 0.2455, Validation Loss: 2.3332\n",
      "Epoch #6, Batch #440, Train Loss: 0.0736, Validation Loss: 0.5561\n",
      "Epoch #6, Batch #450, Train Loss: 0.0378, Validation Loss: 4.6437\n",
      "Epoch #6, Batch #460, Train Loss: 0.0139, Validation Loss: 1.0062\n",
      "Epoch #6, Batch #470, Train Loss: 2.0040, Validation Loss: 0.2763\n",
      "Epoch #6, Batch #480, Train Loss: 0.0703, Validation Loss: 2.4156\n",
      "Epoch #6, Batch #490, Train Loss: 0.0208, Validation Loss: 2.5476\n",
      "Epoch #6, Batch #500, Train Loss: 0.0325, Validation Loss: 0.7178\n",
      "Epoch #6, Batch #510, Train Loss: 0.1708, Validation Loss: 0.6197\n",
      "Epoch #6, Batch #520, Train Loss: 0.0151, Validation Loss: 0.5566\n",
      "Epoch #6, Batch #530, Train Loss: 0.5789, Validation Loss: 2.3951\n",
      "Epoch #6, Batch #540, Train Loss: 0.3947, Validation Loss: 4.6962\n",
      "Epoch #6, Batch #550, Train Loss: 0.1904, Validation Loss: 0.2550\n",
      "Epoch #6, Batch #560, Train Loss: 0.4942, Validation Loss: 1.5478\n",
      "Epoch #6, Batch #570, Train Loss: 0.2033, Validation Loss: 1.5137\n",
      "Epoch #6, Batch #580, Train Loss: 0.1180, Validation Loss: 0.5643\n",
      "Epoch #6, Batch #590, Train Loss: 0.0576, Validation Loss: 2.2022\n",
      "Epoch #6, Batch #600, Train Loss: 0.0420, Validation Loss: 0.3723\n",
      "Epoch #6, Batch #610, Train Loss: 0.0121, Validation Loss: 2.4464\n",
      "Epoch #6, Batch #620, Train Loss: 0.0539, Validation Loss: 2.2037\n",
      "Epoch #6, Batch #630, Train Loss: 0.0628, Validation Loss: 8.4956\n",
      "Epoch #6, Batch #640, Train Loss: 0.1317, Validation Loss: 0.3830\n",
      "Epoch #6, Batch #650, Train Loss: 0.2307, Validation Loss: 1.3787\n",
      "Epoch #6, Batch #660, Train Loss: 0.4016, Validation Loss: 0.2851\n",
      "Epoch #6, Batch #670, Train Loss: 0.0623, Validation Loss: 1.4038\n",
      "Epoch #6, Batch #680, Train Loss: 0.0079, Validation Loss: 9.3838\n",
      "Epoch #6, Batch #690, Train Loss: 0.0586, Validation Loss: 1.2868\n",
      "Epoch #6, Batch #700, Train Loss: 0.0739, Validation Loss: 0.3647\n",
      "Epoch #6, Batch #710, Train Loss: 0.0454, Validation Loss: 0.2315\n",
      "Epoch #6, Batch #720, Train Loss: 0.0835, Validation Loss: 1.7763\n",
      "Epoch #6, Batch #730, Train Loss: 0.2731, Validation Loss: 0.9549\n",
      "Epoch #6, Batch #740, Train Loss: 0.0909, Validation Loss: 0.9670\n",
      "Epoch #6, Batch #750, Train Loss: 0.7611, Validation Loss: 2.3105\n",
      "Epoch #6, Batch #760, Train Loss: 0.0031, Validation Loss: 1.3036\n",
      "Epoch #6, Batch #770, Train Loss: 0.1244, Validation Loss: 0.8835\n",
      "Epoch #6, Batch #780, Train Loss: 0.0817, Validation Loss: 1.3793\n",
      "Epoch #6, Batch #790, Train Loss: 0.6663, Validation Loss: 0.5897\n",
      "Epoch #6, Batch #800, Train Loss: 0.0081, Validation Loss: 0.8819\n",
      "Epoch #6, Batch #810, Train Loss: 0.1313, Validation Loss: 0.5456\n",
      "Epoch #6, Batch #820, Train Loss: 0.4319, Validation Loss: 0.8058\n",
      "Epoch #6, Batch #830, Train Loss: 0.0338, Validation Loss: 0.2608\n",
      "Epoch #6, Batch #840, Train Loss: 0.0131, Validation Loss: 4.0618\n",
      "Epoch #6, Batch #850, Train Loss: 0.1099, Validation Loss: 1.5342\n",
      "Epoch #6, Batch #860, Train Loss: 0.0266, Validation Loss: 0.3171\n",
      "Epoch #6, Batch #870, Train Loss: 0.4898, Validation Loss: 0.8271\n",
      "Epoch #6, Batch #880, Train Loss: 0.0723, Validation Loss: 0.0212\n",
      "Epoch #6, Batch #890, Train Loss: 0.2030, Validation Loss: 0.1773\n",
      "Epoch #6, Batch #900, Train Loss: 0.2512, Validation Loss: 1.6118\n",
      "Epoch #6, Batch #910, Train Loss: 0.0245, Validation Loss: 2.8398\n",
      "Epoch #6, Batch #920, Train Loss: 0.1766, Validation Loss: 1.7284\n",
      "Epoch #6, Batch #930, Train Loss: 0.0709, Validation Loss: 0.0297\n",
      "Epoch #6, Batch #940, Train Loss: 0.0140, Validation Loss: 0.3238\n",
      "Epoch #6, Batch #950, Train Loss: 0.0795, Validation Loss: 0.5694\n",
      "Epoch #6, Batch #960, Train Loss: 0.0261, Validation Loss: 0.1559\n",
      "Epoch #6, Batch #970, Train Loss: 0.0534, Validation Loss: 0.3898\n",
      "Epoch #6, Batch #980, Train Loss: 1.2997, Validation Loss: 0.3669\n",
      "Epoch #6, Batch #990, Train Loss: 0.0034, Validation Loss: 1.3325\n",
      "Epoch #6, Batch #1000, Train Loss: 0.0678, Validation Loss: 11.5242\n",
      "Epoch #6, Batch #1010, Train Loss: 0.1120, Validation Loss: 0.0370\n",
      "Epoch #6, Batch #1020, Train Loss: 0.2365, Validation Loss: 0.0650\n",
      "Epoch #6, Batch #1030, Train Loss: 0.0762, Validation Loss: 2.5567\n",
      "Epoch #6, Batch #1040, Train Loss: 0.0095, Validation Loss: 7.1705\n",
      "Epoch #6, Batch #1050, Train Loss: 0.1593, Validation Loss: 1.3437\n",
      "Epoch #6, Batch #1060, Train Loss: 0.1067, Validation Loss: 0.1182\n",
      "Epoch #6, Batch #1070, Train Loss: 0.1566, Validation Loss: 1.4861\n",
      "Epoch #6, Batch #1080, Train Loss: 0.2643, Validation Loss: 0.0302\n",
      "Epoch #6, Batch #1090, Train Loss: 0.0469, Validation Loss: 0.0217\n",
      "Epoch #6, Batch #1100, Train Loss: 0.1118, Validation Loss: 0.0976\n",
      "Epoch #6, Batch #1110, Train Loss: 0.1019, Validation Loss: 4.3466\n",
      "Epoch #6, Batch #1120, Train Loss: 0.0899, Validation Loss: 0.9665\n",
      "Epoch #6, Batch #1130, Train Loss: 0.0553, Validation Loss: 0.5043\n",
      "Epoch #6, Batch #1140, Train Loss: 0.1611, Validation Loss: 3.0846\n",
      "Epoch #6, Batch #1150, Train Loss: 0.2604, Validation Loss: 1.7725\n",
      "Epoch #6, Batch #1160, Train Loss: 0.0645, Validation Loss: 3.4629\n",
      "Epoch #6, Batch #1170, Train Loss: 0.0464, Validation Loss: 2.4935\n",
      "Epoch #6, Batch #1180, Train Loss: 0.1395, Validation Loss: 3.1287\n",
      "Epoch #6, Batch #1190, Train Loss: 0.1119, Validation Loss: 1.2744\n",
      "Epoch #6, Batch #1200, Train Loss: 0.1732, Validation Loss: 0.2681\n",
      "Epoch #6, Batch #1210, Train Loss: 0.0498, Validation Loss: 0.2316\n",
      "Epoch #6, Batch #1220, Train Loss: 0.0057, Validation Loss: 0.2754\n",
      "Epoch #7, Batch #0, Train Loss: 0.1870, Validation Loss: 0.9508\n",
      "Epoch #7, Batch #10, Train Loss: 0.0634, Validation Loss: 0.0149\n",
      "Epoch #7, Batch #20, Train Loss: 0.0498, Validation Loss: 0.4244\n",
      "Epoch #7, Batch #30, Train Loss: 0.1341, Validation Loss: 1.5983\n",
      "Epoch #7, Batch #40, Train Loss: 0.0460, Validation Loss: 0.5448\n",
      "Epoch #7, Batch #50, Train Loss: 0.0589, Validation Loss: 0.9323\n",
      "Epoch #7, Batch #60, Train Loss: 0.1905, Validation Loss: 0.4135\n",
      "Epoch #7, Batch #70, Train Loss: 0.0654, Validation Loss: 1.2790\n",
      "Epoch #7, Batch #80, Train Loss: 0.0543, Validation Loss: 0.9771\n",
      "Epoch #7, Batch #90, Train Loss: 0.0147, Validation Loss: 2.5064\n",
      "Epoch #7, Batch #100, Train Loss: 2.1722, Validation Loss: 1.6421\n",
      "Epoch #7, Batch #110, Train Loss: 0.0986, Validation Loss: 0.9982\n",
      "Epoch #7, Batch #120, Train Loss: 0.0464, Validation Loss: 0.8492\n",
      "Epoch #7, Batch #130, Train Loss: 0.2036, Validation Loss: 3.1913\n",
      "Epoch #7, Batch #140, Train Loss: 0.0099, Validation Loss: 1.1395\n",
      "Epoch #7, Batch #150, Train Loss: 0.0106, Validation Loss: 1.8803\n",
      "Epoch #7, Batch #160, Train Loss: 0.2865, Validation Loss: 0.1337\n",
      "Epoch #7, Batch #170, Train Loss: 0.2375, Validation Loss: 0.1534\n",
      "Epoch #7, Batch #180, Train Loss: 0.0759, Validation Loss: 0.2346\n",
      "Epoch #7, Batch #190, Train Loss: 0.0611, Validation Loss: 0.4799\n",
      "Epoch #7, Batch #200, Train Loss: 0.0205, Validation Loss: 1.1874\n",
      "Epoch #7, Batch #210, Train Loss: 0.0483, Validation Loss: 0.9872\n",
      "Epoch #7, Batch #220, Train Loss: 0.0069, Validation Loss: 4.1743\n",
      "Epoch #7, Batch #230, Train Loss: 0.0051, Validation Loss: 0.5446\n",
      "Epoch #7, Batch #240, Train Loss: 0.0124, Validation Loss: 1.8195\n",
      "Epoch #7, Batch #250, Train Loss: 0.0317, Validation Loss: 1.8881\n",
      "Epoch #7, Batch #260, Train Loss: 0.3096, Validation Loss: 8.9849\n",
      "Epoch #7, Batch #270, Train Loss: 0.1923, Validation Loss: 0.2561\n",
      "Epoch #7, Batch #280, Train Loss: 0.0461, Validation Loss: 0.3810\n",
      "Epoch #7, Batch #290, Train Loss: 0.2386, Validation Loss: 0.1890\n",
      "Epoch #7, Batch #300, Train Loss: 0.0628, Validation Loss: 1.4999\n",
      "Epoch #7, Batch #310, Train Loss: 0.4821, Validation Loss: 0.0671\n",
      "Epoch #7, Batch #320, Train Loss: 0.1272, Validation Loss: 0.7925\n",
      "Epoch #7, Batch #330, Train Loss: 0.0160, Validation Loss: 0.5148\n",
      "Epoch #7, Batch #340, Train Loss: 0.2979, Validation Loss: 4.8450\n",
      "Epoch #7, Batch #350, Train Loss: 0.1296, Validation Loss: 3.1046\n",
      "Epoch #7, Batch #360, Train Loss: 0.3736, Validation Loss: 0.2090\n",
      "Epoch #7, Batch #370, Train Loss: 0.1843, Validation Loss: 0.8658\n",
      "Epoch #7, Batch #380, Train Loss: 0.0422, Validation Loss: 0.1615\n",
      "Epoch #7, Batch #390, Train Loss: 0.1596, Validation Loss: 1.1097\n",
      "Epoch #7, Batch #400, Train Loss: 0.0095, Validation Loss: 0.8175\n",
      "Epoch #7, Batch #410, Train Loss: 0.1671, Validation Loss: 3.0736\n",
      "Epoch #7, Batch #420, Train Loss: 0.1737, Validation Loss: 3.8268\n",
      "Epoch #7, Batch #430, Train Loss: 0.1683, Validation Loss: 1.4351\n",
      "Epoch #7, Batch #440, Train Loss: 0.1687, Validation Loss: 0.0969\n",
      "Epoch #7, Batch #450, Train Loss: 0.0483, Validation Loss: 3.4920\n",
      "Epoch #7, Batch #460, Train Loss: 0.2837, Validation Loss: 2.7983\n",
      "Epoch #7, Batch #470, Train Loss: 0.1778, Validation Loss: 0.1566\n",
      "Epoch #7, Batch #480, Train Loss: 0.0774, Validation Loss: 0.0703\n",
      "Epoch #7, Batch #490, Train Loss: 0.0219, Validation Loss: 0.9094\n",
      "Epoch #7, Batch #500, Train Loss: 0.0182, Validation Loss: 4.7979\n",
      "Epoch #7, Batch #510, Train Loss: 0.6181, Validation Loss: 0.0043\n",
      "Epoch #7, Batch #520, Train Loss: 1.2588, Validation Loss: 0.3013\n",
      "Epoch #7, Batch #530, Train Loss: 0.0544, Validation Loss: 1.6976\n",
      "Epoch #7, Batch #540, Train Loss: 0.0755, Validation Loss: 1.2048\n",
      "Epoch #7, Batch #550, Train Loss: 0.0252, Validation Loss: 0.2340\n",
      "Epoch #7, Batch #560, Train Loss: 0.0998, Validation Loss: 10.9774\n",
      "Epoch #7, Batch #570, Train Loss: 0.0089, Validation Loss: 3.3657\n",
      "Epoch #7, Batch #580, Train Loss: 0.1245, Validation Loss: 1.7492\n",
      "Epoch #7, Batch #590, Train Loss: 0.0647, Validation Loss: 0.1764\n",
      "Epoch #7, Batch #600, Train Loss: 0.0279, Validation Loss: 0.4929\n",
      "Epoch #7, Batch #610, Train Loss: 0.0438, Validation Loss: 1.4912\n",
      "Epoch #7, Batch #620, Train Loss: 0.2186, Validation Loss: 1.6584\n",
      "Epoch #7, Batch #630, Train Loss: 0.0793, Validation Loss: 1.2227\n",
      "Epoch #7, Batch #640, Train Loss: 0.0578, Validation Loss: 1.6607\n",
      "Epoch #7, Batch #650, Train Loss: 0.0755, Validation Loss: 0.2579\n",
      "Epoch #7, Batch #660, Train Loss: 0.0746, Validation Loss: 2.2896\n",
      "Epoch #7, Batch #670, Train Loss: 0.0145, Validation Loss: 0.3393\n",
      "Epoch #7, Batch #680, Train Loss: 0.0657, Validation Loss: 1.6011\n",
      "Epoch #7, Batch #690, Train Loss: 0.0143, Validation Loss: 7.8085\n",
      "Epoch #7, Batch #700, Train Loss: 0.0128, Validation Loss: 0.1857\n",
      "Epoch #7, Batch #710, Train Loss: 0.0258, Validation Loss: 6.3988\n",
      "Epoch #7, Batch #720, Train Loss: 2.4742, Validation Loss: 2.5430\n",
      "Epoch #7, Batch #730, Train Loss: 0.0427, Validation Loss: 0.3092\n",
      "Epoch #7, Batch #740, Train Loss: 0.1999, Validation Loss: 0.2694\n",
      "Epoch #7, Batch #750, Train Loss: 1.4664, Validation Loss: 0.6160\n",
      "Epoch #7, Batch #760, Train Loss: 0.0064, Validation Loss: 1.8795\n",
      "Epoch #7, Batch #770, Train Loss: 0.0482, Validation Loss: 0.3025\n",
      "Epoch #7, Batch #780, Train Loss: 0.3148, Validation Loss: 0.9751\n",
      "Epoch #7, Batch #790, Train Loss: 0.0661, Validation Loss: 7.0683\n",
      "Epoch #7, Batch #800, Train Loss: 0.0045, Validation Loss: 0.3908\n",
      "Epoch #7, Batch #810, Train Loss: 0.2649, Validation Loss: 1.3317\n",
      "Epoch #7, Batch #820, Train Loss: 0.0290, Validation Loss: 1.2564\n",
      "Epoch #7, Batch #830, Train Loss: 0.1073, Validation Loss: 1.4921\n",
      "Epoch #7, Batch #840, Train Loss: 0.1890, Validation Loss: 1.2577\n",
      "Epoch #7, Batch #850, Train Loss: 0.1005, Validation Loss: 0.6143\n",
      "Epoch #7, Batch #860, Train Loss: 0.0070, Validation Loss: 0.0765\n",
      "Epoch #7, Batch #870, Train Loss: 0.0172, Validation Loss: 1.0279\n",
      "Epoch #7, Batch #880, Train Loss: 0.0347, Validation Loss: 0.0566\n",
      "Epoch #7, Batch #890, Train Loss: 0.0514, Validation Loss: 5.1326\n",
      "Epoch #7, Batch #900, Train Loss: 0.0850, Validation Loss: 0.3972\n",
      "Epoch #7, Batch #910, Train Loss: 0.1240, Validation Loss: 0.0164\n",
      "Epoch #7, Batch #920, Train Loss: 0.0342, Validation Loss: 0.6971\n",
      "Epoch #7, Batch #930, Train Loss: 0.0153, Validation Loss: 0.6168\n",
      "Epoch #7, Batch #940, Train Loss: 0.0587, Validation Loss: 1.9617\n",
      "Epoch #7, Batch #950, Train Loss: 0.0817, Validation Loss: 0.2345\n",
      "Epoch #7, Batch #960, Train Loss: 0.0897, Validation Loss: 1.0702\n",
      "Epoch #7, Batch #970, Train Loss: 0.0040, Validation Loss: 0.3222\n",
      "Epoch #7, Batch #980, Train Loss: 0.0876, Validation Loss: 1.1628\n",
      "Epoch #7, Batch #990, Train Loss: 0.0283, Validation Loss: 0.0724\n",
      "Epoch #7, Batch #1000, Train Loss: 0.0073, Validation Loss: 1.2343\n",
      "Epoch #7, Batch #1010, Train Loss: 0.1093, Validation Loss: 0.4780\n",
      "Epoch #7, Batch #1020, Train Loss: 0.0762, Validation Loss: 0.3557\n",
      "Epoch #7, Batch #1030, Train Loss: 0.2302, Validation Loss: 1.4809\n",
      "Epoch #7, Batch #1040, Train Loss: 0.0750, Validation Loss: 5.0309\n",
      "Epoch #7, Batch #1050, Train Loss: 0.2086, Validation Loss: 0.0509\n",
      "Epoch #7, Batch #1060, Train Loss: 0.0026, Validation Loss: 0.5654\n",
      "Epoch #7, Batch #1070, Train Loss: 0.2169, Validation Loss: 4.7957\n",
      "Epoch #7, Batch #1080, Train Loss: 0.0596, Validation Loss: 0.2264\n",
      "Epoch #7, Batch #1090, Train Loss: 0.0997, Validation Loss: 0.4315\n",
      "Epoch #7, Batch #1100, Train Loss: 0.1141, Validation Loss: 0.0388\n",
      "Epoch #7, Batch #1110, Train Loss: 0.0288, Validation Loss: 1.9498\n",
      "Epoch #7, Batch #1120, Train Loss: 0.0228, Validation Loss: 0.3743\n",
      "Epoch #7, Batch #1130, Train Loss: 0.0300, Validation Loss: 0.1583\n",
      "Epoch #7, Batch #1140, Train Loss: 0.0170, Validation Loss: 0.0588\n",
      "Epoch #7, Batch #1150, Train Loss: 0.0647, Validation Loss: 0.5870\n",
      "Epoch #7, Batch #1160, Train Loss: 0.0393, Validation Loss: 0.9957\n",
      "Epoch #7, Batch #1170, Train Loss: 0.0175, Validation Loss: 0.2422\n",
      "Epoch #7, Batch #1180, Train Loss: 0.1298, Validation Loss: 0.4121\n",
      "Epoch #7, Batch #1190, Train Loss: 0.3246, Validation Loss: 0.2441\n",
      "Epoch #7, Batch #1200, Train Loss: 0.1361, Validation Loss: 2.9883\n",
      "Epoch #7, Batch #1210, Train Loss: 0.0764, Validation Loss: 0.8622\n",
      "Epoch #7, Batch #1220, Train Loss: 0.0163, Validation Loss: 1.1627\n",
      "Epoch #8, Batch #0, Train Loss: 0.0392, Validation Loss: 0.0382\n",
      "Epoch #8, Batch #10, Train Loss: 0.1296, Validation Loss: 3.2871\n",
      "Epoch #8, Batch #20, Train Loss: 0.0810, Validation Loss: 0.5724\n",
      "Epoch #8, Batch #30, Train Loss: 0.1946, Validation Loss: 0.8652\n",
      "Epoch #8, Batch #40, Train Loss: 0.0202, Validation Loss: 1.4062\n",
      "Epoch #8, Batch #50, Train Loss: 0.1774, Validation Loss: 0.5385\n",
      "Epoch #8, Batch #60, Train Loss: 0.1696, Validation Loss: 0.2945\n",
      "Epoch #8, Batch #70, Train Loss: 1.1936, Validation Loss: 0.9476\n",
      "Epoch #8, Batch #80, Train Loss: 0.2445, Validation Loss: 8.2810\n",
      "Epoch #8, Batch #90, Train Loss: 0.0607, Validation Loss: 0.5200\n",
      "Epoch #8, Batch #100, Train Loss: 0.0370, Validation Loss: 1.0854\n",
      "Epoch #8, Batch #110, Train Loss: 0.0186, Validation Loss: 0.1254\n",
      "Epoch #8, Batch #120, Train Loss: 0.0390, Validation Loss: 0.9454\n",
      "Epoch #8, Batch #130, Train Loss: 0.0993, Validation Loss: 0.2618\n",
      "Epoch #8, Batch #140, Train Loss: 0.3613, Validation Loss: 0.8260\n",
      "Epoch #8, Batch #150, Train Loss: 0.0330, Validation Loss: 0.5590\n",
      "Epoch #8, Batch #160, Train Loss: 0.0622, Validation Loss: 2.8415\n",
      "Epoch #8, Batch #170, Train Loss: 0.0144, Validation Loss: 1.3870\n",
      "Epoch #8, Batch #180, Train Loss: 0.0136, Validation Loss: 0.2830\n",
      "Epoch #8, Batch #190, Train Loss: 0.1583, Validation Loss: 0.8730\n",
      "Epoch #8, Batch #200, Train Loss: 0.1334, Validation Loss: 0.4889\n",
      "Epoch #8, Batch #210, Train Loss: 0.0551, Validation Loss: 2.0782\n",
      "Epoch #8, Batch #220, Train Loss: 0.1801, Validation Loss: 10.2403\n",
      "Epoch #8, Batch #230, Train Loss: 0.0080, Validation Loss: 0.3016\n",
      "Epoch #8, Batch #240, Train Loss: 0.0995, Validation Loss: 0.3644\n",
      "Epoch #8, Batch #250, Train Loss: 0.9152, Validation Loss: 0.0670\n",
      "Epoch #8, Batch #260, Train Loss: 0.1522, Validation Loss: 0.2265\n",
      "Epoch #8, Batch #270, Train Loss: 0.2771, Validation Loss: 3.1399\n",
      "Epoch #8, Batch #280, Train Loss: 0.0323, Validation Loss: 3.1368\n",
      "Epoch #8, Batch #290, Train Loss: 0.0163, Validation Loss: 0.6248\n",
      "Epoch #8, Batch #300, Train Loss: 0.2274, Validation Loss: 0.4240\n",
      "Epoch #8, Batch #310, Train Loss: 0.3483, Validation Loss: 8.1989\n",
      "Epoch #8, Batch #320, Train Loss: 0.6881, Validation Loss: 4.4025\n",
      "Epoch #8, Batch #330, Train Loss: 0.1577, Validation Loss: 0.5160\n",
      "Epoch #8, Batch #340, Train Loss: 0.0849, Validation Loss: 0.4396\n",
      "Epoch #8, Batch #350, Train Loss: 0.0844, Validation Loss: 0.4542\n",
      "Epoch #8, Batch #360, Train Loss: 0.0467, Validation Loss: 2.8711\n",
      "Epoch #8, Batch #370, Train Loss: 0.2616, Validation Loss: 1.1927\n",
      "Epoch #8, Batch #380, Train Loss: 0.0693, Validation Loss: 0.0649\n",
      "Epoch #8, Batch #390, Train Loss: 0.0034, Validation Loss: 0.1053\n",
      "Epoch #8, Batch #400, Train Loss: 0.0033, Validation Loss: 0.0780\n",
      "Epoch #8, Batch #410, Train Loss: 0.0376, Validation Loss: 0.2297\n",
      "Epoch #8, Batch #420, Train Loss: 0.0996, Validation Loss: 0.0072\n",
      "Epoch #8, Batch #430, Train Loss: 0.0223, Validation Loss: 0.2136\n",
      "Epoch #8, Batch #440, Train Loss: 0.0443, Validation Loss: 0.2330\n",
      "Epoch #8, Batch #450, Train Loss: 0.0863, Validation Loss: 0.6659\n",
      "Epoch #8, Batch #460, Train Loss: 0.1419, Validation Loss: 0.4894\n",
      "Epoch #8, Batch #470, Train Loss: 0.0135, Validation Loss: 0.0666\n",
      "Epoch #8, Batch #480, Train Loss: 0.1223, Validation Loss: 0.0744\n",
      "Epoch #8, Batch #490, Train Loss: 0.2867, Validation Loss: 0.1287\n",
      "Epoch #8, Batch #500, Train Loss: 0.0674, Validation Loss: 0.5011\n",
      "Epoch #8, Batch #510, Train Loss: 0.0435, Validation Loss: 1.0249\n",
      "Epoch #8, Batch #520, Train Loss: 0.3940, Validation Loss: 3.8461\n",
      "Epoch #8, Batch #530, Train Loss: 0.0282, Validation Loss: 0.2203\n",
      "Epoch #8, Batch #540, Train Loss: 0.0310, Validation Loss: 0.4196\n",
      "Epoch #8, Batch #550, Train Loss: 0.0152, Validation Loss: 0.6072\n",
      "Epoch #8, Batch #560, Train Loss: 0.2087, Validation Loss: 0.4201\n",
      "Epoch #8, Batch #570, Train Loss: 0.0016, Validation Loss: 10.0721\n",
      "Epoch #8, Batch #580, Train Loss: 0.2551, Validation Loss: 0.2885\n",
      "Epoch #8, Batch #590, Train Loss: 0.2363, Validation Loss: 0.5441\n",
      "Epoch #8, Batch #600, Train Loss: 0.0097, Validation Loss: 0.1202\n",
      "Epoch #8, Batch #610, Train Loss: 0.0930, Validation Loss: 8.1078\n",
      "Epoch #8, Batch #620, Train Loss: 0.1744, Validation Loss: 9.6257\n",
      "Epoch #8, Batch #630, Train Loss: 0.0217, Validation Loss: 0.4142\n",
      "Epoch #8, Batch #640, Train Loss: 0.0407, Validation Loss: 1.1285\n",
      "Epoch #8, Batch #650, Train Loss: 0.1063, Validation Loss: 0.1266\n",
      "Epoch #8, Batch #660, Train Loss: 0.1477, Validation Loss: 4.4632\n",
      "Epoch #8, Batch #670, Train Loss: 0.1004, Validation Loss: 7.4863\n",
      "Epoch #8, Batch #680, Train Loss: 2.0422, Validation Loss: 0.1356\n",
      "Epoch #8, Batch #690, Train Loss: 0.1113, Validation Loss: 2.6424\n",
      "Epoch #8, Batch #700, Train Loss: 0.2137, Validation Loss: 0.1389\n",
      "Epoch #8, Batch #710, Train Loss: 0.1017, Validation Loss: 0.2660\n",
      "Epoch #8, Batch #720, Train Loss: 0.2189, Validation Loss: 0.1953\n",
      "Epoch #8, Batch #730, Train Loss: 0.0232, Validation Loss: 5.2837\n",
      "Epoch #8, Batch #740, Train Loss: 0.0150, Validation Loss: 0.0367\n",
      "Epoch #8, Batch #750, Train Loss: 0.2589, Validation Loss: 0.0811\n",
      "Epoch #8, Batch #760, Train Loss: 0.0350, Validation Loss: 0.4918\n",
      "Epoch #8, Batch #770, Train Loss: 0.1122, Validation Loss: 1.4934\n",
      "Epoch #8, Batch #780, Train Loss: 0.3289, Validation Loss: 0.3069\n",
      "Epoch #8, Batch #790, Train Loss: 0.1750, Validation Loss: 0.0178\n",
      "Epoch #8, Batch #800, Train Loss: 0.0250, Validation Loss: 0.0775\n",
      "Epoch #8, Batch #810, Train Loss: 0.2252, Validation Loss: 0.0918\n",
      "Epoch #8, Batch #820, Train Loss: 0.0151, Validation Loss: 0.1376\n",
      "Epoch #8, Batch #830, Train Loss: 0.9516, Validation Loss: 0.3621\n",
      "Epoch #8, Batch #840, Train Loss: 0.1197, Validation Loss: 1.0137\n",
      "Epoch #8, Batch #850, Train Loss: 0.0344, Validation Loss: 0.0720\n",
      "Epoch #8, Batch #860, Train Loss: 0.0767, Validation Loss: 0.6941\n",
      "Epoch #8, Batch #870, Train Loss: 0.1730, Validation Loss: 0.2374\n",
      "Epoch #8, Batch #880, Train Loss: 0.0939, Validation Loss: 1.2374\n",
      "Epoch #8, Batch #890, Train Loss: 0.0231, Validation Loss: 2.4124\n",
      "Epoch #8, Batch #900, Train Loss: 0.1402, Validation Loss: 0.2594\n",
      "Epoch #8, Batch #910, Train Loss: 0.3548, Validation Loss: 0.3253\n",
      "Epoch #8, Batch #920, Train Loss: 0.1583, Validation Loss: 2.9002\n",
      "Epoch #8, Batch #930, Train Loss: 0.0744, Validation Loss: 0.6259\n",
      "Epoch #8, Batch #940, Train Loss: 0.0252, Validation Loss: 0.6224\n",
      "Epoch #8, Batch #950, Train Loss: 0.0358, Validation Loss: 0.0522\n",
      "Epoch #8, Batch #960, Train Loss: 0.6431, Validation Loss: 0.1501\n",
      "Epoch #8, Batch #970, Train Loss: 0.0518, Validation Loss: 4.3661\n",
      "Epoch #8, Batch #980, Train Loss: 0.0097, Validation Loss: 0.5884\n",
      "Epoch #8, Batch #990, Train Loss: 0.1868, Validation Loss: 0.0771\n",
      "Epoch #8, Batch #1000, Train Loss: 0.1807, Validation Loss: 0.6897\n",
      "Epoch #8, Batch #1010, Train Loss: 0.1153, Validation Loss: 0.7439\n",
      "Epoch #8, Batch #1020, Train Loss: 0.0408, Validation Loss: 1.9441\n",
      "Epoch #8, Batch #1030, Train Loss: 0.0321, Validation Loss: 0.3147\n",
      "Epoch #8, Batch #1040, Train Loss: 0.1924, Validation Loss: 1.1008\n",
      "Epoch #8, Batch #1050, Train Loss: 0.0873, Validation Loss: 0.4698\n",
      "Epoch #8, Batch #1060, Train Loss: 0.0171, Validation Loss: 0.3193\n",
      "Epoch #8, Batch #1070, Train Loss: 0.0315, Validation Loss: 1.2345\n",
      "Epoch #8, Batch #1080, Train Loss: 0.0113, Validation Loss: 1.2611\n",
      "Epoch #8, Batch #1090, Train Loss: 0.3043, Validation Loss: 0.4424\n",
      "Epoch #8, Batch #1100, Train Loss: 0.0378, Validation Loss: 0.3819\n",
      "Epoch #8, Batch #1110, Train Loss: 0.1248, Validation Loss: 0.1782\n",
      "Epoch #8, Batch #1120, Train Loss: 2.1763, Validation Loss: 0.5873\n",
      "Epoch #8, Batch #1130, Train Loss: 0.1553, Validation Loss: 0.2621\n",
      "Epoch #8, Batch #1140, Train Loss: 0.4280, Validation Loss: 0.4279\n",
      "Epoch #8, Batch #1150, Train Loss: 0.1787, Validation Loss: 7.9841\n",
      "Epoch #8, Batch #1160, Train Loss: 0.2560, Validation Loss: 0.4540\n",
      "Epoch #8, Batch #1170, Train Loss: 0.0170, Validation Loss: 0.2098\n",
      "Epoch #8, Batch #1180, Train Loss: 0.1212, Validation Loss: 0.3784\n",
      "Epoch #8, Batch #1190, Train Loss: 0.1367, Validation Loss: 0.4731\n",
      "Epoch #8, Batch #1200, Train Loss: 0.0637, Validation Loss: 0.3333\n",
      "Epoch #8, Batch #1210, Train Loss: 0.1717, Validation Loss: 0.0466\n",
      "Epoch #8, Batch #1220, Train Loss: 0.0658, Validation Loss: 0.4098\n",
      "Epoch #9, Batch #0, Train Loss: 0.0544, Validation Loss: 6.3320\n",
      "Epoch #9, Batch #10, Train Loss: 0.0195, Validation Loss: 9.4253\n",
      "Epoch #9, Batch #20, Train Loss: 0.2352, Validation Loss: 0.1061\n",
      "Epoch #9, Batch #30, Train Loss: 0.1168, Validation Loss: 0.1543\n",
      "Epoch #9, Batch #40, Train Loss: 0.0058, Validation Loss: 0.4130\n",
      "Epoch #9, Batch #50, Train Loss: 0.0308, Validation Loss: 0.0067\n",
      "Epoch #9, Batch #60, Train Loss: 0.0176, Validation Loss: 0.0871\n",
      "Epoch #9, Batch #70, Train Loss: 0.1749, Validation Loss: 0.0839\n",
      "Epoch #9, Batch #80, Train Loss: 0.0999, Validation Loss: 0.7387\n",
      "Epoch #9, Batch #90, Train Loss: 0.0110, Validation Loss: 0.0322\n",
      "Epoch #9, Batch #100, Train Loss: 0.0297, Validation Loss: 0.3789\n",
      "Epoch #9, Batch #110, Train Loss: 0.0123, Validation Loss: 0.5235\n",
      "Epoch #9, Batch #120, Train Loss: 0.0113, Validation Loss: 0.4673\n",
      "Epoch #9, Batch #130, Train Loss: 0.2413, Validation Loss: 1.2539\n",
      "Epoch #9, Batch #140, Train Loss: 0.1307, Validation Loss: 0.0840\n",
      "Epoch #9, Batch #150, Train Loss: 0.3381, Validation Loss: 0.0534\n",
      "Epoch #9, Batch #160, Train Loss: 0.0065, Validation Loss: 4.1093\n",
      "Epoch #9, Batch #170, Train Loss: 0.2301, Validation Loss: 1.3677\n",
      "Epoch #9, Batch #180, Train Loss: 0.0667, Validation Loss: 3.8308\n",
      "Epoch #9, Batch #190, Train Loss: 0.0609, Validation Loss: 0.3127\n",
      "Epoch #9, Batch #200, Train Loss: 0.0075, Validation Loss: 0.9086\n",
      "Epoch #9, Batch #210, Train Loss: 0.5435, Validation Loss: 0.0066\n",
      "Epoch #9, Batch #220, Train Loss: 0.1911, Validation Loss: 5.0953\n",
      "Epoch #9, Batch #230, Train Loss: 0.1256, Validation Loss: 0.0622\n",
      "Epoch #9, Batch #240, Train Loss: 1.0827, Validation Loss: 0.0745\n",
      "Epoch #9, Batch #250, Train Loss: 0.0686, Validation Loss: 0.2099\n",
      "Epoch #9, Batch #260, Train Loss: 0.1193, Validation Loss: 8.9268\n",
      "Epoch #9, Batch #270, Train Loss: 0.6565, Validation Loss: 0.1942\n",
      "Epoch #9, Batch #280, Train Loss: 0.0267, Validation Loss: 0.2104\n",
      "Epoch #9, Batch #290, Train Loss: 0.0142, Validation Loss: 0.0471\n",
      "Epoch #9, Batch #300, Train Loss: 0.0091, Validation Loss: 0.2458\n",
      "Epoch #9, Batch #310, Train Loss: 0.0734, Validation Loss: 0.9104\n",
      "Epoch #9, Batch #320, Train Loss: 0.1675, Validation Loss: 0.1280\n",
      "Epoch #9, Batch #330, Train Loss: 0.0880, Validation Loss: 0.0677\n",
      "Epoch #9, Batch #340, Train Loss: 0.0115, Validation Loss: 0.0090\n",
      "Epoch #9, Batch #350, Train Loss: 0.6148, Validation Loss: 0.8786\n",
      "Epoch #9, Batch #360, Train Loss: 0.0479, Validation Loss: 5.6799\n",
      "Epoch #9, Batch #370, Train Loss: 0.0702, Validation Loss: 1.2395\n",
      "Epoch #9, Batch #380, Train Loss: 0.0171, Validation Loss: 0.7633\n",
      "Epoch #9, Batch #390, Train Loss: 0.0472, Validation Loss: 1.5829\n",
      "Epoch #9, Batch #400, Train Loss: 0.0062, Validation Loss: 0.0281\n",
      "Epoch #9, Batch #410, Train Loss: 0.0758, Validation Loss: 0.0815\n",
      "Epoch #9, Batch #420, Train Loss: 0.2197, Validation Loss: 0.1731\n",
      "Epoch #9, Batch #430, Train Loss: 0.0412, Validation Loss: 0.7852\n",
      "Epoch #9, Batch #440, Train Loss: 0.0167, Validation Loss: 0.2184\n",
      "Epoch #9, Batch #450, Train Loss: 0.0670, Validation Loss: 1.1561\n",
      "Epoch #9, Batch #460, Train Loss: 0.1618, Validation Loss: 0.4788\n",
      "Epoch #9, Batch #470, Train Loss: 0.0298, Validation Loss: 0.2741\n",
      "Epoch #9, Batch #480, Train Loss: 0.0265, Validation Loss: 1.5405\n",
      "Epoch #9, Batch #490, Train Loss: 0.8432, Validation Loss: 0.3594\n",
      "Epoch #9, Batch #500, Train Loss: 0.1058, Validation Loss: 0.7261\n",
      "Epoch #9, Batch #510, Train Loss: 0.0671, Validation Loss: 0.3238\n",
      "Epoch #9, Batch #520, Train Loss: 0.0283, Validation Loss: 0.1589\n",
      "Epoch #9, Batch #530, Train Loss: 0.4827, Validation Loss: 0.6169\n",
      "Epoch #9, Batch #540, Train Loss: 0.0046, Validation Loss: 0.1993\n",
      "Epoch #9, Batch #550, Train Loss: 0.0947, Validation Loss: 0.2349\n",
      "Epoch #9, Batch #560, Train Loss: 0.0393, Validation Loss: 0.8482\n",
      "Epoch #9, Batch #570, Train Loss: 0.0584, Validation Loss: 0.1581\n",
      "Epoch #9, Batch #580, Train Loss: 0.0654, Validation Loss: 0.0514\n",
      "Epoch #9, Batch #590, Train Loss: 0.2992, Validation Loss: 0.2232\n",
      "Epoch #9, Batch #600, Train Loss: 0.0685, Validation Loss: 0.0344\n",
      "Epoch #9, Batch #610, Train Loss: 0.0061, Validation Loss: 0.4202\n",
      "Epoch #9, Batch #620, Train Loss: 0.0204, Validation Loss: 0.1282\n",
      "Epoch #9, Batch #630, Train Loss: 0.0502, Validation Loss: 0.1989\n",
      "Epoch #9, Batch #640, Train Loss: 1.1297, Validation Loss: 0.0736\n",
      "Epoch #9, Batch #650, Train Loss: 0.1723, Validation Loss: 8.1542\n",
      "Epoch #9, Batch #660, Train Loss: 0.1020, Validation Loss: 5.5068\n",
      "Epoch #9, Batch #670, Train Loss: 0.4049, Validation Loss: 0.1463\n",
      "Epoch #9, Batch #680, Train Loss: 0.0167, Validation Loss: 0.0056\n",
      "Epoch #9, Batch #690, Train Loss: 0.1100, Validation Loss: 0.3892\n",
      "Epoch #9, Batch #700, Train Loss: 0.0320, Validation Loss: 0.2449\n",
      "Epoch #9, Batch #710, Train Loss: 0.0575, Validation Loss: 7.6540\n",
      "Epoch #9, Batch #720, Train Loss: 0.2665, Validation Loss: 0.1269\n",
      "Epoch #9, Batch #730, Train Loss: 2.1104, Validation Loss: 0.6737\n",
      "Epoch #9, Batch #740, Train Loss: 0.1114, Validation Loss: 0.3703\n",
      "Epoch #9, Batch #750, Train Loss: 0.0953, Validation Loss: 0.6471\n",
      "Epoch #9, Batch #760, Train Loss: 0.1372, Validation Loss: 0.5290\n",
      "Epoch #9, Batch #770, Train Loss: 0.2173, Validation Loss: 0.2393\n",
      "Epoch #9, Batch #780, Train Loss: 0.0150, Validation Loss: 4.1913\n",
      "Epoch #9, Batch #790, Train Loss: 0.0482, Validation Loss: 0.2896\n",
      "Epoch #9, Batch #800, Train Loss: 1.7158, Validation Loss: 0.0732\n",
      "Epoch #9, Batch #810, Train Loss: 0.1462, Validation Loss: 0.0982\n",
      "Epoch #9, Batch #820, Train Loss: 0.1396, Validation Loss: 0.0526\n",
      "Epoch #9, Batch #830, Train Loss: 0.0254, Validation Loss: 0.2490\n",
      "Epoch #9, Batch #840, Train Loss: 0.0482, Validation Loss: 0.0904\n",
      "Epoch #9, Batch #850, Train Loss: 0.6967, Validation Loss: 0.4100\n",
      "Epoch #9, Batch #860, Train Loss: 0.0804, Validation Loss: 0.0220\n",
      "Epoch #9, Batch #870, Train Loss: 0.0143, Validation Loss: 0.1296\n",
      "Epoch #9, Batch #880, Train Loss: 0.0939, Validation Loss: 0.1405\n",
      "Epoch #9, Batch #890, Train Loss: 0.2378, Validation Loss: 0.1332\n",
      "Epoch #9, Batch #900, Train Loss: 0.0699, Validation Loss: 0.0880\n",
      "Epoch #9, Batch #910, Train Loss: 0.1923, Validation Loss: 0.1133\n",
      "Epoch #9, Batch #920, Train Loss: 0.2771, Validation Loss: 0.4098\n",
      "Epoch #9, Batch #930, Train Loss: 0.0716, Validation Loss: 0.6161\n",
      "Epoch #9, Batch #940, Train Loss: 0.6042, Validation Loss: 0.3052\n",
      "Epoch #9, Batch #950, Train Loss: 0.0174, Validation Loss: 1.5245\n",
      "Epoch #9, Batch #960, Train Loss: 0.3857, Validation Loss: 0.1267\n",
      "Epoch #9, Batch #970, Train Loss: 0.1124, Validation Loss: 5.8551\n",
      "Epoch #9, Batch #980, Train Loss: 0.0784, Validation Loss: 0.2353\n",
      "Epoch #9, Batch #990, Train Loss: 0.0247, Validation Loss: 0.1719\n",
      "Epoch #9, Batch #1000, Train Loss: 0.0698, Validation Loss: 0.4462\n",
      "Epoch #9, Batch #1010, Train Loss: 0.0665, Validation Loss: 0.9571\n",
      "Epoch #9, Batch #1020, Train Loss: 4.0717, Validation Loss: 0.2493\n",
      "Epoch #9, Batch #1030, Train Loss: 0.5291, Validation Loss: 0.0265\n",
      "Epoch #9, Batch #1040, Train Loss: 0.0139, Validation Loss: 0.5952\n",
      "Epoch #9, Batch #1050, Train Loss: 0.1893, Validation Loss: 4.6400\n",
      "Epoch #9, Batch #1060, Train Loss: 0.0069, Validation Loss: 7.1777\n",
      "Epoch #9, Batch #1070, Train Loss: 0.0149, Validation Loss: 0.1444\n",
      "Epoch #9, Batch #1080, Train Loss: 0.1654, Validation Loss: 0.0188\n",
      "Epoch #9, Batch #1090, Train Loss: 0.1954, Validation Loss: 0.1132\n",
      "Epoch #9, Batch #1100, Train Loss: 0.1686, Validation Loss: 0.2834\n",
      "Epoch #9, Batch #1110, Train Loss: 0.0020, Validation Loss: 0.2085\n",
      "Epoch #9, Batch #1120, Train Loss: 0.1711, Validation Loss: 1.4992\n",
      "Epoch #9, Batch #1130, Train Loss: 0.0123, Validation Loss: 7.6246\n",
      "Epoch #9, Batch #1140, Train Loss: 2.5800, Validation Loss: 2.7698\n",
      "Epoch #9, Batch #1150, Train Loss: 0.0384, Validation Loss: 6.3152\n",
      "Epoch #9, Batch #1160, Train Loss: 0.0159, Validation Loss: 0.1871\n",
      "Epoch #9, Batch #1170, Train Loss: 0.0743, Validation Loss: 0.3649\n",
      "Epoch #9, Batch #1180, Train Loss: 0.1157, Validation Loss: 0.0106\n",
      "Epoch #9, Batch #1190, Train Loss: 0.0222, Validation Loss: 0.3017\n",
      "Epoch #9, Batch #1200, Train Loss: 0.0725, Validation Loss: 0.1485\n",
      "Epoch #9, Batch #1210, Train Loss: 1.7431, Validation Loss: 0.0489\n",
      "Epoch #9, Batch #1220, Train Loss: 0.3139, Validation Loss: 0.0061\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "early_stopper = EarlyStopper(patience=3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  for i, (x, t) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    model.refresh_hidden()\n",
    "    y = model(x)\n",
    "    y = y[:,-1,:]\n",
    "    loss_train = criterion(y, t)\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "      for x, t in validation_loader:\n",
    "        y = model(x)\n",
    "        y = y[:,-1,:]\n",
    "        loss_validation = criterion(y, t)\n",
    "        if early_stopper.early_stop(loss_validation):             \n",
    "          break\n",
    "\n",
    "      print(f\"Epoch #{epoch}, Batch #{i}, Train Loss: {loss_train.item():.4f}, Validation Loss: {loss_validation.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.559599199837872"
      ]
     },
     "execution_count": 712,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "torch.no_grad()\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "item_count = len(test.indices)\n",
    "loss_total = 0\n",
    "\n",
    "for n in range(item_count):\n",
    "  x = dataset.data[test.indices[n]].reshape(1, 10, 5)\n",
    "  t = dataset.target[test.indices[n]]\n",
    "\n",
    "  y = model(x)\n",
    "  loss = criterion(y, t)\n",
    "  loss_total += loss.item()\n",
    "\n",
    "loss_total / item_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y=31.5742, t=30.7450'"
      ]
     },
     "execution_count": 726,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "x, t=next(iter(test_loader))\n",
    "t=t * target_std + target_mean\n",
    "\n",
    "y=model(x)\n",
    "y=y * target_std + target_mean\n",
    "\n",
    "y=y[:,-1,:][0][0].item()\n",
    "t=t[0][0].item()\n",
    "\n",
    "f\"y={y:.4f}, t={t:.4f}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
