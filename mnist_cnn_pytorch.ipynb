{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "with open(\"./datasets/mnist/train-images-2d.pkl\", \"rb\") as f:\n",
    "  train_x = pickle.load(f)\n",
    "\n",
    "with open(\"./datasets/mnist/train-labels.pkl\", \"rb\") as f:\n",
    "  train_t = pickle.load(f)\n",
    "\n",
    "with open(\"./datasets/mnist/test-images-2d.pkl\", \"rb\") as f:\n",
    "  test_x = pickle.load(f)\n",
    "\n",
    "with open(\"./datasets/mnist/test-labels.pkl\", \"rb\") as f:\n",
    "  test_t = pickle.load(f)\n",
    "  \n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "  def __init__(self, data, label) :\n",
    "    self.data = data\n",
    "    self.label = label\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx) :\n",
    "    data = torch.FloatTensor(self.data[idx])\n",
    "    label = torch.FloatTensor(self.label[idx])\n",
    "\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_x, train_t)\n",
    "test_dataset = Dataset(test_x, test_t)\n",
    "\n",
    "train, validation = torch.utils.data.random_split(train_dataset, [0.9, 0.1])\n",
    "\n",
    "train_loader = data.DataLoader(train, batch_size=100, shuffle=True)\n",
    "validation_loader = data.DataLoader(validation, batch_size=100, shuffle=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layers): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(15, 15))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): ReLU()\n",
       "    (6): Dropout(p=0.5, inplace=False)\n",
       "    (7): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): ReLU()\n",
       "    (10): Dropout(p=0.5, inplace=False)\n",
       "    (11): Flatten(start_dim=1, end_dim=-1)\n",
       "    (12): Linear(in_features=576, out_features=100, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Dropout(p=0.5, inplace=False)\n",
       "    (15): Linear(in_features=100, out_features=10, bias=True)\n",
       "    (16): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conv2d(in_channel, out_channel, (patch_x, patch_y), stride, padding)\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Conv2d(1, 32, 5, padding=15),\n",
    "      nn.MaxPool2d(2),\n",
    "      nn.ReLU(),\n",
    "\n",
    "      nn.Conv2d(32, 32, 5),\n",
    "      nn.MaxPool2d(2),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(0.5),\n",
    "  \n",
    "      nn.Conv2d(32, 64, 5),\n",
    "      nn.MaxPool2d(2),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(0.5),\n",
    "\n",
    "      nn.Flatten(1),\n",
    "      nn.Linear(576, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(0.5),\n",
    "\n",
    "      nn.Linear(100, 10),\n",
    "      nn.Softmax(dim=1),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    y=self.layers(x)\n",
    "    return y\n",
    "  \n",
    "model = Model()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, Batch #0, Train Loss: 2.3016, Validation Loss: 2.3024\n",
      "Epoch #0, Batch #10, Train Loss: 2.3003, Validation Loss: 2.2995\n",
      "Epoch #0, Batch #20, Train Loss: 2.2784, Validation Loss: 2.2835\n",
      "Epoch #0, Batch #30, Train Loss: 2.2122, Validation Loss: 2.2472\n",
      "Epoch #0, Batch #40, Train Loss: 2.1776, Validation Loss: 2.1480\n",
      "Epoch #0, Batch #50, Train Loss: 2.0313, Validation Loss: 2.0919\n",
      "Epoch #0, Batch #60, Train Loss: 2.0119, Validation Loss: 1.9527\n",
      "Epoch #0, Batch #70, Train Loss: 1.9933, Validation Loss: 1.9192\n",
      "Epoch #0, Batch #80, Train Loss: 1.8823, Validation Loss: 1.9807\n",
      "Epoch #0, Batch #90, Train Loss: 1.8466, Validation Loss: 1.9045\n",
      "Epoch #0, Batch #100, Train Loss: 1.8873, Validation Loss: 1.8436\n",
      "Epoch #0, Batch #110, Train Loss: 1.8527, Validation Loss: 1.7876\n",
      "Epoch #0, Batch #120, Train Loss: 1.7763, Validation Loss: 1.8092\n",
      "Epoch #0, Batch #130, Train Loss: 1.7613, Validation Loss: 1.8460\n",
      "Epoch #0, Batch #140, Train Loss: 1.8142, Validation Loss: 1.7596\n",
      "Epoch #0, Batch #150, Train Loss: 1.8101, Validation Loss: 1.7634\n",
      "Epoch #0, Batch #160, Train Loss: 1.8039, Validation Loss: 1.7442\n",
      "Epoch #0, Batch #170, Train Loss: 1.7558, Validation Loss: 1.8361\n",
      "Epoch #0, Batch #180, Train Loss: 1.7328, Validation Loss: 1.7880\n",
      "Epoch #0, Batch #190, Train Loss: 1.7843, Validation Loss: 1.7215\n",
      "Epoch #0, Batch #200, Train Loss: 1.6743, Validation Loss: 1.7159\n",
      "Epoch #0, Batch #210, Train Loss: 1.6399, Validation Loss: 1.6299\n",
      "Epoch #0, Batch #220, Train Loss: 1.6673, Validation Loss: 1.6317\n",
      "Epoch #0, Batch #230, Train Loss: 1.6431, Validation Loss: 1.6252\n",
      "Epoch #0, Batch #240, Train Loss: 1.6341, Validation Loss: 1.6046\n",
      "Epoch #0, Batch #250, Train Loss: 1.7052, Validation Loss: 1.6654\n",
      "Epoch #0, Batch #260, Train Loss: 1.5709, Validation Loss: 1.5981\n",
      "Epoch #0, Batch #270, Train Loss: 1.6689, Validation Loss: 1.6045\n",
      "Epoch #0, Batch #280, Train Loss: 1.6317, Validation Loss: 1.6513\n",
      "Epoch #0, Batch #290, Train Loss: 1.5913, Validation Loss: 1.6002\n",
      "Epoch #0, Batch #300, Train Loss: 1.5625, Validation Loss: 1.5850\n",
      "Epoch #0, Batch #310, Train Loss: 1.6461, Validation Loss: 1.5481\n",
      "Epoch #0, Batch #320, Train Loss: 1.6293, Validation Loss: 1.5501\n",
      "Epoch #0, Batch #330, Train Loss: 1.6206, Validation Loss: 1.5787\n",
      "Epoch #0, Batch #340, Train Loss: 1.5731, Validation Loss: 1.5002\n",
      "Epoch #0, Batch #350, Train Loss: 1.5494, Validation Loss: 1.5531\n",
      "Epoch #0, Batch #360, Train Loss: 1.5937, Validation Loss: 1.5688\n",
      "Epoch #0, Batch #370, Train Loss: 1.6221, Validation Loss: 1.5976\n",
      "Epoch #0, Batch #380, Train Loss: 1.5771, Validation Loss: 1.5819\n",
      "Epoch #0, Batch #390, Train Loss: 1.5642, Validation Loss: 1.5761\n",
      "Epoch #0, Batch #400, Train Loss: 1.5925, Validation Loss: 1.5666\n",
      "Epoch #0, Batch #410, Train Loss: 1.5779, Validation Loss: 1.5555\n",
      "Epoch #0, Batch #420, Train Loss: 1.6040, Validation Loss: 1.5469\n",
      "Epoch #0, Batch #430, Train Loss: 1.5109, Validation Loss: 1.5796\n",
      "Epoch #0, Batch #440, Train Loss: 1.5911, Validation Loss: 1.5203\n",
      "Epoch #0, Batch #450, Train Loss: 1.5423, Validation Loss: 1.5465\n",
      "Epoch #0, Batch #460, Train Loss: 1.5282, Validation Loss: 1.5294\n",
      "Epoch #0, Batch #470, Train Loss: 1.5297, Validation Loss: 1.5413\n",
      "Epoch #0, Batch #480, Train Loss: 1.5326, Validation Loss: 1.5431\n",
      "Epoch #0, Batch #490, Train Loss: 1.5444, Validation Loss: 1.5386\n",
      "Epoch #0, Batch #500, Train Loss: 1.5564, Validation Loss: 1.5795\n",
      "Epoch #0, Batch #510, Train Loss: 1.4928, Validation Loss: 1.5615\n",
      "Epoch #0, Batch #520, Train Loss: 1.5878, Validation Loss: 1.5568\n",
      "Epoch #0, Batch #530, Train Loss: 1.5168, Validation Loss: 1.5692\n",
      "Epoch #1, Batch #0, Train Loss: 1.5215, Validation Loss: 1.5509\n",
      "Epoch #1, Batch #10, Train Loss: 1.5769, Validation Loss: 1.5654\n",
      "Epoch #1, Batch #20, Train Loss: 1.5521, Validation Loss: 1.5360\n",
      "Epoch #1, Batch #30, Train Loss: 1.5255, Validation Loss: 1.5382\n",
      "Epoch #1, Batch #40, Train Loss: 1.5423, Validation Loss: 1.5441\n",
      "Epoch #1, Batch #50, Train Loss: 1.5578, Validation Loss: 1.5290\n",
      "Epoch #1, Batch #60, Train Loss: 1.5023, Validation Loss: 1.5615\n",
      "Epoch #1, Batch #70, Train Loss: 1.5277, Validation Loss: 1.5512\n",
      "Epoch #1, Batch #80, Train Loss: 1.5537, Validation Loss: 1.5388\n",
      "Epoch #1, Batch #90, Train Loss: 1.5182, Validation Loss: 1.5148\n",
      "Epoch #1, Batch #100, Train Loss: 1.5631, Validation Loss: 1.5510\n",
      "Epoch #1, Batch #110, Train Loss: 1.5434, Validation Loss: 1.5676\n",
      "Epoch #1, Batch #120, Train Loss: 1.5470, Validation Loss: 1.5187\n",
      "Epoch #1, Batch #130, Train Loss: 1.5367, Validation Loss: 1.5302\n",
      "Epoch #1, Batch #140, Train Loss: 1.5250, Validation Loss: 1.4934\n",
      "Epoch #1, Batch #150, Train Loss: 1.5434, Validation Loss: 1.5890\n",
      "Epoch #1, Batch #160, Train Loss: 1.5251, Validation Loss: 1.5535\n",
      "Epoch #1, Batch #170, Train Loss: 1.5374, Validation Loss: 1.5519\n",
      "Epoch #1, Batch #180, Train Loss: 1.5462, Validation Loss: 1.5683\n",
      "Epoch #1, Batch #190, Train Loss: 1.5790, Validation Loss: 1.5376\n",
      "Epoch #1, Batch #200, Train Loss: 1.5085, Validation Loss: 1.5168\n",
      "Epoch #1, Batch #210, Train Loss: 1.5135, Validation Loss: 1.5399\n",
      "Epoch #1, Batch #220, Train Loss: 1.5339, Validation Loss: 1.5476\n",
      "Epoch #1, Batch #230, Train Loss: 1.5028, Validation Loss: 1.5624\n",
      "Epoch #1, Batch #240, Train Loss: 1.5410, Validation Loss: 1.5335\n",
      "Epoch #1, Batch #250, Train Loss: 1.5065, Validation Loss: 1.5489\n",
      "Epoch #1, Batch #260, Train Loss: 1.5396, Validation Loss: 1.5233\n",
      "Epoch #1, Batch #270, Train Loss: 1.5394, Validation Loss: 1.5543\n",
      "Epoch #1, Batch #280, Train Loss: 1.5410, Validation Loss: 1.5006\n",
      "Epoch #1, Batch #290, Train Loss: 1.5235, Validation Loss: 1.5262\n",
      "Epoch #1, Batch #300, Train Loss: 1.5539, Validation Loss: 1.5483\n",
      "Epoch #1, Batch #310, Train Loss: 1.5248, Validation Loss: 1.5465\n",
      "Epoch #1, Batch #320, Train Loss: 1.5045, Validation Loss: 1.5070\n",
      "Epoch #1, Batch #330, Train Loss: 1.5213, Validation Loss: 1.5548\n",
      "Epoch #1, Batch #340, Train Loss: 1.4969, Validation Loss: 1.5104\n",
      "Epoch #1, Batch #350, Train Loss: 1.5349, Validation Loss: 1.5648\n",
      "Epoch #1, Batch #360, Train Loss: 1.4916, Validation Loss: 1.5063\n",
      "Epoch #1, Batch #370, Train Loss: 1.5081, Validation Loss: 1.5604\n",
      "Epoch #1, Batch #380, Train Loss: 1.5037, Validation Loss: 1.5408\n",
      "Epoch #1, Batch #390, Train Loss: 1.5259, Validation Loss: 1.5424\n",
      "Epoch #1, Batch #400, Train Loss: 1.4819, Validation Loss: 1.5177\n",
      "Epoch #1, Batch #410, Train Loss: 1.5026, Validation Loss: 1.5239\n",
      "Epoch #1, Batch #420, Train Loss: 1.5281, Validation Loss: 1.5389\n",
      "Epoch #1, Batch #430, Train Loss: 1.5108, Validation Loss: 1.5585\n",
      "Epoch #1, Batch #440, Train Loss: 1.5307, Validation Loss: 1.5192\n",
      "Epoch #1, Batch #450, Train Loss: 1.5316, Validation Loss: 1.5634\n",
      "Epoch #1, Batch #460, Train Loss: 1.5124, Validation Loss: 1.5431\n",
      "Epoch #1, Batch #470, Train Loss: 1.5234, Validation Loss: 1.5258\n",
      "Epoch #1, Batch #480, Train Loss: 1.5012, Validation Loss: 1.4792\n",
      "Epoch #1, Batch #490, Train Loss: 1.5234, Validation Loss: 1.5466\n",
      "Epoch #1, Batch #500, Train Loss: 1.5019, Validation Loss: 1.5184\n",
      "Epoch #1, Batch #510, Train Loss: 1.5307, Validation Loss: 1.5011\n",
      "Epoch #1, Batch #520, Train Loss: 1.5149, Validation Loss: 1.5033\n",
      "Epoch #1, Batch #530, Train Loss: 1.4955, Validation Loss: 1.5320\n",
      "Epoch #2, Batch #0, Train Loss: 1.5479, Validation Loss: 1.5289\n",
      "Epoch #2, Batch #10, Train Loss: 1.5190, Validation Loss: 1.5205\n",
      "Epoch #2, Batch #20, Train Loss: 1.5670, Validation Loss: 1.5863\n",
      "Epoch #2, Batch #30, Train Loss: 1.5386, Validation Loss: 1.5424\n",
      "Epoch #2, Batch #40, Train Loss: 1.4800, Validation Loss: 1.5440\n",
      "Epoch #2, Batch #50, Train Loss: 1.5218, Validation Loss: 1.5612\n",
      "Epoch #2, Batch #60, Train Loss: 1.5200, Validation Loss: 1.4861\n",
      "Epoch #2, Batch #70, Train Loss: 1.5360, Validation Loss: 1.5562\n",
      "Epoch #2, Batch #80, Train Loss: 1.5159, Validation Loss: 1.5184\n",
      "Epoch #2, Batch #90, Train Loss: 1.4953, Validation Loss: 1.5661\n",
      "Epoch #2, Batch #100, Train Loss: 1.5270, Validation Loss: 1.5687\n",
      "Epoch #2, Batch #110, Train Loss: 1.5406, Validation Loss: 1.5600\n",
      "Epoch #2, Batch #120, Train Loss: 1.5054, Validation Loss: 1.5229\n",
      "Epoch #2, Batch #130, Train Loss: 1.5528, Validation Loss: 1.5390\n",
      "Epoch #2, Batch #140, Train Loss: 1.5157, Validation Loss: 1.5126\n",
      "Epoch #2, Batch #150, Train Loss: 1.5161, Validation Loss: 1.5116\n",
      "Epoch #2, Batch #160, Train Loss: 1.4818, Validation Loss: 1.5315\n",
      "Epoch #2, Batch #170, Train Loss: 1.5042, Validation Loss: 1.5584\n",
      "Epoch #2, Batch #180, Train Loss: 1.5048, Validation Loss: 1.5005\n",
      "Epoch #2, Batch #190, Train Loss: 1.4904, Validation Loss: 1.5368\n",
      "Epoch #2, Batch #200, Train Loss: 1.5577, Validation Loss: 1.5324\n",
      "Epoch #2, Batch #210, Train Loss: 1.5113, Validation Loss: 1.4966\n",
      "Epoch #2, Batch #220, Train Loss: 1.5351, Validation Loss: 1.5080\n",
      "Epoch #2, Batch #230, Train Loss: 1.4849, Validation Loss: 1.5388\n",
      "Epoch #2, Batch #240, Train Loss: 1.4899, Validation Loss: 1.5126\n",
      "Epoch #2, Batch #250, Train Loss: 1.5118, Validation Loss: 1.5376\n",
      "Epoch #2, Batch #260, Train Loss: 1.5722, Validation Loss: 1.4913\n",
      "Epoch #2, Batch #270, Train Loss: 1.5138, Validation Loss: 1.5251\n",
      "Epoch #2, Batch #280, Train Loss: 1.5648, Validation Loss: 1.5095\n",
      "Epoch #2, Batch #290, Train Loss: 1.4836, Validation Loss: 1.5250\n",
      "Epoch #2, Batch #300, Train Loss: 1.4883, Validation Loss: 1.5687\n",
      "Epoch #2, Batch #310, Train Loss: 1.5517, Validation Loss: 1.5053\n",
      "Epoch #2, Batch #320, Train Loss: 1.4881, Validation Loss: 1.5273\n",
      "Epoch #2, Batch #330, Train Loss: 1.5301, Validation Loss: 1.5210\n",
      "Epoch #2, Batch #340, Train Loss: 1.4961, Validation Loss: 1.5394\n",
      "Epoch #2, Batch #350, Train Loss: 1.5060, Validation Loss: 1.5034\n",
      "Epoch #2, Batch #360, Train Loss: 1.4898, Validation Loss: 1.5556\n",
      "Epoch #2, Batch #370, Train Loss: 1.5574, Validation Loss: 1.5445\n",
      "Epoch #2, Batch #380, Train Loss: 1.4787, Validation Loss: 1.5349\n",
      "Epoch #2, Batch #390, Train Loss: 1.4887, Validation Loss: 1.5311\n",
      "Epoch #2, Batch #400, Train Loss: 1.5267, Validation Loss: 1.5046\n",
      "Epoch #2, Batch #410, Train Loss: 1.5388, Validation Loss: 1.5465\n",
      "Epoch #2, Batch #420, Train Loss: 1.4845, Validation Loss: 1.4955\n",
      "Epoch #2, Batch #430, Train Loss: 1.5109, Validation Loss: 1.5151\n",
      "Epoch #2, Batch #440, Train Loss: 1.4995, Validation Loss: 1.5465\n",
      "Epoch #2, Batch #450, Train Loss: 1.5475, Validation Loss: 1.5348\n",
      "Epoch #2, Batch #460, Train Loss: 1.5257, Validation Loss: 1.5508\n",
      "Epoch #2, Batch #470, Train Loss: 1.5165, Validation Loss: 1.5354\n",
      "Epoch #2, Batch #480, Train Loss: 1.5248, Validation Loss: 1.5182\n",
      "Epoch #2, Batch #490, Train Loss: 1.5023, Validation Loss: 1.5464\n",
      "Epoch #2, Batch #500, Train Loss: 1.4919, Validation Loss: 1.5208\n",
      "Epoch #2, Batch #510, Train Loss: 1.5430, Validation Loss: 1.5110\n",
      "Epoch #2, Batch #520, Train Loss: 1.5080, Validation Loss: 1.5298\n",
      "Epoch #2, Batch #530, Train Loss: 1.5372, Validation Loss: 1.5457\n",
      "Epoch #3, Batch #0, Train Loss: 1.5046, Validation Loss: 1.5585\n",
      "Epoch #3, Batch #10, Train Loss: 1.4957, Validation Loss: 1.4918\n",
      "Epoch #3, Batch #20, Train Loss: 1.5106, Validation Loss: 1.4888\n",
      "Epoch #3, Batch #30, Train Loss: 1.5129, Validation Loss: 1.5203\n",
      "Epoch #3, Batch #40, Train Loss: 1.5440, Validation Loss: 1.5141\n",
      "Epoch #3, Batch #50, Train Loss: 1.5261, Validation Loss: 1.5311\n",
      "Epoch #3, Batch #60, Train Loss: 1.5247, Validation Loss: 1.5314\n",
      "Epoch #3, Batch #70, Train Loss: 1.5233, Validation Loss: 1.5835\n",
      "Epoch #3, Batch #80, Train Loss: 1.5077, Validation Loss: 1.5383\n",
      "Epoch #3, Batch #90, Train Loss: 1.5125, Validation Loss: 1.5038\n",
      "Epoch #3, Batch #100, Train Loss: 1.4829, Validation Loss: 1.4802\n",
      "Epoch #3, Batch #110, Train Loss: 1.5021, Validation Loss: 1.5100\n",
      "Epoch #3, Batch #120, Train Loss: 1.4792, Validation Loss: 1.5014\n",
      "Epoch #3, Batch #130, Train Loss: 1.4816, Validation Loss: 1.4806\n",
      "Epoch #3, Batch #140, Train Loss: 1.5328, Validation Loss: 1.4922\n",
      "Epoch #3, Batch #150, Train Loss: 1.5288, Validation Loss: 1.4954\n",
      "Epoch #3, Batch #160, Train Loss: 1.4994, Validation Loss: 1.5085\n",
      "Epoch #3, Batch #170, Train Loss: 1.5129, Validation Loss: 1.5210\n",
      "Epoch #3, Batch #180, Train Loss: 1.5689, Validation Loss: 1.5042\n",
      "Epoch #3, Batch #190, Train Loss: 1.4698, Validation Loss: 1.5351\n",
      "Epoch #3, Batch #200, Train Loss: 1.5032, Validation Loss: 1.5248\n",
      "Epoch #3, Batch #210, Train Loss: 1.5045, Validation Loss: 1.5256\n",
      "Epoch #3, Batch #220, Train Loss: 1.5139, Validation Loss: 1.5109\n",
      "Epoch #3, Batch #230, Train Loss: 1.5173, Validation Loss: 1.4852\n",
      "Epoch #3, Batch #240, Train Loss: 1.5110, Validation Loss: 1.4865\n",
      "Epoch #3, Batch #250, Train Loss: 1.5085, Validation Loss: 1.5205\n",
      "Epoch #3, Batch #260, Train Loss: 1.4819, Validation Loss: 1.5331\n",
      "Epoch #3, Batch #270, Train Loss: 1.5001, Validation Loss: 1.5118\n",
      "Epoch #3, Batch #280, Train Loss: 1.5539, Validation Loss: 1.5261\n",
      "Epoch #3, Batch #290, Train Loss: 1.5679, Validation Loss: 1.4818\n",
      "Epoch #3, Batch #300, Train Loss: 1.5237, Validation Loss: 1.5669\n",
      "Epoch #3, Batch #310, Train Loss: 1.5197, Validation Loss: 1.4924\n",
      "Epoch #3, Batch #320, Train Loss: 1.5122, Validation Loss: 1.4925\n",
      "Epoch #3, Batch #330, Train Loss: 1.5346, Validation Loss: 1.5026\n",
      "Epoch #3, Batch #340, Train Loss: 1.5216, Validation Loss: 1.5253\n",
      "Epoch #3, Batch #350, Train Loss: 1.5376, Validation Loss: 1.5087\n",
      "Epoch #3, Batch #360, Train Loss: 1.5110, Validation Loss: 1.5050\n",
      "Epoch #3, Batch #370, Train Loss: 1.4727, Validation Loss: 1.5135\n",
      "Epoch #3, Batch #380, Train Loss: 1.4820, Validation Loss: 1.5555\n",
      "Epoch #3, Batch #390, Train Loss: 1.4993, Validation Loss: 1.5064\n",
      "Epoch #3, Batch #400, Train Loss: 1.5487, Validation Loss: 1.5295\n",
      "Epoch #3, Batch #410, Train Loss: 1.5163, Validation Loss: 1.4957\n",
      "Epoch #3, Batch #420, Train Loss: 1.4730, Validation Loss: 1.5132\n",
      "Epoch #3, Batch #430, Train Loss: 1.4985, Validation Loss: 1.5625\n",
      "Epoch #3, Batch #440, Train Loss: 1.4799, Validation Loss: 1.5168\n",
      "Epoch #3, Batch #450, Train Loss: 1.5135, Validation Loss: 1.5504\n",
      "Epoch #3, Batch #460, Train Loss: 1.5200, Validation Loss: 1.5093\n",
      "Epoch #3, Batch #470, Train Loss: 1.5238, Validation Loss: 1.5092\n",
      "Epoch #3, Batch #480, Train Loss: 1.5001, Validation Loss: 1.4997\n",
      "Epoch #3, Batch #490, Train Loss: 1.4828, Validation Loss: 1.5039\n",
      "Epoch #3, Batch #500, Train Loss: 1.4934, Validation Loss: 1.5415\n",
      "Epoch #3, Batch #510, Train Loss: 1.5151, Validation Loss: 1.5087\n",
      "Epoch #3, Batch #520, Train Loss: 1.5358, Validation Loss: 1.5206\n",
      "Epoch #3, Batch #530, Train Loss: 1.5726, Validation Loss: 1.5298\n",
      "Epoch #4, Batch #0, Train Loss: 1.5222, Validation Loss: 1.5739\n",
      "Epoch #4, Batch #10, Train Loss: 1.4721, Validation Loss: 1.5181\n",
      "Epoch #4, Batch #20, Train Loss: 1.5468, Validation Loss: 1.5102\n",
      "Epoch #4, Batch #30, Train Loss: 1.5120, Validation Loss: 1.4914\n",
      "Epoch #4, Batch #40, Train Loss: 1.4968, Validation Loss: 1.5263\n",
      "Epoch #4, Batch #50, Train Loss: 1.5203, Validation Loss: 1.5212\n",
      "Epoch #4, Batch #60, Train Loss: 1.5491, Validation Loss: 1.4813\n",
      "Epoch #4, Batch #70, Train Loss: 1.5309, Validation Loss: 1.4810\n",
      "Epoch #4, Batch #80, Train Loss: 1.4935, Validation Loss: 1.5501\n",
      "Epoch #4, Batch #90, Train Loss: 1.5070, Validation Loss: 1.4795\n",
      "Epoch #4, Batch #100, Train Loss: 1.5237, Validation Loss: 1.5216\n",
      "Epoch #4, Batch #110, Train Loss: 1.4894, Validation Loss: 1.5237\n",
      "Epoch #4, Batch #120, Train Loss: 1.5161, Validation Loss: 1.4879\n",
      "Epoch #4, Batch #130, Train Loss: 1.4750, Validation Loss: 1.5221\n",
      "Epoch #4, Batch #140, Train Loss: 1.5182, Validation Loss: 1.5165\n",
      "Epoch #4, Batch #150, Train Loss: 1.4715, Validation Loss: 1.4878\n",
      "Epoch #4, Batch #160, Train Loss: 1.5025, Validation Loss: 1.5270\n",
      "Epoch #4, Batch #170, Train Loss: 1.5078, Validation Loss: 1.5378\n",
      "Epoch #4, Batch #180, Train Loss: 1.5243, Validation Loss: 1.4786\n",
      "Epoch #4, Batch #190, Train Loss: 1.5309, Validation Loss: 1.5253\n",
      "Epoch #4, Batch #200, Train Loss: 1.5275, Validation Loss: 1.5044\n",
      "Epoch #4, Batch #210, Train Loss: 1.5102, Validation Loss: 1.4982\n",
      "Epoch #4, Batch #220, Train Loss: 1.5198, Validation Loss: 1.5166\n",
      "Epoch #4, Batch #230, Train Loss: 1.5296, Validation Loss: 1.4729\n",
      "Epoch #4, Batch #240, Train Loss: 1.5090, Validation Loss: 1.5223\n",
      "Epoch #4, Batch #250, Train Loss: 1.5207, Validation Loss: 1.5220\n",
      "Epoch #4, Batch #260, Train Loss: 1.5114, Validation Loss: 1.4985\n",
      "Epoch #4, Batch #270, Train Loss: 1.4717, Validation Loss: 1.5016\n",
      "Epoch #4, Batch #280, Train Loss: 1.5006, Validation Loss: 1.4982\n",
      "Epoch #4, Batch #290, Train Loss: 1.5025, Validation Loss: 1.5212\n",
      "Epoch #4, Batch #300, Train Loss: 1.5143, Validation Loss: 1.5128\n",
      "Epoch #4, Batch #310, Train Loss: 1.5135, Validation Loss: 1.5153\n",
      "Epoch #4, Batch #320, Train Loss: 1.5159, Validation Loss: 1.4970\n",
      "Epoch #4, Batch #330, Train Loss: 1.5301, Validation Loss: 1.5081\n",
      "Epoch #4, Batch #340, Train Loss: 1.4957, Validation Loss: 1.5194\n",
      "Epoch #4, Batch #350, Train Loss: 1.5196, Validation Loss: 1.4824\n",
      "Epoch #4, Batch #360, Train Loss: 1.5029, Validation Loss: 1.5000\n",
      "Epoch #4, Batch #370, Train Loss: 1.4918, Validation Loss: 1.4918\n",
      "Epoch #4, Batch #380, Train Loss: 1.4914, Validation Loss: 1.4871\n",
      "Epoch #4, Batch #390, Train Loss: 1.5463, Validation Loss: 1.5206\n",
      "Epoch #4, Batch #400, Train Loss: 1.5424, Validation Loss: 1.5017\n",
      "Epoch #4, Batch #410, Train Loss: 1.4832, Validation Loss: 1.5119\n",
      "Epoch #4, Batch #420, Train Loss: 1.5108, Validation Loss: 1.5194\n",
      "Epoch #4, Batch #430, Train Loss: 1.5015, Validation Loss: 1.5495\n",
      "Epoch #4, Batch #440, Train Loss: 1.4843, Validation Loss: 1.5611\n",
      "Epoch #4, Batch #450, Train Loss: 1.5029, Validation Loss: 1.4984\n",
      "Epoch #4, Batch #460, Train Loss: 1.4929, Validation Loss: 1.5363\n",
      "Epoch #4, Batch #470, Train Loss: 1.5229, Validation Loss: 1.5063\n",
      "Epoch #4, Batch #480, Train Loss: 1.4721, Validation Loss: 1.5215\n",
      "Epoch #4, Batch #490, Train Loss: 1.4786, Validation Loss: 1.5048\n",
      "Epoch #4, Batch #500, Train Loss: 1.5176, Validation Loss: 1.5316\n",
      "Epoch #4, Batch #510, Train Loss: 1.4813, Validation Loss: 1.4992\n",
      "Epoch #4, Batch #520, Train Loss: 1.5290, Validation Loss: 1.4881\n",
      "Epoch #4, Batch #530, Train Loss: 1.4929, Validation Loss: 1.4750\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(5):\n",
    "  for i, (x, t) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    y = model(x)\n",
    "    loss_train = criterion(y, t)\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "      for x, t in validation_loader:\n",
    "        y = model(x)\n",
    "        loss_validation = criterion(y, t)\n",
    "\n",
    "      print(f\"Epoch #{epoch}, Batch #{i}, Train Loss: {loss_train.item():.4f}, Validation Loss: {loss_validation.item():.4f}\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.13%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for x, t in test_loader:\n",
    "  y = model(x)\n",
    "  y = torch.argmax(y, dim=1)\n",
    "  t = torch.argmax(t, dim=1)\n",
    "\n",
    "  total += t.size(0)\n",
    "  correct += (y == t).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
