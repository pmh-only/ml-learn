{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS ---\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "with open(\"./datasets/mnist/train-images-flatten.pkl\", \"rb\") as f:\n",
    "  train_x = pickle.load(f)\n",
    "\n",
    "with open(\"./datasets/mnist/train-labels.pkl\", \"rb\") as f:\n",
    "  train_t = pickle.load(f)\n",
    "\n",
    "with open(\"./datasets/mnist/test-images-flatten.pkl\", \"rb\") as f:\n",
    "  test_x = pickle.load(f)\n",
    "\n",
    "with open(\"./datasets/mnist/test-labels.pkl\", \"rb\") as f:\n",
    "  test_t = pickle.load(f)\n",
    "  \n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "  def __init__(self, data, label) :\n",
    "    self.data = data\n",
    "    self.label = label\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx) :\n",
    "    data = torch.FloatTensor(self.data[idx])\n",
    "    label = torch.FloatTensor(self.label[idx])\n",
    "\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_x, train_t)\n",
    "test_dataset = Dataset(test_x, test_t)\n",
    "\n",
    "train, validation = torch.utils.data.random_split(train_dataset, [0.9, 0.1])\n",
    "\n",
    "train_loader = data.DataLoader(train, batch_size, shuffle=True)\n",
    "validation_loader = data.DataLoader(validation, batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (rnn): RNN(784, 100, num_layers=5, batch_first=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.rnn = nn.RNN(28*28, 100, 1, nonlinearity='relu', batch_first=True)\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(100, 10)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    y, hn = self.rnn(x)\n",
    "    y = self.layers(y)\n",
    "    return y\n",
    "  \n",
    "model = Model()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, Batch #0, Train Loss: 2.3007, Validation Loss: 2.3077\n",
      "Epoch #0, Batch #10, Train Loss: 2.2867, Validation Loss: 2.2700\n",
      "Epoch #0, Batch #20, Train Loss: 2.1413, Validation Loss: 2.0721\n",
      "Epoch #0, Batch #30, Train Loss: 1.6439, Validation Loss: 1.5722\n",
      "Epoch #0, Batch #40, Train Loss: 1.3682, Validation Loss: 1.4241\n",
      "Epoch #0, Batch #50, Train Loss: 1.1721, Validation Loss: 1.3392\n",
      "Epoch #0, Batch #60, Train Loss: 1.3817, Validation Loss: 1.1258\n",
      "Epoch #0, Batch #70, Train Loss: 1.0462, Validation Loss: 1.0477\n",
      "Epoch #0, Batch #80, Train Loss: 1.2601, Validation Loss: 0.9941\n",
      "Epoch #0, Batch #90, Train Loss: 0.9834, Validation Loss: 0.8770\n",
      "Epoch #0, Batch #100, Train Loss: 0.8066, Validation Loss: 0.6819\n",
      "Epoch #0, Batch #110, Train Loss: 0.6940, Validation Loss: 0.7530\n",
      "Epoch #0, Batch #120, Train Loss: 0.7374, Validation Loss: 0.7338\n",
      "Epoch #0, Batch #130, Train Loss: 0.5382, Validation Loss: 0.7074\n",
      "Epoch #0, Batch #140, Train Loss: 0.7957, Validation Loss: 0.5829\n",
      "Epoch #0, Batch #150, Train Loss: 0.6395, Validation Loss: 0.6322\n",
      "Epoch #0, Batch #160, Train Loss: 0.4648, Validation Loss: 0.6066\n",
      "Epoch #0, Batch #170, Train Loss: 0.7337, Validation Loss: 0.4733\n",
      "Epoch #0, Batch #180, Train Loss: 0.4452, Validation Loss: 0.4849\n",
      "Epoch #0, Batch #190, Train Loss: 0.7406, Validation Loss: 0.3981\n",
      "Epoch #0, Batch #200, Train Loss: 0.4563, Validation Loss: 0.4988\n",
      "Epoch #0, Batch #210, Train Loss: 0.4545, Validation Loss: 0.4688\n",
      "Epoch #0, Batch #220, Train Loss: 0.2939, Validation Loss: 0.3725\n",
      "Epoch #0, Batch #230, Train Loss: 0.5975, Validation Loss: 0.4741\n",
      "Epoch #0, Batch #240, Train Loss: 0.4230, Validation Loss: 0.4928\n",
      "Epoch #0, Batch #250, Train Loss: 0.4090, Validation Loss: 0.4361\n",
      "Epoch #0, Batch #260, Train Loss: 0.4836, Validation Loss: 0.4135\n",
      "Epoch #0, Batch #270, Train Loss: 0.3335, Validation Loss: 0.4256\n",
      "Epoch #0, Batch #280, Train Loss: 0.3841, Validation Loss: 0.4934\n",
      "Epoch #0, Batch #290, Train Loss: 0.3945, Validation Loss: 0.3326\n",
      "Epoch #0, Batch #300, Train Loss: 0.4539, Validation Loss: 0.5162\n",
      "Epoch #0, Batch #310, Train Loss: 0.2389, Validation Loss: 0.5327\n",
      "Epoch #0, Batch #320, Train Loss: 0.4850, Validation Loss: 0.3233\n",
      "Epoch #0, Batch #330, Train Loss: 0.4212, Validation Loss: 0.3458\n",
      "Epoch #0, Batch #340, Train Loss: 0.5081, Validation Loss: 0.2754\n",
      "Epoch #0, Batch #350, Train Loss: 0.5512, Validation Loss: 0.3898\n",
      "Epoch #0, Batch #360, Train Loss: 0.5655, Validation Loss: 0.3499\n",
      "Epoch #0, Batch #370, Train Loss: 0.2447, Validation Loss: 0.3030\n",
      "Epoch #0, Batch #380, Train Loss: 0.4048, Validation Loss: 0.2728\n",
      "Epoch #0, Batch #390, Train Loss: 0.2081, Validation Loss: 0.5249\n",
      "Epoch #0, Batch #400, Train Loss: 0.2705, Validation Loss: 0.1606\n",
      "Epoch #0, Batch #410, Train Loss: 0.3188, Validation Loss: 0.2340\n",
      "Epoch #0, Batch #420, Train Loss: 0.4182, Validation Loss: 0.2902\n",
      "Epoch #0, Batch #430, Train Loss: 0.3141, Validation Loss: 0.3544\n",
      "Epoch #0, Batch #440, Train Loss: 0.4214, Validation Loss: 0.3129\n",
      "Epoch #0, Batch #450, Train Loss: 0.4201, Validation Loss: 0.2533\n",
      "Epoch #0, Batch #460, Train Loss: 0.3038, Validation Loss: 0.2861\n",
      "Epoch #0, Batch #470, Train Loss: 0.3831, Validation Loss: 0.3018\n",
      "Epoch #0, Batch #480, Train Loss: 0.2251, Validation Loss: 0.3111\n",
      "Epoch #0, Batch #490, Train Loss: 0.2202, Validation Loss: 0.3137\n",
      "Epoch #0, Batch #500, Train Loss: 0.3509, Validation Loss: 0.3442\n",
      "Epoch #0, Batch #510, Train Loss: 0.3873, Validation Loss: 0.3523\n",
      "Epoch #0, Batch #520, Train Loss: 0.2835, Validation Loss: 0.1915\n",
      "Epoch #0, Batch #530, Train Loss: 0.1938, Validation Loss: 0.2929\n",
      "Epoch #1, Batch #0, Train Loss: 0.3032, Validation Loss: 0.3552\n",
      "Epoch #1, Batch #10, Train Loss: 0.2902, Validation Loss: 0.2045\n",
      "Epoch #1, Batch #20, Train Loss: 0.2156, Validation Loss: 0.2700\n",
      "Epoch #1, Batch #30, Train Loss: 0.2304, Validation Loss: 0.2791\n",
      "Epoch #1, Batch #40, Train Loss: 0.1807, Validation Loss: 0.2615\n",
      "Epoch #1, Batch #50, Train Loss: 0.2143, Validation Loss: 0.3230\n",
      "Epoch #1, Batch #60, Train Loss: 0.2647, Validation Loss: 0.2272\n",
      "Epoch #1, Batch #70, Train Loss: 0.3391, Validation Loss: 0.2089\n",
      "Epoch #1, Batch #80, Train Loss: 0.2307, Validation Loss: 0.1925\n",
      "Epoch #1, Batch #90, Train Loss: 0.2237, Validation Loss: 0.2010\n",
      "Epoch #1, Batch #100, Train Loss: 0.2576, Validation Loss: 0.2831\n",
      "Epoch #1, Batch #110, Train Loss: 0.0818, Validation Loss: 0.1623\n",
      "Epoch #1, Batch #120, Train Loss: 0.1578, Validation Loss: 0.3440\n",
      "Epoch #1, Batch #130, Train Loss: 0.1949, Validation Loss: 0.2451\n",
      "Epoch #1, Batch #140, Train Loss: 0.2074, Validation Loss: 0.2481\n",
      "Epoch #1, Batch #150, Train Loss: 0.1761, Validation Loss: 0.3245\n",
      "Epoch #1, Batch #160, Train Loss: 0.2524, Validation Loss: 0.2439\n",
      "Epoch #1, Batch #170, Train Loss: 0.4549, Validation Loss: 0.1601\n",
      "Epoch #1, Batch #180, Train Loss: 0.1927, Validation Loss: 0.2766\n",
      "Epoch #1, Batch #190, Train Loss: 0.2051, Validation Loss: 0.1706\n",
      "Epoch #1, Batch #200, Train Loss: 0.1114, Validation Loss: 0.2608\n",
      "Epoch #1, Batch #210, Train Loss: 0.3804, Validation Loss: 0.1959\n",
      "Epoch #1, Batch #220, Train Loss: 0.2453, Validation Loss: 0.2907\n",
      "Epoch #1, Batch #230, Train Loss: 0.2318, Validation Loss: 0.2288\n",
      "Epoch #1, Batch #240, Train Loss: 0.1562, Validation Loss: 0.1740\n",
      "Epoch #1, Batch #250, Train Loss: 0.1476, Validation Loss: 0.2972\n",
      "Epoch #1, Batch #260, Train Loss: 0.2818, Validation Loss: 0.3050\n",
      "Epoch #1, Batch #270, Train Loss: 0.1858, Validation Loss: 0.1058\n",
      "Epoch #1, Batch #280, Train Loss: 0.3291, Validation Loss: 0.1856\n",
      "Epoch #1, Batch #290, Train Loss: 0.1759, Validation Loss: 0.2392\n",
      "Epoch #1, Batch #300, Train Loss: 0.2309, Validation Loss: 0.2740\n",
      "Epoch #1, Batch #310, Train Loss: 0.0600, Validation Loss: 0.0757\n",
      "Epoch #1, Batch #320, Train Loss: 0.1536, Validation Loss: 0.1324\n",
      "Epoch #1, Batch #330, Train Loss: 0.2638, Validation Loss: 0.2555\n",
      "Epoch #1, Batch #340, Train Loss: 0.2282, Validation Loss: 0.1773\n",
      "Epoch #1, Batch #350, Train Loss: 0.2002, Validation Loss: 0.2995\n",
      "Epoch #1, Batch #360, Train Loss: 0.0989, Validation Loss: 0.2430\n",
      "Epoch #1, Batch #370, Train Loss: 0.2190, Validation Loss: 0.1554\n",
      "Epoch #1, Batch #380, Train Loss: 0.1277, Validation Loss: 0.1955\n",
      "Epoch #1, Batch #390, Train Loss: 0.1860, Validation Loss: 0.1276\n",
      "Epoch #1, Batch #400, Train Loss: 0.1732, Validation Loss: 0.1474\n",
      "Epoch #1, Batch #410, Train Loss: 0.1594, Validation Loss: 0.1472\n",
      "Epoch #1, Batch #420, Train Loss: 0.4457, Validation Loss: 0.1384\n",
      "Epoch #1, Batch #430, Train Loss: 0.2621, Validation Loss: 0.1567\n",
      "Epoch #1, Batch #440, Train Loss: 0.3281, Validation Loss: 0.1989\n",
      "Epoch #1, Batch #450, Train Loss: 0.2491, Validation Loss: 0.2124\n",
      "Epoch #1, Batch #460, Train Loss: 0.1301, Validation Loss: 0.1217\n",
      "Epoch #1, Batch #470, Train Loss: 0.2234, Validation Loss: 0.1439\n",
      "Epoch #1, Batch #480, Train Loss: 0.1629, Validation Loss: 0.1493\n",
      "Epoch #1, Batch #490, Train Loss: 0.1327, Validation Loss: 0.1422\n",
      "Epoch #1, Batch #500, Train Loss: 0.1443, Validation Loss: 0.1005\n",
      "Epoch #1, Batch #510, Train Loss: 0.1617, Validation Loss: 0.2482\n",
      "Epoch #1, Batch #520, Train Loss: 0.2796, Validation Loss: 0.1748\n",
      "Epoch #1, Batch #530, Train Loss: 0.1541, Validation Loss: 0.1939\n",
      "Epoch #2, Batch #0, Train Loss: 0.0823, Validation Loss: 0.1588\n",
      "Epoch #2, Batch #10, Train Loss: 0.0717, Validation Loss: 0.1662\n",
      "Epoch #2, Batch #20, Train Loss: 0.1225, Validation Loss: 0.2003\n",
      "Epoch #2, Batch #30, Train Loss: 0.1861, Validation Loss: 0.3329\n",
      "Epoch #2, Batch #40, Train Loss: 0.2150, Validation Loss: 0.2163\n",
      "Epoch #2, Batch #50, Train Loss: 0.1297, Validation Loss: 0.0902\n",
      "Epoch #2, Batch #60, Train Loss: 0.3162, Validation Loss: 0.1701\n",
      "Epoch #2, Batch #70, Train Loss: 0.1125, Validation Loss: 0.1406\n",
      "Epoch #2, Batch #80, Train Loss: 0.2312, Validation Loss: 0.1591\n",
      "Epoch #2, Batch #90, Train Loss: 0.0797, Validation Loss: 0.3753\n",
      "Epoch #2, Batch #100, Train Loss: 0.2571, Validation Loss: 0.1114\n",
      "Epoch #2, Batch #110, Train Loss: 0.0956, Validation Loss: 0.1745\n",
      "Epoch #2, Batch #120, Train Loss: 0.1701, Validation Loss: 0.1370\n",
      "Epoch #2, Batch #130, Train Loss: 0.0775, Validation Loss: 0.2013\n",
      "Epoch #2, Batch #140, Train Loss: 0.0852, Validation Loss: 0.0671\n",
      "Epoch #2, Batch #150, Train Loss: 0.1482, Validation Loss: 0.1655\n",
      "Epoch #2, Batch #160, Train Loss: 0.1226, Validation Loss: 0.2589\n",
      "Epoch #2, Batch #170, Train Loss: 0.1872, Validation Loss: 0.2278\n",
      "Epoch #2, Batch #180, Train Loss: 0.0574, Validation Loss: 0.2251\n",
      "Epoch #2, Batch #190, Train Loss: 0.0509, Validation Loss: 0.1457\n",
      "Epoch #2, Batch #200, Train Loss: 0.0912, Validation Loss: 0.2226\n",
      "Epoch #2, Batch #210, Train Loss: 0.1941, Validation Loss: 0.1164\n",
      "Epoch #2, Batch #220, Train Loss: 0.1750, Validation Loss: 0.1892\n",
      "Epoch #2, Batch #230, Train Loss: 0.0985, Validation Loss: 0.1721\n",
      "Epoch #2, Batch #240, Train Loss: 0.1654, Validation Loss: 0.2150\n",
      "Epoch #2, Batch #250, Train Loss: 0.1279, Validation Loss: 0.1988\n",
      "Epoch #2, Batch #260, Train Loss: 0.2376, Validation Loss: 0.1497\n",
      "Epoch #2, Batch #270, Train Loss: 0.0900, Validation Loss: 0.1720\n",
      "Epoch #2, Batch #280, Train Loss: 0.0383, Validation Loss: 0.0944\n",
      "Epoch #2, Batch #290, Train Loss: 0.0751, Validation Loss: 0.0636\n",
      "Epoch #2, Batch #300, Train Loss: 0.3045, Validation Loss: 0.1673\n",
      "Epoch #2, Batch #310, Train Loss: 0.1429, Validation Loss: 0.1276\n",
      "Epoch #2, Batch #320, Train Loss: 0.2145, Validation Loss: 0.1802\n",
      "Epoch #2, Batch #330, Train Loss: 0.1231, Validation Loss: 0.1645\n",
      "Epoch #2, Batch #340, Train Loss: 0.0829, Validation Loss: 0.1638\n",
      "Epoch #2, Batch #350, Train Loss: 0.0910, Validation Loss: 0.0878\n",
      "Epoch #2, Batch #360, Train Loss: 0.2453, Validation Loss: 0.1735\n",
      "Epoch #2, Batch #370, Train Loss: 0.1752, Validation Loss: 0.1299\n",
      "Epoch #2, Batch #380, Train Loss: 0.1579, Validation Loss: 0.1295\n",
      "Epoch #2, Batch #390, Train Loss: 0.0840, Validation Loss: 0.0772\n",
      "Epoch #2, Batch #400, Train Loss: 0.2062, Validation Loss: 0.0996\n",
      "Epoch #2, Batch #410, Train Loss: 0.2162, Validation Loss: 0.0409\n",
      "Epoch #2, Batch #420, Train Loss: 0.1267, Validation Loss: 0.1399\n",
      "Epoch #2, Batch #430, Train Loss: 0.0681, Validation Loss: 0.1836\n",
      "Epoch #2, Batch #440, Train Loss: 0.0818, Validation Loss: 0.1202\n",
      "Epoch #2, Batch #450, Train Loss: 0.2453, Validation Loss: 0.1738\n",
      "Epoch #2, Batch #460, Train Loss: 0.1000, Validation Loss: 0.1521\n",
      "Epoch #2, Batch #470, Train Loss: 0.2227, Validation Loss: 0.0906\n",
      "Epoch #2, Batch #480, Train Loss: 0.1085, Validation Loss: 0.2760\n",
      "Epoch #2, Batch #490, Train Loss: 0.0847, Validation Loss: 0.1169\n",
      "Epoch #2, Batch #500, Train Loss: 0.1262, Validation Loss: 0.2404\n",
      "Epoch #2, Batch #510, Train Loss: 0.1652, Validation Loss: 0.1328\n",
      "Epoch #2, Batch #520, Train Loss: 0.1525, Validation Loss: 0.1472\n",
      "Epoch #2, Batch #530, Train Loss: 0.0644, Validation Loss: 0.1081\n",
      "Epoch #3, Batch #0, Train Loss: 0.1551, Validation Loss: 0.1049\n",
      "Epoch #3, Batch #10, Train Loss: 0.0992, Validation Loss: 0.0905\n",
      "Epoch #3, Batch #20, Train Loss: 0.0843, Validation Loss: 0.1614\n",
      "Epoch #3, Batch #30, Train Loss: 0.0656, Validation Loss: 0.2926\n",
      "Epoch #3, Batch #40, Train Loss: 0.0906, Validation Loss: 0.0976\n",
      "Epoch #3, Batch #50, Train Loss: 0.1442, Validation Loss: 0.2622\n",
      "Epoch #3, Batch #60, Train Loss: 0.1476, Validation Loss: 0.0799\n",
      "Epoch #3, Batch #70, Train Loss: 0.0674, Validation Loss: 0.0917\n",
      "Epoch #3, Batch #80, Train Loss: 0.1292, Validation Loss: 0.0532\n",
      "Epoch #3, Batch #90, Train Loss: 0.0657, Validation Loss: 0.2507\n",
      "Epoch #3, Batch #100, Train Loss: 0.0845, Validation Loss: 0.0813\n",
      "Epoch #3, Batch #110, Train Loss: 0.0901, Validation Loss: 0.1534\n",
      "Epoch #3, Batch #120, Train Loss: 0.1164, Validation Loss: 0.1932\n",
      "Epoch #3, Batch #130, Train Loss: 0.0736, Validation Loss: 0.0979\n",
      "Epoch #3, Batch #140, Train Loss: 0.1030, Validation Loss: 0.2443\n",
      "Epoch #3, Batch #150, Train Loss: 0.0650, Validation Loss: 0.1424\n",
      "Epoch #3, Batch #160, Train Loss: 0.1627, Validation Loss: 0.0653\n",
      "Epoch #3, Batch #170, Train Loss: 0.0515, Validation Loss: 0.2816\n",
      "Epoch #3, Batch #180, Train Loss: 0.1055, Validation Loss: 0.1007\n",
      "Epoch #3, Batch #190, Train Loss: 0.2431, Validation Loss: 0.0610\n",
      "Epoch #3, Batch #200, Train Loss: 0.1621, Validation Loss: 0.1433\n",
      "Epoch #3, Batch #210, Train Loss: 0.1172, Validation Loss: 0.2538\n",
      "Epoch #3, Batch #220, Train Loss: 0.0815, Validation Loss: 0.1309\n",
      "Epoch #3, Batch #230, Train Loss: 0.1417, Validation Loss: 0.0471\n",
      "Epoch #3, Batch #240, Train Loss: 0.1488, Validation Loss: 0.0190\n",
      "Epoch #3, Batch #250, Train Loss: 0.2036, Validation Loss: 0.0912\n",
      "Epoch #3, Batch #260, Train Loss: 0.1207, Validation Loss: 0.0495\n",
      "Epoch #3, Batch #270, Train Loss: 0.1742, Validation Loss: 0.1426\n",
      "Epoch #3, Batch #280, Train Loss: 0.1274, Validation Loss: 0.1084\n",
      "Epoch #3, Batch #290, Train Loss: 0.1814, Validation Loss: 0.1291\n",
      "Epoch #3, Batch #300, Train Loss: 0.1064, Validation Loss: 0.1773\n",
      "Epoch #3, Batch #310, Train Loss: 0.1602, Validation Loss: 0.1521\n",
      "Epoch #3, Batch #320, Train Loss: 0.0757, Validation Loss: 0.1954\n",
      "Epoch #3, Batch #330, Train Loss: 0.0465, Validation Loss: 0.2075\n",
      "Epoch #3, Batch #340, Train Loss: 0.1915, Validation Loss: 0.2165\n",
      "Epoch #3, Batch #350, Train Loss: 0.0601, Validation Loss: 0.2759\n",
      "Epoch #3, Batch #360, Train Loss: 0.3995, Validation Loss: 0.0533\n",
      "Epoch #3, Batch #370, Train Loss: 0.2604, Validation Loss: 0.1197\n",
      "Epoch #3, Batch #380, Train Loss: 0.1047, Validation Loss: 0.2030\n",
      "Epoch #3, Batch #390, Train Loss: 0.1894, Validation Loss: 0.1434\n",
      "Epoch #3, Batch #400, Train Loss: 0.1760, Validation Loss: 0.1948\n",
      "Epoch #3, Batch #410, Train Loss: 0.0887, Validation Loss: 0.0803\n",
      "Epoch #3, Batch #420, Train Loss: 0.1190, Validation Loss: 0.1031\n",
      "Epoch #3, Batch #430, Train Loss: 0.0804, Validation Loss: 0.1358\n",
      "Epoch #3, Batch #440, Train Loss: 0.1311, Validation Loss: 0.1017\n",
      "Epoch #3, Batch #450, Train Loss: 0.0670, Validation Loss: 0.0785\n",
      "Epoch #3, Batch #460, Train Loss: 0.0704, Validation Loss: 0.2348\n",
      "Epoch #3, Batch #470, Train Loss: 0.0935, Validation Loss: 0.3848\n",
      "Epoch #3, Batch #480, Train Loss: 0.2143, Validation Loss: 0.2361\n",
      "Epoch #3, Batch #490, Train Loss: 0.0556, Validation Loss: 0.1551\n",
      "Epoch #3, Batch #500, Train Loss: 0.0465, Validation Loss: 0.0988\n",
      "Epoch #3, Batch #510, Train Loss: 0.2187, Validation Loss: 0.1749\n",
      "Epoch #3, Batch #520, Train Loss: 0.0957, Validation Loss: 0.0695\n",
      "Epoch #3, Batch #530, Train Loss: 0.0539, Validation Loss: 0.1909\n",
      "Epoch #4, Batch #0, Train Loss: 0.0982, Validation Loss: 0.0892\n",
      "Epoch #4, Batch #10, Train Loss: 0.0760, Validation Loss: 0.0913\n",
      "Epoch #4, Batch #20, Train Loss: 0.0690, Validation Loss: 0.1268\n",
      "Epoch #4, Batch #30, Train Loss: 0.0499, Validation Loss: 0.0741\n",
      "Epoch #4, Batch #40, Train Loss: 0.0180, Validation Loss: 0.3466\n",
      "Epoch #4, Batch #50, Train Loss: 0.1030, Validation Loss: 0.0737\n",
      "Epoch #4, Batch #60, Train Loss: 0.1150, Validation Loss: 0.1152\n",
      "Epoch #4, Batch #70, Train Loss: 0.1335, Validation Loss: 0.0902\n",
      "Epoch #4, Batch #80, Train Loss: 0.1224, Validation Loss: 0.1563\n",
      "Epoch #4, Batch #90, Train Loss: 0.0974, Validation Loss: 0.1520\n",
      "Epoch #4, Batch #100, Train Loss: 0.2045, Validation Loss: 0.1246\n",
      "Epoch #4, Batch #110, Train Loss: 0.1228, Validation Loss: 0.2121\n",
      "Epoch #4, Batch #120, Train Loss: 0.2556, Validation Loss: 0.1111\n",
      "Epoch #4, Batch #130, Train Loss: 0.0983, Validation Loss: 0.1678\n",
      "Epoch #4, Batch #140, Train Loss: 0.1223, Validation Loss: 0.1051\n",
      "Epoch #4, Batch #150, Train Loss: 0.0969, Validation Loss: 0.0928\n",
      "Epoch #4, Batch #160, Train Loss: 0.0228, Validation Loss: 0.1241\n",
      "Epoch #4, Batch #170, Train Loss: 0.0845, Validation Loss: 0.1894\n",
      "Epoch #4, Batch #180, Train Loss: 0.1182, Validation Loss: 0.0747\n",
      "Epoch #4, Batch #190, Train Loss: 0.0600, Validation Loss: 0.0913\n",
      "Epoch #4, Batch #200, Train Loss: 0.0688, Validation Loss: 0.1478\n",
      "Epoch #4, Batch #210, Train Loss: 0.0351, Validation Loss: 0.1321\n",
      "Epoch #4, Batch #220, Train Loss: 0.0952, Validation Loss: 0.1656\n",
      "Epoch #4, Batch #230, Train Loss: 0.1671, Validation Loss: 0.0779\n",
      "Epoch #4, Batch #240, Train Loss: 0.0742, Validation Loss: 0.1310\n",
      "Epoch #4, Batch #250, Train Loss: 0.0675, Validation Loss: 0.0951\n",
      "Epoch #4, Batch #260, Train Loss: 0.0406, Validation Loss: 0.2127\n",
      "Epoch #4, Batch #270, Train Loss: 0.0600, Validation Loss: 0.1788\n",
      "Epoch #4, Batch #280, Train Loss: 0.0505, Validation Loss: 0.1791\n",
      "Epoch #4, Batch #290, Train Loss: 0.1628, Validation Loss: 0.1667\n",
      "Epoch #4, Batch #300, Train Loss: 0.1551, Validation Loss: 0.0995\n",
      "Epoch #4, Batch #310, Train Loss: 0.0892, Validation Loss: 0.3404\n",
      "Epoch #4, Batch #320, Train Loss: 0.1222, Validation Loss: 0.0575\n",
      "Epoch #4, Batch #330, Train Loss: 0.0762, Validation Loss: 0.1137\n",
      "Epoch #4, Batch #340, Train Loss: 0.0658, Validation Loss: 0.1321\n",
      "Epoch #4, Batch #350, Train Loss: 0.1879, Validation Loss: 0.1487\n",
      "Epoch #4, Batch #360, Train Loss: 0.1918, Validation Loss: 0.1573\n",
      "Epoch #4, Batch #370, Train Loss: 0.1733, Validation Loss: 0.0816\n",
      "Epoch #4, Batch #380, Train Loss: 0.0501, Validation Loss: 0.0898\n",
      "Epoch #4, Batch #390, Train Loss: 0.0803, Validation Loss: 0.1986\n",
      "Epoch #4, Batch #400, Train Loss: 0.1374, Validation Loss: 0.0915\n",
      "Epoch #4, Batch #410, Train Loss: 0.0377, Validation Loss: 0.0309\n",
      "Epoch #4, Batch #420, Train Loss: 0.0897, Validation Loss: 0.1125\n",
      "Epoch #4, Batch #430, Train Loss: 0.0171, Validation Loss: 0.2433\n",
      "Epoch #4, Batch #440, Train Loss: 0.1663, Validation Loss: 0.1970\n",
      "Epoch #4, Batch #450, Train Loss: 0.0842, Validation Loss: 0.2238\n",
      "Epoch #4, Batch #460, Train Loss: 0.0880, Validation Loss: 0.0931\n",
      "Epoch #4, Batch #470, Train Loss: 0.0400, Validation Loss: 0.2070\n",
      "Epoch #4, Batch #480, Train Loss: 0.0882, Validation Loss: 0.0807\n",
      "Epoch #4, Batch #490, Train Loss: 0.1425, Validation Loss: 0.1227\n",
      "Epoch #4, Batch #500, Train Loss: 0.0478, Validation Loss: 0.0970\n",
      "Epoch #4, Batch #510, Train Loss: 0.0797, Validation Loss: 0.0749\n",
      "Epoch #4, Batch #520, Train Loss: 0.1059, Validation Loss: 0.2942\n",
      "Epoch #4, Batch #530, Train Loss: 0.0225, Validation Loss: 0.1087\n",
      "Epoch #5, Batch #0, Train Loss: 0.0518, Validation Loss: 0.1492\n",
      "Epoch #5, Batch #10, Train Loss: 0.1368, Validation Loss: 0.1726\n",
      "Epoch #5, Batch #20, Train Loss: 0.1416, Validation Loss: 0.1834\n",
      "Epoch #5, Batch #30, Train Loss: 0.0596, Validation Loss: 0.1804\n",
      "Epoch #5, Batch #40, Train Loss: 0.0477, Validation Loss: 0.1369\n",
      "Epoch #5, Batch #50, Train Loss: 0.0331, Validation Loss: 0.1656\n",
      "Epoch #5, Batch #60, Train Loss: 0.0975, Validation Loss: 0.0426\n",
      "Epoch #5, Batch #70, Train Loss: 0.0853, Validation Loss: 0.1438\n",
      "Epoch #5, Batch #80, Train Loss: 0.1489, Validation Loss: 0.1548\n",
      "Epoch #5, Batch #90, Train Loss: 0.1670, Validation Loss: 0.1764\n",
      "Epoch #5, Batch #100, Train Loss: 0.1147, Validation Loss: 0.1018\n",
      "Epoch #5, Batch #110, Train Loss: 0.0645, Validation Loss: 0.0903\n",
      "Epoch #5, Batch #120, Train Loss: 0.1038, Validation Loss: 0.1353\n",
      "Epoch #5, Batch #130, Train Loss: 0.0594, Validation Loss: 0.0338\n",
      "Epoch #5, Batch #140, Train Loss: 0.1259, Validation Loss: 0.0449\n",
      "Epoch #5, Batch #150, Train Loss: 0.1703, Validation Loss: 0.0678\n",
      "Epoch #5, Batch #160, Train Loss: 0.0536, Validation Loss: 0.1567\n",
      "Epoch #5, Batch #170, Train Loss: 0.0476, Validation Loss: 0.1720\n",
      "Epoch #5, Batch #180, Train Loss: 0.0627, Validation Loss: 0.0962\n",
      "Epoch #5, Batch #190, Train Loss: 0.2329, Validation Loss: 0.1574\n",
      "Epoch #5, Batch #200, Train Loss: 0.0991, Validation Loss: 0.0790\n",
      "Epoch #5, Batch #210, Train Loss: 0.1522, Validation Loss: 0.1292\n",
      "Epoch #5, Batch #220, Train Loss: 0.0721, Validation Loss: 0.2004\n",
      "Epoch #5, Batch #230, Train Loss: 0.1581, Validation Loss: 0.1756\n",
      "Epoch #5, Batch #240, Train Loss: 0.0690, Validation Loss: 0.1598\n",
      "Epoch #5, Batch #250, Train Loss: 0.0803, Validation Loss: 0.1198\n",
      "Epoch #5, Batch #260, Train Loss: 0.0634, Validation Loss: 0.2280\n",
      "Epoch #5, Batch #270, Train Loss: 0.1836, Validation Loss: 0.1329\n",
      "Epoch #5, Batch #280, Train Loss: 0.1217, Validation Loss: 0.0202\n",
      "Epoch #5, Batch #290, Train Loss: 0.1030, Validation Loss: 0.0976\n",
      "Epoch #5, Batch #300, Train Loss: 0.0577, Validation Loss: 0.1656\n",
      "Epoch #5, Batch #310, Train Loss: 0.0804, Validation Loss: 0.0901\n",
      "Epoch #5, Batch #320, Train Loss: 0.0902, Validation Loss: 0.0988\n",
      "Epoch #5, Batch #330, Train Loss: 0.1809, Validation Loss: 0.1150\n",
      "Epoch #5, Batch #340, Train Loss: 0.0878, Validation Loss: 0.1881\n",
      "Epoch #5, Batch #350, Train Loss: 0.1356, Validation Loss: 0.1281\n",
      "Epoch #5, Batch #360, Train Loss: 0.1535, Validation Loss: 0.0869\n",
      "Epoch #5, Batch #370, Train Loss: 0.0753, Validation Loss: 0.1372\n",
      "Epoch #5, Batch #380, Train Loss: 0.1934, Validation Loss: 0.0916\n",
      "Epoch #5, Batch #390, Train Loss: 0.0487, Validation Loss: 0.1326\n",
      "Epoch #5, Batch #400, Train Loss: 0.0385, Validation Loss: 0.0597\n",
      "Epoch #5, Batch #410, Train Loss: 0.0726, Validation Loss: 0.1009\n",
      "Epoch #5, Batch #420, Train Loss: 0.0343, Validation Loss: 0.1661\n",
      "Epoch #5, Batch #430, Train Loss: 0.0327, Validation Loss: 0.0299\n",
      "Epoch #5, Batch #440, Train Loss: 0.1030, Validation Loss: 0.0735\n",
      "Epoch #5, Batch #450, Train Loss: 0.0622, Validation Loss: 0.2060\n",
      "Epoch #5, Batch #460, Train Loss: 0.0603, Validation Loss: 0.0318\n",
      "Epoch #5, Batch #470, Train Loss: 0.0858, Validation Loss: 0.0400\n",
      "Epoch #5, Batch #480, Train Loss: 0.0415, Validation Loss: 0.1326\n",
      "Epoch #5, Batch #490, Train Loss: 0.0869, Validation Loss: 0.1441\n",
      "Epoch #5, Batch #500, Train Loss: 0.0681, Validation Loss: 0.1032\n",
      "Epoch #5, Batch #510, Train Loss: 0.2276, Validation Loss: 0.2044\n",
      "Epoch #5, Batch #520, Train Loss: 0.0693, Validation Loss: 0.1270\n",
      "Epoch #5, Batch #530, Train Loss: 0.0764, Validation Loss: 0.1008\n",
      "Epoch #6, Batch #0, Train Loss: 0.0232, Validation Loss: 0.0735\n",
      "Epoch #6, Batch #10, Train Loss: 0.0091, Validation Loss: 0.0496\n",
      "Epoch #6, Batch #20, Train Loss: 0.0232, Validation Loss: 0.0304\n",
      "Epoch #6, Batch #30, Train Loss: 0.0484, Validation Loss: 0.0337\n",
      "Epoch #6, Batch #40, Train Loss: 0.0914, Validation Loss: 0.0809\n",
      "Epoch #6, Batch #50, Train Loss: 0.1151, Validation Loss: 0.0563\n",
      "Epoch #6, Batch #60, Train Loss: 0.0293, Validation Loss: 0.0838\n",
      "Epoch #6, Batch #70, Train Loss: 0.0919, Validation Loss: 0.1287\n",
      "Epoch #6, Batch #80, Train Loss: 0.1008, Validation Loss: 0.1833\n",
      "Epoch #6, Batch #90, Train Loss: 0.0278, Validation Loss: 0.1471\n",
      "Epoch #6, Batch #100, Train Loss: 0.0541, Validation Loss: 0.1180\n",
      "Epoch #6, Batch #110, Train Loss: 0.0194, Validation Loss: 0.1404\n",
      "Epoch #6, Batch #120, Train Loss: 0.0701, Validation Loss: 0.0734\n",
      "Epoch #6, Batch #130, Train Loss: 0.0494, Validation Loss: 0.1391\n",
      "Epoch #6, Batch #140, Train Loss: 0.0769, Validation Loss: 0.1508\n",
      "Epoch #6, Batch #150, Train Loss: 0.0443, Validation Loss: 0.0922\n",
      "Epoch #6, Batch #160, Train Loss: 0.1313, Validation Loss: 0.0897\n",
      "Epoch #6, Batch #170, Train Loss: 0.0517, Validation Loss: 0.0609\n",
      "Epoch #6, Batch #180, Train Loss: 0.0482, Validation Loss: 0.1905\n",
      "Epoch #6, Batch #190, Train Loss: 0.0886, Validation Loss: 0.0581\n",
      "Epoch #6, Batch #200, Train Loss: 0.0705, Validation Loss: 0.0410\n",
      "Epoch #6, Batch #210, Train Loss: 0.0309, Validation Loss: 0.0685\n",
      "Epoch #6, Batch #220, Train Loss: 0.1062, Validation Loss: 0.0456\n",
      "Epoch #6, Batch #230, Train Loss: 0.0646, Validation Loss: 0.2140\n",
      "Epoch #6, Batch #240, Train Loss: 0.0255, Validation Loss: 0.0584\n",
      "Epoch #6, Batch #250, Train Loss: 0.1240, Validation Loss: 0.0820\n",
      "Epoch #6, Batch #260, Train Loss: 0.0878, Validation Loss: 0.0536\n",
      "Epoch #6, Batch #270, Train Loss: 0.1212, Validation Loss: 0.0660\n",
      "Epoch #6, Batch #280, Train Loss: 0.1671, Validation Loss: 0.1531\n",
      "Epoch #6, Batch #290, Train Loss: 0.0073, Validation Loss: 0.1082\n",
      "Epoch #6, Batch #300, Train Loss: 0.1250, Validation Loss: 0.1401\n",
      "Epoch #6, Batch #310, Train Loss: 0.1301, Validation Loss: 0.2121\n",
      "Epoch #6, Batch #320, Train Loss: 0.0604, Validation Loss: 0.0697\n",
      "Epoch #6, Batch #330, Train Loss: 0.0741, Validation Loss: 0.1323\n",
      "Epoch #6, Batch #340, Train Loss: 0.0760, Validation Loss: 0.1325\n",
      "Epoch #6, Batch #350, Train Loss: 0.0114, Validation Loss: 0.1711\n",
      "Epoch #6, Batch #360, Train Loss: 0.0421, Validation Loss: 0.0210\n",
      "Epoch #6, Batch #370, Train Loss: 0.1557, Validation Loss: 0.0596\n",
      "Epoch #6, Batch #380, Train Loss: 0.0233, Validation Loss: 0.1967\n",
      "Epoch #6, Batch #390, Train Loss: 0.0819, Validation Loss: 0.1384\n",
      "Epoch #6, Batch #400, Train Loss: 0.0594, Validation Loss: 0.1133\n",
      "Epoch #6, Batch #410, Train Loss: 0.1143, Validation Loss: 0.1625\n",
      "Epoch #6, Batch #420, Train Loss: 0.0219, Validation Loss: 0.0527\n",
      "Epoch #6, Batch #430, Train Loss: 0.1321, Validation Loss: 0.0773\n",
      "Epoch #6, Batch #440, Train Loss: 0.0176, Validation Loss: 0.1907\n",
      "Epoch #6, Batch #450, Train Loss: 0.0249, Validation Loss: 0.0825\n",
      "Epoch #6, Batch #460, Train Loss: 0.0937, Validation Loss: 0.1073\n",
      "Epoch #6, Batch #470, Train Loss: 0.0309, Validation Loss: 0.0372\n",
      "Epoch #6, Batch #480, Train Loss: 0.0184, Validation Loss: 0.2210\n",
      "Epoch #6, Batch #490, Train Loss: 0.0495, Validation Loss: 0.0467\n",
      "Epoch #6, Batch #500, Train Loss: 0.0584, Validation Loss: 0.1006\n",
      "Epoch #6, Batch #510, Train Loss: 0.0811, Validation Loss: 0.0301\n",
      "Epoch #6, Batch #520, Train Loss: 0.0404, Validation Loss: 0.1715\n",
      "Epoch #6, Batch #530, Train Loss: 0.1862, Validation Loss: 0.0831\n",
      "Epoch #7, Batch #0, Train Loss: 0.0599, Validation Loss: 0.0640\n",
      "Epoch #7, Batch #10, Train Loss: 0.1218, Validation Loss: 0.0989\n",
      "Epoch #7, Batch #20, Train Loss: 0.0436, Validation Loss: 0.0654\n",
      "Epoch #7, Batch #30, Train Loss: 0.0911, Validation Loss: 0.1668\n",
      "Epoch #7, Batch #40, Train Loss: 0.0613, Validation Loss: 0.0912\n",
      "Epoch #7, Batch #50, Train Loss: 0.0435, Validation Loss: 0.0159\n",
      "Epoch #7, Batch #60, Train Loss: 0.0530, Validation Loss: 0.0706\n",
      "Epoch #7, Batch #70, Train Loss: 0.0523, Validation Loss: 0.0850\n",
      "Epoch #7, Batch #80, Train Loss: 0.0616, Validation Loss: 0.0795\n",
      "Epoch #7, Batch #90, Train Loss: 0.1568, Validation Loss: 0.1420\n",
      "Epoch #7, Batch #100, Train Loss: 0.1198, Validation Loss: 0.0367\n",
      "Epoch #7, Batch #110, Train Loss: 0.0982, Validation Loss: 0.1226\n",
      "Epoch #7, Batch #120, Train Loss: 0.1294, Validation Loss: 0.1133\n",
      "Epoch #7, Batch #130, Train Loss: 0.0132, Validation Loss: 0.1010\n",
      "Epoch #7, Batch #140, Train Loss: 0.0647, Validation Loss: 0.1442\n",
      "Epoch #7, Batch #150, Train Loss: 0.1855, Validation Loss: 0.0515\n",
      "Epoch #7, Batch #160, Train Loss: 0.0711, Validation Loss: 0.0897\n",
      "Epoch #7, Batch #170, Train Loss: 0.0431, Validation Loss: 0.0899\n",
      "Epoch #7, Batch #180, Train Loss: 0.0322, Validation Loss: 0.1120\n",
      "Epoch #7, Batch #190, Train Loss: 0.2113, Validation Loss: 0.0183\n",
      "Epoch #7, Batch #200, Train Loss: 0.1899, Validation Loss: 0.1013\n",
      "Epoch #7, Batch #210, Train Loss: 0.0694, Validation Loss: 0.0388\n",
      "Epoch #7, Batch #220, Train Loss: 0.0598, Validation Loss: 0.1619\n",
      "Epoch #7, Batch #230, Train Loss: 0.0682, Validation Loss: 0.0577\n",
      "Epoch #7, Batch #240, Train Loss: 0.0395, Validation Loss: 0.1141\n",
      "Epoch #7, Batch #250, Train Loss: 0.0327, Validation Loss: 0.0518\n",
      "Epoch #7, Batch #260, Train Loss: 0.0294, Validation Loss: 0.0472\n",
      "Epoch #7, Batch #270, Train Loss: 0.0865, Validation Loss: 0.2214\n",
      "Epoch #7, Batch #280, Train Loss: 0.0461, Validation Loss: 0.2909\n",
      "Epoch #7, Batch #290, Train Loss: 0.0769, Validation Loss: 0.1933\n",
      "Epoch #7, Batch #300, Train Loss: 0.1425, Validation Loss: 0.1490\n",
      "Epoch #7, Batch #310, Train Loss: 0.0307, Validation Loss: 0.0785\n",
      "Epoch #7, Batch #320, Train Loss: 0.0179, Validation Loss: 0.0614\n",
      "Epoch #7, Batch #330, Train Loss: 0.0199, Validation Loss: 0.0560\n",
      "Epoch #7, Batch #340, Train Loss: 0.1174, Validation Loss: 0.1057\n",
      "Epoch #7, Batch #350, Train Loss: 0.0328, Validation Loss: 0.0483\n",
      "Epoch #7, Batch #360, Train Loss: 0.0576, Validation Loss: 0.0665\n",
      "Epoch #7, Batch #370, Train Loss: 0.0295, Validation Loss: 0.0142\n",
      "Epoch #7, Batch #380, Train Loss: 0.0628, Validation Loss: 0.1536\n",
      "Epoch #7, Batch #390, Train Loss: 0.1381, Validation Loss: 0.1346\n",
      "Epoch #7, Batch #400, Train Loss: 0.0336, Validation Loss: 0.0729\n",
      "Epoch #7, Batch #410, Train Loss: 0.1257, Validation Loss: 0.2548\n",
      "Epoch #7, Batch #420, Train Loss: 0.0480, Validation Loss: 0.0523\n",
      "Epoch #7, Batch #430, Train Loss: 0.1549, Validation Loss: 0.0770\n",
      "Epoch #7, Batch #440, Train Loss: 0.1149, Validation Loss: 0.1555\n",
      "Epoch #7, Batch #450, Train Loss: 0.0882, Validation Loss: 0.0886\n",
      "Epoch #7, Batch #460, Train Loss: 0.0205, Validation Loss: 0.1094\n",
      "Epoch #7, Batch #470, Train Loss: 0.1046, Validation Loss: 0.2591\n",
      "Epoch #7, Batch #480, Train Loss: 0.1580, Validation Loss: 0.0485\n",
      "Epoch #7, Batch #490, Train Loss: 0.0998, Validation Loss: 0.0362\n",
      "Epoch #7, Batch #500, Train Loss: 0.1119, Validation Loss: 0.1146\n",
      "Epoch #7, Batch #510, Train Loss: 0.0330, Validation Loss: 0.1974\n",
      "Epoch #7, Batch #520, Train Loss: 0.0375, Validation Loss: 0.0352\n",
      "Epoch #7, Batch #530, Train Loss: 0.0682, Validation Loss: 0.1558\n",
      "Epoch #8, Batch #0, Train Loss: 0.0393, Validation Loss: 0.0948\n",
      "Epoch #8, Batch #10, Train Loss: 0.1059, Validation Loss: 0.0725\n",
      "Epoch #8, Batch #20, Train Loss: 0.1119, Validation Loss: 0.0328\n",
      "Epoch #8, Batch #30, Train Loss: 0.0516, Validation Loss: 0.0259\n",
      "Epoch #8, Batch #40, Train Loss: 0.0573, Validation Loss: 0.1480\n",
      "Epoch #8, Batch #50, Train Loss: 0.1330, Validation Loss: 0.0870\n",
      "Epoch #8, Batch #60, Train Loss: 0.0744, Validation Loss: 0.1285\n",
      "Epoch #8, Batch #70, Train Loss: 0.1185, Validation Loss: 0.0981\n",
      "Epoch #8, Batch #80, Train Loss: 0.0227, Validation Loss: 0.1143\n",
      "Epoch #8, Batch #90, Train Loss: 0.0283, Validation Loss: 0.1095\n",
      "Epoch #8, Batch #100, Train Loss: 0.0708, Validation Loss: 0.0303\n",
      "Epoch #8, Batch #110, Train Loss: 0.1009, Validation Loss: 0.1307\n",
      "Epoch #8, Batch #120, Train Loss: 0.0406, Validation Loss: 0.1140\n",
      "Epoch #8, Batch #130, Train Loss: 0.1054, Validation Loss: 0.0434\n",
      "Epoch #8, Batch #140, Train Loss: 0.0773, Validation Loss: 0.1407\n",
      "Epoch #8, Batch #150, Train Loss: 0.1048, Validation Loss: 0.2556\n",
      "Epoch #8, Batch #160, Train Loss: 0.0190, Validation Loss: 0.1771\n",
      "Epoch #8, Batch #170, Train Loss: 0.0476, Validation Loss: 0.1135\n",
      "Epoch #8, Batch #180, Train Loss: 0.0926, Validation Loss: 0.1429\n",
      "Epoch #8, Batch #190, Train Loss: 0.0275, Validation Loss: 0.0139\n",
      "Epoch #8, Batch #200, Train Loss: 0.1454, Validation Loss: 0.0753\n",
      "Epoch #8, Batch #210, Train Loss: 0.0592, Validation Loss: 0.0345\n",
      "Epoch #8, Batch #220, Train Loss: 0.1117, Validation Loss: 0.0155\n",
      "Epoch #8, Batch #230, Train Loss: 0.0120, Validation Loss: 0.2305\n",
      "Epoch #8, Batch #240, Train Loss: 0.0530, Validation Loss: 0.1990\n",
      "Epoch #8, Batch #250, Train Loss: 0.0701, Validation Loss: 0.1013\n",
      "Epoch #8, Batch #260, Train Loss: 0.1126, Validation Loss: 0.0147\n",
      "Epoch #8, Batch #270, Train Loss: 0.0309, Validation Loss: 0.1861\n",
      "Epoch #8, Batch #280, Train Loss: 0.0852, Validation Loss: 0.1846\n",
      "Epoch #8, Batch #290, Train Loss: 0.1047, Validation Loss: 0.0861\n",
      "Epoch #8, Batch #300, Train Loss: 0.0158, Validation Loss: 0.0795\n",
      "Epoch #8, Batch #310, Train Loss: 0.1050, Validation Loss: 0.0694\n",
      "Epoch #8, Batch #320, Train Loss: 0.0680, Validation Loss: 0.0847\n",
      "Epoch #8, Batch #330, Train Loss: 0.0190, Validation Loss: 0.1189\n",
      "Epoch #8, Batch #340, Train Loss: 0.1565, Validation Loss: 0.1347\n",
      "Epoch #8, Batch #350, Train Loss: 0.1308, Validation Loss: 0.2766\n",
      "Epoch #8, Batch #360, Train Loss: 0.0306, Validation Loss: 0.0597\n",
      "Epoch #8, Batch #370, Train Loss: 0.0171, Validation Loss: 0.1099\n",
      "Epoch #8, Batch #380, Train Loss: 0.1800, Validation Loss: 0.1579\n",
      "Epoch #8, Batch #390, Train Loss: 0.0649, Validation Loss: 0.0339\n",
      "Epoch #8, Batch #400, Train Loss: 0.0758, Validation Loss: 0.0653\n",
      "Epoch #8, Batch #410, Train Loss: 0.0351, Validation Loss: 0.1361\n",
      "Epoch #8, Batch #420, Train Loss: 0.0165, Validation Loss: 0.1073\n",
      "Epoch #8, Batch #430, Train Loss: 0.0856, Validation Loss: 0.1530\n",
      "Epoch #8, Batch #440, Train Loss: 0.0452, Validation Loss: 0.1084\n",
      "Epoch #8, Batch #450, Train Loss: 0.1168, Validation Loss: 0.1160\n",
      "Epoch #8, Batch #460, Train Loss: 0.0424, Validation Loss: 0.1143\n",
      "Epoch #8, Batch #470, Train Loss: 0.0796, Validation Loss: 0.1832\n",
      "Epoch #8, Batch #480, Train Loss: 0.0403, Validation Loss: 0.1420\n",
      "Epoch #8, Batch #490, Train Loss: 0.0804, Validation Loss: 0.1731\n",
      "Epoch #8, Batch #500, Train Loss: 0.1400, Validation Loss: 0.0632\n",
      "Epoch #8, Batch #510, Train Loss: 0.1047, Validation Loss: 0.1477\n",
      "Epoch #8, Batch #520, Train Loss: 0.1252, Validation Loss: 0.0615\n",
      "Epoch #8, Batch #530, Train Loss: 0.0581, Validation Loss: 0.2214\n",
      "Epoch #9, Batch #0, Train Loss: 0.0410, Validation Loss: 0.1439\n",
      "Epoch #9, Batch #10, Train Loss: 0.0379, Validation Loss: 0.1620\n",
      "Epoch #9, Batch #20, Train Loss: 0.0811, Validation Loss: 0.1526\n",
      "Epoch #9, Batch #30, Train Loss: 0.0765, Validation Loss: 0.1228\n",
      "Epoch #9, Batch #40, Train Loss: 0.0434, Validation Loss: 0.0645\n",
      "Epoch #9, Batch #50, Train Loss: 0.1440, Validation Loss: 0.1408\n",
      "Epoch #9, Batch #60, Train Loss: 0.0442, Validation Loss: 0.1384\n",
      "Epoch #9, Batch #70, Train Loss: 0.0259, Validation Loss: 0.1505\n",
      "Epoch #9, Batch #80, Train Loss: 0.0736, Validation Loss: 0.1378\n",
      "Epoch #9, Batch #90, Train Loss: 0.0940, Validation Loss: 0.0772\n",
      "Epoch #9, Batch #100, Train Loss: 0.0103, Validation Loss: 0.0663\n",
      "Epoch #9, Batch #110, Train Loss: 0.0137, Validation Loss: 0.0982\n",
      "Epoch #9, Batch #120, Train Loss: 0.0358, Validation Loss: 0.0898\n",
      "Epoch #9, Batch #130, Train Loss: 0.0720, Validation Loss: 0.1677\n",
      "Epoch #9, Batch #140, Train Loss: 0.0580, Validation Loss: 0.2000\n",
      "Epoch #9, Batch #150, Train Loss: 0.0094, Validation Loss: 0.1610\n",
      "Epoch #9, Batch #160, Train Loss: 0.0108, Validation Loss: 0.1407\n",
      "Epoch #9, Batch #170, Train Loss: 0.0805, Validation Loss: 0.1313\n",
      "Epoch #9, Batch #180, Train Loss: 0.0221, Validation Loss: 0.1561\n",
      "Epoch #9, Batch #190, Train Loss: 0.0579, Validation Loss: 0.0613\n",
      "Epoch #9, Batch #200, Train Loss: 0.0859, Validation Loss: 0.0907\n",
      "Epoch #9, Batch #210, Train Loss: 0.1884, Validation Loss: 0.0667\n",
      "Epoch #9, Batch #220, Train Loss: 0.0295, Validation Loss: 0.1436\n",
      "Epoch #9, Batch #230, Train Loss: 0.0471, Validation Loss: 0.0657\n",
      "Epoch #9, Batch #240, Train Loss: 0.0221, Validation Loss: 0.1569\n",
      "Epoch #9, Batch #250, Train Loss: 0.0541, Validation Loss: 0.1191\n",
      "Epoch #9, Batch #260, Train Loss: 0.0412, Validation Loss: 0.1641\n",
      "Epoch #9, Batch #270, Train Loss: 0.0378, Validation Loss: 0.1026\n",
      "Epoch #9, Batch #280, Train Loss: 0.0607, Validation Loss: 0.1202\n",
      "Epoch #9, Batch #290, Train Loss: 0.0709, Validation Loss: 0.0546\n",
      "Epoch #9, Batch #300, Train Loss: 0.0184, Validation Loss: 0.1584\n",
      "Epoch #9, Batch #310, Train Loss: 0.0279, Validation Loss: 0.0354\n",
      "Epoch #9, Batch #320, Train Loss: 0.0845, Validation Loss: 0.0448\n",
      "Epoch #9, Batch #330, Train Loss: 0.0140, Validation Loss: 0.0870\n",
      "Epoch #9, Batch #340, Train Loss: 0.0278, Validation Loss: 0.2567\n",
      "Epoch #9, Batch #350, Train Loss: 0.0244, Validation Loss: 0.0840\n",
      "Epoch #9, Batch #360, Train Loss: 0.0399, Validation Loss: 0.0750\n",
      "Epoch #9, Batch #370, Train Loss: 0.0554, Validation Loss: 0.1340\n",
      "Epoch #9, Batch #380, Train Loss: 0.0203, Validation Loss: 0.1209\n",
      "Epoch #9, Batch #390, Train Loss: 0.1922, Validation Loss: 0.1921\n",
      "Epoch #9, Batch #400, Train Loss: 0.0051, Validation Loss: 0.1214\n",
      "Epoch #9, Batch #410, Train Loss: 0.0357, Validation Loss: 0.0896\n",
      "Epoch #9, Batch #420, Train Loss: 0.0223, Validation Loss: 0.0722\n",
      "Epoch #9, Batch #430, Train Loss: 0.0410, Validation Loss: 0.0968\n",
      "Epoch #9, Batch #440, Train Loss: 0.0143, Validation Loss: 0.1127\n",
      "Epoch #9, Batch #450, Train Loss: 0.0343, Validation Loss: 0.0966\n",
      "Epoch #9, Batch #460, Train Loss: 0.1439, Validation Loss: 0.3071\n",
      "Epoch #9, Batch #470, Train Loss: 0.0131, Validation Loss: 0.0573\n",
      "Epoch #9, Batch #480, Train Loss: 0.0642, Validation Loss: 0.1870\n",
      "Epoch #9, Batch #490, Train Loss: 0.0094, Validation Loss: 0.0496\n",
      "Epoch #9, Batch #500, Train Loss: 0.0107, Validation Loss: 0.0189\n",
      "Epoch #9, Batch #510, Train Loss: 0.0210, Validation Loss: 0.0688\n",
      "Epoch #9, Batch #520, Train Loss: 0.0260, Validation Loss: 0.2278\n",
      "Epoch #9, Batch #530, Train Loss: 0.0517, Validation Loss: 0.0438\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  for i, (x, t) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    y = model(x)\n",
    "    loss_train = criterion(y, t)\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "      for x, t in validation_loader:\n",
    "        y = model(x)\n",
    "        loss_validation = criterion(y, t)\n",
    "\n",
    "      print(f\"Epoch #{epoch}, Batch #{i}, Train Loss: {loss_train.item():.4f}, Validation Loss: {loss_validation.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.28%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for x, t in test_loader:\n",
    "  y = model(x)\n",
    "  y = torch.argmax(y, dim=1)\n",
    "  t = torch.argmax(t, dim=1)\n",
    "\n",
    "  total += t.size(0)\n",
    "  correct += (y == t).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
